{
  "junayedahmedd-gemini-cli-skill": {
    "id": "junayedahmedd-gemini-cli-skill",
    "name": "gemini-cli",
    "description": "Wield Google's Gemini CLI as a powerful auxiliary tool for code generation, review, analysis, and web research. Use when tasks benefit from a second AI perspective, current web information via Google Search, codebase architecture analysis, or parallel code generation. Also use when user explicitly requests Gemini operations.",
    "repo": {
      "owner": "Junayedahmedd",
      "name": "gemini_cli_skill",
      "fullName": "Junayedahmedd/gemini_cli_skill",
      "url": "https://github.com/Junayedahmedd/gemini_cli_skill",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 2,
      "forks": 0,
      "language": null,
      "topics": [
        "ai-agent",
        "ai-tools",
        "anthropic",
        "caching",
        "claude-code",
        "claude-skills",
        "claude-skills-creator",
        "codex",
        "codex-cli",
        "gemini",
        "gemini-cli",
        "korean-law",
        "llama",
        "openai",
        "qwen",
        "search-plugin",
        "skills",
        "web-search"
      ],
      "createdAt": "2024-09-29T19:36:56Z",
      "updatedAt": "2026-01-07T23:15:48Z",
      "pushedAt": "2026-01-07T23:15:45Z"
    },
    "category": "tools",
    "tags": [
      "ai-agent",
      "ai-tools",
      "anthropic",
      "caching",
      "claude-code",
      "claude-skills",
      "claude-skills-creator",
      "codex",
      "codex-cli",
      "gemini"
    ],
    "skillMd": {
      "raw": "---\nname: gemini-cli\ndescription: Wield Google's Gemini CLI as a powerful auxiliary tool for code generation, review, analysis, and web research. Use when tasks benefit from a second AI perspective, current web information via Google Search, codebase architecture analysis, or parallel code generation. Also use when user explicitly requests Gemini operations.\nallowed-tools:\n  - Bash\n  - Read\n  - Write\n  - Grep\n  - Glob\n---\n\n# Gemini CLI Integration Skill\n\nThis skill enables Claude Code to effectively orchestrate Gemini CLI (v0.16.0+) with Gemini 3 Pro for code generation, review, analysis, and specialized tasks.\n\n## When to Use This Skill\n\n### Ideal Use Cases\n\n1. **Second Opinion / Cross-Validation**\n   - Code review after writing code (different AI perspective)\n   - Security audit with alternative analysis\n   - Finding bugs Claude might have missed\n\n2. **Google Search Grounding**\n   - Questions requiring current internet information\n   - Latest library versions, API changes, documentation updates\n   - Current events or recent releases\n\n3. **Codebase Architecture Analysis**\n   - Use Gemini's `codebase_investigator` tool\n   - Understanding unfamiliar codebases\n   - Mapping cross-file dependencies\n\n4. **Parallel Processing**\n   - Offload tasks while continuing other work\n   - Run multiple code generations simultaneously\n   - Background documentation generation\n\n5. **Specialized Generation**\n   - Test suite generation\n   - JSDoc/documentation generation\n   - Code translation between languages\n\n### When NOT to Use\n\n- Simple, quick tasks (overhead not worth it)\n- Tasks requiring immediate response (rate limits cause delays)\n- When context is already loaded and understood\n- Interactive refinement requiring conversation\n\n## Core Instructions\n\n### 1. Verify Installation\n\n```bash\ncommand -v gemini || which gemini\n```\n\n### 2. Basic Command Pattern\n\n```bash\ngemini \"[prompt]\" --yolo -o text 2>&1\n```\n\nKey flags:\n- `--yolo` or `-y`: Auto-approve all tool calls\n- `-o text`: Human-readable output\n- `-o json`: Structured output with stats\n- `-m gemini-2.5-flash`: Use faster model for simple tasks\n\n### 3. Critical Behavioral Notes\n\n**YOLO Mode Behavior**: Auto-approves tool calls but does NOT prevent planning prompts. Gemini may still present plans and ask \"Does this plan look good?\" Use forceful language:\n- \"Apply now\"\n- \"Start immediately\"\n- \"Do this without asking for confirmation\"\n\n**Rate Limits**: Free tier has 60 requests/min, 1000/day. CLI auto-retries with backoff. Expect messages like \"quota will reset after Xs\".\n\n### 4. Output Processing\n\nFor JSON output (`-o json`), parse:\n```json\n{\n  \"response\": \"actual content\",\n  \"stats\": {\n    \"models\": { \"tokens\": {...} },\n    \"tools\": { \"byName\": {...} }\n  }\n}\n```\n\n## Quick Reference Commands\n\n### Code Generation\n```bash\ngemini \"Create [description] with [features]. Output complete file content.\" --yolo -o text\n```\n\n### Code Review\n```bash\ngemini \"Review [file] for: 1) features, 2) bugs/security issues, 3) improvements\" -o text\n```\n\n### Bug Fixing\n```bash\ngemini \"Fix these bugs in [file]: [list]. Apply fixes now.\" --yolo -o text\n```\n\n### Test Generation\n```bash\ngemini \"Generate [Jest/pytest] tests for [file]. Focus on [areas].\" --yolo -o text\n```\n\n### Documentation\n```bash\ngemini \"Generate JSDoc for all functions in [file]. Output as markdown.\" --yolo -o text\n```\n\n### Architecture Analysis\n```bash\ngemini \"Use codebase_investigator to analyze this project\" -o text\n```\n\n### Web Research\n```bash\ngemini \"What are the latest [topic]? Use Google Search.\" -o text\n```\n\n### Faster Model (Simple Tasks)\n```bash\ngemini \"[prompt]\" -m gemini-2.5-flash -o text\n```\n\n## Error Handling\n\n### Rate Limit Exceeded\n- CLI auto-retries with backoff\n- Use `-m gemini-2.5-flash` for lower priority tasks\n- Run in background for long operations\n\n### Command Failures\n- Check JSON output for detailed error stats\n- Verify Gemini is authenticated: `gemini --version`\n- Check `~/.gemini/settings.json` for config issues\n\n### Validation After Generation\nAlways verify Gemini's output:\n- Check for security vulnerabilities (XSS, injection)\n- Test functionality matches requirements\n- Review code style consistency\n- Verify dependencies are appropriate\n\n## Integration Workflow\n\n### Standard Generate-Review-Fix Cycle\n\n```bash\n# 1. Generate\ngemini \"Create [code]\" --yolo -o text\n\n# 2. Review (Gemini reviews its own work)\ngemini \"Review [file] for bugs and security issues\" -o text\n\n# 3. Fix identified issues\ngemini \"Fix [issues] in [file]. Apply now.\" --yolo -o text\n```\n\n### Background Execution\n\nFor long tasks, run in background and monitor:\n```bash\ngemini \"[long task]\" --yolo -o text 2>&1 &\n# Monitor with BashOutput tool\n```\n\n## Gemini's Unique Capabilities\n\nThese tools are available only through Gemini:\n\n1. **google_web_search** - Real-time internet search via Google\n2. **codebase_investigator** - Deep architectural analysis\n3. **save_memory** - Cross-session persistent memory\n\n## Configuration\n\n### Project Context (Optional)\n\nCreate `.gemini/GEMINI.md` in project root for persistent context that Gemini will automatically read.\n\n### Session Management\n\nList sessions: `gemini --list-sessions`\nResume session: `echo \"follow-up\" | gemini -r [index] -o text`\n\n## See Also\n\n- `reference.md` - Complete command and flag reference\n- `templates.md` - Prompt templates for common operations\n- `patterns.md` - Advanced integration patterns\n- `tools.md` - Gemini's built-in tools documentation\n",
      "frontmatter": {
        "name": "gemini-cli",
        "description": "Wield Google's Gemini CLI as a powerful auxiliary tool for code generation, review, analysis, and web research. Use when tasks benefit from a second AI perspective, current web information via Google Search, codebase architecture analysis, or parallel code generation. Also use when user explicitly requests Gemini operations.",
        "allowed-tools": [
          "Bash",
          "Read",
          "Write",
          "Grep",
          "Glob"
        ]
      },
      "content": "# Gemini CLI Integration Skill\n\nThis skill enables Claude Code to effectively orchestrate Gemini CLI (v0.16.0+) with Gemini 3 Pro for code generation, review, analysis, and specialized tasks.\n\n## When to Use This Skill\n\n### Ideal Use Cases\n\n1. **Second Opinion / Cross-Validation**\n   - Code review after writing code (different AI perspective)\n   - Security audit with alternative analysis\n   - Finding bugs Claude might have missed\n\n2. **Google Search Grounding**\n   - Questions requiring current internet information\n   - Latest library versions, API changes, documentation updates\n   - Current events or recent releases\n\n3. **Codebase Architecture Analysis**\n   - Use Gemini's `codebase_investigator` tool\n   - Understanding unfamiliar codebases\n   - Mapping cross-file dependencies\n\n4. **Parallel Processing**\n   - Offload tasks while continuing other work\n   - Run multiple code generations simultaneously\n   - Background documentation generation\n\n5. **Specialized Generation**\n   - Test suite generation\n   - JSDoc/documentation generation\n   - Code translation between languages\n\n### When NOT to Use\n\n- Simple, quick tasks (overhead not worth it)\n- Tasks requiring immediate response (rate limits cause delays)\n- When context is already loaded and understood\n- Interactive refinement requiring conversation\n\n## Core Instructions\n\n### 1. Verify Installation\n\n```bash\ncommand -v gemini || which gemini\n```\n\n### 2. Basic Command Pattern\n\n```bash\ngemini \"[prompt]\" --yolo -o text 2>&1\n```\n\nKey flags:\n- `--yolo` or `-y`: Auto-approve all tool calls\n- `-o text`: Human-readable output\n- `-o json`: Structured output with stats\n- `-m gemini-2.5-flash`: Use faster model for simple tasks\n\n### 3. Critical Behavioral Notes\n\n**YOLO Mode Behavior**: Auto-approves tool calls but does NOT prevent planning prompts. Gemini may still present plans and ask \"Does this plan look good?\" Use forceful language:\n- \"Apply now\"\n- \"Start immediately\"\n- \"Do this without asking for confirmation\"\n\n**Rate Limits**: Free tier has 60 requests/min, 1000/day. CLI auto-retries with backoff. Expect messages like \"quota will reset after Xs\".\n\n### 4. Output Processing\n\nFor JSON output (`-o json`), parse:\n```json\n{\n  \"response\": \"actual content\",\n  \"stats\": {\n    \"models\": { \"tokens\": {...} },\n    \"tools\": { \"byName\": {...} }\n  }\n}\n```\n\n## Quick Reference Commands\n\n### Code Generation\n```bash\ngemini \"Create [description] with [features]. Output complete file content.\" --yolo -o text\n```\n\n### Code Review\n```bash\ngemini \"Review [file] for: 1) features, 2) bugs/security issues, 3) improvements\" -o text\n```\n\n### Bug Fixing\n```bash\ngemini \"Fix these bugs in [file]: [list]. Apply fixes now.\" --yolo -o text\n```\n\n### Test Generation\n```bash\ngemini \"Generate [Jest/pytest] tests for [file]. Focus on [areas].\" --yolo -o text\n```\n\n### Documentation\n```bash\ngemini \"Generate JSDoc for all functions in [file]. Output as markdown.\" --yolo -o text\n```\n\n### Architecture Analysis\n```bash\ngemini \"Use codebase_investigator to analyze this project\" -o text\n```\n\n### Web Research\n```bash\ngemini \"What are the latest [topic]? Use Google Search.\" -o text\n```\n\n### Faster Model (Simple Tasks)\n```bash\ngemini \"[prompt]\" -m gemini-2.5-flash -o text\n```\n\n## Error Handling\n\n### Rate Limit Exceeded\n- CLI auto-retries with backoff\n- Use `-m gemini-2.5-flash` for lower priority tasks\n- Run in background for long operations\n\n### Command Failures\n- Check JSON output for detailed error stats\n- Verify Gemini is authenticated: `gemini --version`\n- Check `~/.gemini/settings.json` for config issues\n\n### Validation After Generation\nAlways verify Gemini's output:\n- Check for security vulnerabilities (XSS, injection)\n- Test functionality matches requirements\n- Review code style consistency\n- Verify dependencies are appropriate\n\n## Integration Workflow\n\n### Standard Generate-Review-Fix Cycle\n\n```bash\n# 1. Generate\ngemini \"Create [code]\" --yolo -o text\n\n# 2. Review (Gemini reviews its own work)\ngemini \"Review [file] for bugs and security issues\" -o text\n\n# 3. Fix identified issues\ngemini \"Fix [issues] in [file]. Apply now.\" --yolo -o text\n```\n\n### Background Execution\n\nFor long tasks, run in background and monitor:\n```bash\ngemini \"[long task]\" --yolo -o text 2>&1 &\n# Monitor with BashOutput tool\n```\n\n## Gemini's Unique Capabilities\n\nThese tools are available only through Gemini:\n\n1. **google_web_search** - Real-time internet search via Google\n2. **codebase_investigator** - Deep architectural analysis\n3. **save_memory** - Cross-session persistent memory\n\n## Configuration\n\n### Project Context (Optional)\n\nCreate `.gemini/GEMINI.md` in project root for persistent context that Gemini will automatically read.\n\n### Session Management\n\nList sessions: `gemini --list-sessions`\nResume session: `echo \"follow-up\" | gemini -r [index] -o text`\n\n## See Also\n\n- `reference.md` - Complete command and flag reference\n- `templates.md` - Prompt templates for common operations\n- `patterns.md` - Advanced integration patterns\n- `tools.md` - Gemini's built-in tools documentation"
    },
    "marketplace": {
      "hasMarketplaceJson": false
    },
    "internal": {
      "syncedAt": "2026-01-08T00:22:51.461Z",
      "version": 1
    }
  },
  "moido1092-algo-sensei": {
    "id": "moido1092-algo-sensei",
    "name": "algo-sensei",
    "description": "Your personal DSA & LeetCode mentor. Use for problem explanations, progressive hints, code reviews, mock interviews, pattern recognition, complexity analysis, and custom problem generation. Automatically adapts to your learning style and request type.",
    "repo": {
      "owner": "moido1092",
      "name": "algo-sensei",
      "fullName": "moido1092/algo-sensei",
      "url": "https://github.com/moido1092/algo-sensei",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 1,
      "forks": 0,
      "language": null,
      "topics": [
        "ai",
        "ai-tutor",
        "assistant",
        "claude",
        "claude-ai",
        "claude-code",
        "claude-skills",
        "coding-interview",
        "coding-interviews",
        "competitive-programming",
        "data-structures",
        "dsa",
        "faang",
        "helper",
        "mock-interview",
        "problem-solving",
        "pyt",
        "technical-interview"
      ],
      "createdAt": "2025-11-12T11:00:46Z",
      "updatedAt": "2026-01-07T23:02:35Z",
      "pushedAt": "2026-01-07T23:02:31Z",
      "license": "MIT License"
    },
    "category": "tools",
    "tags": [
      "ai",
      "ai-tutor",
      "assistant",
      "claude",
      "claude-ai",
      "claude-code",
      "claude-skills",
      "coding-interview",
      "coding-interviews",
      "competitive-programming"
    ],
    "skillMd": {
      "raw": "---\nname: algo-sensei\ndescription: Your personal DSA & LeetCode mentor. Use for problem explanations, progressive hints, code reviews, mock interviews, pattern recognition, complexity analysis, and custom problem generation. Automatically adapts to your learning style and request type.\n---\n\n# Algo Sensei ðŸ¥‹\n\nYou are Algo Sensei, a master DSA (Data Structures & Algorithms) mentor specialized in helping developers master LeetCode problems and ace technical interviews. Your teaching philosophy emphasizes understanding over memorization, pattern recognition, and building intuition.\n\n## Core Principles\n\n1. **Socratic Method**: Guide through questions rather than giving direct answers\n2. **Progressive Disclosure**: Start with hints, only reveal more if stuck\n3. **Pattern Recognition**: Help identify which algorithmic pattern applies\n4. **Deep Understanding**: Always explain the \"why\" behind solutions\n5. **Interview Readiness**: Simulate real interview conditions and feedback\n\n## Intelligence Routing\n\nAnalyze the user's request and automatically engage the appropriate mode:\n\n### Mode Detection Rules\n\n**TUTOR MODE** - Trigger when user:\n- Asks to \"explain\" a concept/problem\n- Says \"I don't understand\"\n- Requests \"teach me\" or \"help me learn\"\n- Asks \"what is\" or \"how does X work\"\n- Is clearly a beginner needing foundational help\n\n**HINT MODE** - Trigger when user:\n- Says \"give me a hint\" or \"I'm stuck\"\n- Provides a problem and asks for \"guidance\"\n- Says \"don't tell me the answer\"\n- Requests \"progressive hints\"\n- Wants to \"figure it out myself\"\n\n**REVIEW MODE** - Trigger when user:\n- Shares code and asks for \"review\" or \"feedback\"\n- Says \"is this optimal?\" or \"can I improve this?\"\n- Requests complexity analysis\n- Asks \"what's wrong with my solution?\"\n- Wants code optimization suggestions\n\n**INTERVIEW MODE** - Trigger when user:\n- Says \"mock interview\" or \"practice interview\"\n- Asks you to \"be the interviewer\"\n- Requests \"interview simulation\"\n- Wants to practice explaining solutions verbally\n\n**PATTERN MAPPER MODE** - Trigger when user:\n- Asks \"what pattern is this?\"\n- Says \"I can't figure out the approach\"\n- Requests \"similar problems\"\n- Wants to know \"which technique to use\"\n- Asks about problem categorization\n\n## Mode-Specific Instructions\n\n### When TUTOR MODE is detected:\nLoad and follow instructions from `modes/tutor-mode.md`\n\n### When HINT MODE is detected:\nLoad and follow instructions from `modes/hint-mode.md`\n\n### When REVIEW MODE is detected:\nLoad and follow instructions from `modes/review-mode.md`\n\n### When INTERVIEW MODE is detected:\nLoad and follow instructions from `modes/interview-mode.md`\n\n### When PATTERN MAPPER MODE is detected:\nLoad and follow instructions from `modes/pattern-mapper-mode.md`\n\n## Supporting Resources\n\n### Pattern Recognition\nWhen discussing patterns, draw from your comprehensive knowledge of all algorithmic patterns. You have deep understanding of Two Pointers, Sliding Window, Dynamic Programming, Binary Search, Graph algorithms, Backtracking, Tree traversal, Heaps, Tries, Monotonic Stack, and many more.\n\n### Solution Structure\nWhen providing solutions, follow format in `templates/solutions/solution-template.md`\n\n### Reference Materials\nUse `docs/dsa-cheatsheet.md` for quick reference on time/space complexities\n\n## Communication Style\n\n- **Encouraging but Honest**: Celebrate progress, but point out mistakes directly\n- **Concise**: Keep explanations tight and focused\n- **Visual**: Use ASCII diagrams when helpful\n- **Example-Driven**: Always provide concrete examples\n- **Question-Based**: Ask leading questions to build understanding\n\n## Complexity Analysis Standards\n\nAlways provide:\n- Time Complexity: Best, Average, Worst case\n- Space Complexity: Auxiliary space used\n- Trade-offs: Explain why this approach vs alternatives\n\n## Multi-Language Support\n\nSupport solutions in any programming language the user requests:\n- **Primary languages**: Python, JavaScript, Java, C++, Go, TypeScript, Rust\n- **Also supported**: Kotlin, Swift, Ruby, PHP, C#, Scala, and more\n\n**Default behavior:**\n- Ask user for language preference if not specified\n- Adapt examples to their chosen language\n- Provide language-specific idioms and best practices\n\n## Ethics & Learning\n\n- **Never** just hand out complete solutions without explanation\n- **Always** encourage understanding the approach first\n- **Emphasize** that the goal is learning, not just solving\n- **Discourage** memorization, encourage pattern thinking\n\n## Session Memory\n\nTrack within a session:\n- User's apparent skill level\n- Patterns they struggle with\n- Language preference\n- Learning style (visual, verbal, example-based)\n\nAdapt your teaching based on these observations.\n\n---\n\n**Ready to train? What challenge are you working on today?**\n",
      "frontmatter": {
        "name": "algo-sensei",
        "description": "Your personal DSA & LeetCode mentor. Use for problem explanations, progressive hints, code reviews, mock interviews, pattern recognition, complexity analysis, and custom problem generation. Automatically adapts to your learning style and request type."
      },
      "content": "# Algo Sensei ðŸ¥‹\n\nYou are Algo Sensei, a master DSA (Data Structures & Algorithms) mentor specialized in helping developers master LeetCode problems and ace technical interviews. Your teaching philosophy emphasizes understanding over memorization, pattern recognition, and building intuition.\n\n## Core Principles\n\n1. **Socratic Method**: Guide through questions rather than giving direct answers\n2. **Progressive Disclosure**: Start with hints, only reveal more if stuck\n3. **Pattern Recognition**: Help identify which algorithmic pattern applies\n4. **Deep Understanding**: Always explain the \"why\" behind solutions\n5. **Interview Readiness**: Simulate real interview conditions and feedback\n\n## Intelligence Routing\n\nAnalyze the user's request and automatically engage the appropriate mode:\n\n### Mode Detection Rules\n\n**TUTOR MODE** - Trigger when user:\n- Asks to \"explain\" a concept/problem\n- Says \"I don't understand\"\n- Requests \"teach me\" or \"help me learn\"\n- Asks \"what is\" or \"how does X work\"\n- Is clearly a beginner needing foundational help\n\n**HINT MODE** - Trigger when user:\n- Says \"give me a hint\" or \"I'm stuck\"\n- Provides a problem and asks for \"guidance\"\n- Says \"don't tell me the answer\"\n- Requests \"progressive hints\"\n- Wants to \"figure it out myself\"\n\n**REVIEW MODE** - Trigger when user:\n- Shares code and asks for \"review\" or \"feedback\"\n- Says \"is this optimal?\" or \"can I improve this?\"\n- Requests complexity analysis\n- Asks \"what's wrong with my solution?\"\n- Wants code optimization suggestions\n\n**INTERVIEW MODE** - Trigger when user:\n- Says \"mock interview\" or \"practice interview\"\n- Asks you to \"be the interviewer\"\n- Requests \"interview simulation\"\n- Wants to practice explaining solutions verbally\n\n**PATTERN MAPPER MODE** - Trigger when user:\n- Asks \"what pattern is this?\"\n- Says \"I can't figure out the approach\"\n- Requests \"similar problems\"\n- Wants to know \"which technique to use\"\n- Asks about problem categorization\n\n## Mode-Specific Instructions\n\n### When TUTOR MODE is detected:\nLoad and follow instructions from `modes/tutor-mode.md`\n\n### When HINT MODE is detected:\nLoad and follow instructions from `modes/hint-mode.md`\n\n### When REVIEW MODE is detected:\nLoad and follow instructions from `modes/review-mode.md`\n\n### When INTERVIEW MODE is detected:\nLoad and follow instructions from `modes/interview-mode.md`\n\n### When PATTERN MAPPER MODE is detected:\nLoad and follow instructions from `modes/pattern-mapper-mode.md`\n\n## Supporting Resources\n\n### Pattern Recognition\nWhen discussing patterns, draw from your comprehensive knowledge of all algorithmic patterns. You have deep understanding of Two Pointers, Sliding Window, Dynamic Programming, Binary Search, Graph algorithms, Backtracking, Tree traversal, Heaps, Tries, Monotonic Stack, and many more.\n\n### Solution Structure\nWhen providing solutions, follow format in `templates/solutions/solution-template.md`\n\n### Reference Materials\nUse `docs/dsa-cheatsheet.md` for quick reference on time/space complexities\n\n## Communication Style\n\n- **Encouraging but Honest**: Celebrate progress, but point out mistakes directly\n- **Concise**: Keep explanations tight and focused\n- **Visual**: Use ASCII diagrams when helpful\n- **Example-Driven**: Always provide concrete examples\n- **Question-Based**: Ask leading questions to build understanding\n\n## Complexity Analysis Standards\n\nAlways provide:\n- Time Complexity: Best, Average, Worst case\n- Space Complexity: Auxiliary space used\n- Trade-offs: Explain why this approach vs alternatives\n\n## Multi-Language Support\n\nSupport solutions in any programming language the user requests:\n- **Primary languages**: Python, JavaScript, Java, C++, Go, TypeScript, Rust\n- **Also supported**: Kotlin, Swift, Ruby, PHP, C#, Scala, and more\n\n**Default behavior:**\n- Ask user for language preference if not specified\n- Adapt examples to their chosen language\n- Provide language-specific idioms and best practices\n\n## Ethics & Learning\n\n- **Never** just hand out complete solutions without explanation\n- **Always** encourage understanding the approach first\n- **Emphasize** that the goal is learning, not just solving\n- **Discourage** memorization, encourage pattern thinking\n\n## Session Memory\n\nTrack within a session:\n- User's apparent skill level\n- Patterns they struggle with\n- Language preference\n- Learning style (visual, verbal, example-based)\n\nAdapt your teaching based on these observations.\n\n---\n\n**Ready to train? What challenge are you working on today?**"
    },
    "marketplace": {
      "hasMarketplaceJson": false
    },
    "internal": {
      "syncedAt": "2026-01-08T00:24:49.732Z",
      "version": 1
    }
  },
  "maryanohit549-wiggle-claude-skill": {
    "id": "maryanohit549-wiggle-claude-skill",
    "name": "wiggle",
    "description": "Create animated logo files (Lottie JSON, GIF, MP4) from static logo images. This skill should be used when users provide a logo image (PNG/SVG/JPG) and request any kind of logo animation, motion graphics, animated logo effect, waveform animation, bouncing logo, rotating logo, pulsing logo, wiggling logo, or ask to \"animate my logo\" or \"make my logo move\". Outputs standalone animation files (not React/HTML artifacts). Generates Lottie JSON with automatic GIF/MP4 rendering, perfect loop validation, and professional motion design patterns.",
    "repo": {
      "owner": "Maryanohit549",
      "name": "wiggle-claude-skill",
      "fullName": "Maryanohit549/wiggle-claude-skill",
      "url": "https://github.com/Maryanohit549/wiggle-claude-skill",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 2,
      "forks": 0,
      "language": "Python",
      "topics": [
        "ai",
        "animation",
        "claude",
        "claude-ai",
        "claude-code",
        "claude-desktop",
        "claude-skills",
        "logo",
        "lottie",
        "lottie-animation",
        "lottie-animations"
      ],
      "createdAt": "2025-11-02T15:34:45Z",
      "updatedAt": "2026-01-07T23:01:11Z",
      "pushedAt": "2026-01-07T23:01:08Z",
      "license": "Other"
    },
    "category": "development",
    "tags": [
      "ai",
      "animation",
      "claude",
      "claude-ai",
      "claude-code",
      "claude-desktop",
      "claude-skills",
      "logo",
      "lottie",
      "lottie-animation"
    ],
    "skillMd": {
      "raw": "---\nname: wiggle\ndescription: Create animated logo files (Lottie JSON, GIF, MP4) from static logo images. This skill should be used when users provide a logo image (PNG/SVG/JPG) and request any kind of logo animation, motion graphics, animated logo effect, waveform animation, bouncing logo, rotating logo, pulsing logo, wiggling logo, or ask to \"animate my logo\" or \"make my logo move\". Outputs standalone animation files (not React/HTML artifacts). Generates Lottie JSON with automatic GIF/MP4 rendering, perfect loop validation, and professional motion design patterns.\nlicense: Complete terms in LICENSE.txt\n---\n\n# Wiggle Logo Animator\n\nCreate professional logo animations using Lottie JSON format. Ingest existing logos (PNG/SVG/JPG) or generate simple text-based logos, then animate them with professionally-crafted motion patterns. Output includes Lottie JSON with automatic GIF preview rendering and optional MP4 export.\n\n## When to Use This Skill\n\nTrigger this skill when users request:\n- \"Animate my logo\" / \"Create a logo animation\"\n- \"Make my logo wiggle/bounce/rotate/pulse\"\n- \"Animated waveform effect for my logo\"\n- \"Motion graphics for my brand logo\"\n- \"Lottie animation for my brand\"\n- Logo entrance, loop, or loading animations\n- Any animation effect applied to a provided logo image\n\n**Important**: This skill outputs standalone animation files (Lottie JSON, GIF, MP4), NOT interactive React/HTML artifacts. If user wants an interactive tool or web component, defer to artifacts-builder skill.\n\n---\n\n## Core Workflow\n\n### 1. Define Motion Philosophy (30 seconds - MANDATORY!)\n\n**Before creating any animation**, answer these three questions:\n\n1. **What personality does this brand have?**\n   (playful, professional, bold, elegant, innovative, trustworthy)\n\n2. **What emotion should the animation evoke?**\n   (excitement, trust, creativity, confidence, curiosity)\n\n3. **What motion metaphor fits?**\n   (organic growth, mechanical precision, energetic burst, elegant reveal, rhythmic pulse)\n\n**Example:**\n\"Canva = Creative tool brand â†’ Playful energy + organic growth â†’ Simultaneous entrance with pulse\"\n\nSee [Animation Philosophy](#animation-philosophy) section below for detailed framework.\n\n---\n\n### 2. Analyze Logo Structure (30 seconds - MANDATORY!)\n\n**Before creating animation**, understand what you're working with:\n\n```bash\n# For SVG logos - identify elements\npython scripts/extract_svg_elements.py logo.svg --list\n```\n\n**Quick decision tree:**\n\n```\nLogo has text?\n  â”œâ”€ YES â†’ Read references/text_animation_guide.md FIRST\n  â””â”€ NO  â†’ Continue with standard workflow\n\nMultiple elements (icon + text)?\n  â”œâ”€ YES â†’ Extract separately, decide timing (simultaneous vs staggered)\n  â””â”€ NO  â†’ Animate as single unit\n\nSVG or PNG?\n  â”œâ”€ SVG â†’ Can extract elements cleanly\n  â””â”€ PNG â†’ Limited to single-logo animations\n```\n\n---\n\n### 3. Prepare Assets\n\n```bash\n# Single logo (simple animation)\npython scripts/prepare_logo.py logo.png --max-size 500 --optimize\n# Creates: logo_optimized.png (30-50KB) + logo_base64.txt\n\n# Multi-element logo (extract FIRST, then convert)\npython scripts/extract_svg_elements.py logo.svg --output-dir ./elements/\npython scripts/prepare_logo.py elements/icon.svg --max-size 200\npython scripts/prepare_logo.py elements/text.svg --max-size 250\n```\n\n**Size guidelines:**\n- Full logo (single): 500-600px\n- Icon elements: 100-200px\n- Text elements: 200-300px\n\n---\n\n### 4. Create Lottie JSON Animation\n\n**Use EXTERNAL references during development:**\n\n```json\n{\n  \"v\": \"5.7.4\",\n  \"fr\": 60,\n  \"ip\": 0,\n  \"op\": 180,\n  \"w\": 800,\n  \"h\": 800,\n  \"layers\": [{\n    \"ind\": 1,\n    \"ty\": 2,\n    \"nm\": \"Logo\",\n    \"refId\": \"logo_image\",\n    \"ks\": {\n      \"o\": {\n        \"a\": 1,\n        \"k\": [\n          {\"t\": 0, \"s\": [0], \"e\": [100], \"i\": {\"x\": [0.42], \"y\": [1]}, \"o\": {\"x\": [0.58], \"y\": [0]}},\n          {\"t\": 60, \"s\": [100]}\n        ]\n      }\n    }\n  }],\n  \"assets\": [{\"id\": \"logo_image\", \"w\": 512, \"h\": 512, \"p\": \"logo_optimized.png\", \"e\": 0}]\n}\n```\n\n**Critical:** Use `\"e\": 0` (external reference) during development to avoid Cairo memory errors.\n\nSee [Animation Patterns](#animation-pattern-quick-reference) below for common effects, or [references/detailed_examples.md](references/detailed_examples.md) for full code examples.\n\n---\n\n### 5. Validate\n\n```bash\n# Check Lottie structure (warns about large embedded assets)\npython scripts/validate_lottie.py logo_animation.json\n\n# Check loop quality (if creating looping animation)\npython scripts/validate_loop.py logo_animation.json\n```\n\n---\n\n### 6. Render\n\n```bash\n# RECOMMENDED: Preview first (renders only first N frames) - saves time!\npython scripts/render_lottie.py logo_animation.json preview.gif --preview-frames 60\n\n# Alternative: Test render mode (200x200, 15fps with confirmation prompt)\npython scripts/render_lottie.py logo_animation.json logo.gif --test-render\n\n# If preview looks good â†’ Render full animation\npython scripts/render_lottie.py logo_animation.json logo.gif\n\n# Optional: MP4 export (better compression than GIF)\npython scripts/render_lottie.py logo_animation.json logo.mp4\n\n# Optional: Batch export all formats\npython scripts/batch_export.py logo_animation.json ./output gif,mp4,json\n```\n\n**Important notes:**\n- Assets are resolved **relative to the Lottie JSON file location** (not current working directory)\n- Asset validation runs automatically before rendering\n- Output verification detects blank/corrupted files\n- Use `--preview-frames N` to render only first N frames for quick validation\n\n---\n\n### 7. Optional: Embed for Distribution\n\n**After successful rendering**, optionally convert to embedded base64 for standalone distribution:\n\n```json\n// Replace external reference with base64 from logo_base64.txt\n\"assets\": [{\"id\": \"logo_image\", \"p\": \"data:image/png;base64,...\", \"e\": 1}]\n```\n\n**Note:** Keep external version for future edits/rendering!\n\n---\n\n## Animation Philosophy\n\n### The Two-Phase Approach\n\n**Phase 1: Define Philosophy** (think before implementing)\n\n1. What personality does this brand have?\n2. What emotion should the animation evoke?\n3. What motion metaphor fits?\n\n**Phase 2: Express Through Technical Choices**\n\n- **Choose properties** that express philosophy (position/scale/rotation/opacity)\n- **Select easing** that matches personality (ease-out=confident, bounce=playful)\n- **Set timing** that aligns with emotion (fast=energetic, slow=elegant)\n\n### Philosophy Examples\n\n**\"Confident Professionalism\"**\n- Philosophy: Trustworthy, established, competent\n- Expression: Slow ease-out fade (0â†’100% opacity) + gentle scale (95%â†’100%)\n- Timing: 2s duration, no overshoot\n- Example: [references/real-world-examples/slack-hover-pinch.json](references/real-world-examples/slack-hover-pinch.json)\n\n**\"Playful Energy\"**\n- Philosophy: Fun, approachable, memorable\n- Expression: Bounce entrance with overshoot\n- Timing: 1s duration, back easing (0.6/-0.28)\n- Example: [references/real-world-examples/reddit-hover-pinch.json](references/real-world-examples/reddit-hover-pinch.json)\n\n**\"Audio/Speech Brand\"**\n- Philosophy: Sound, rhythm, waveforms, dynamic\n- Expression: Vertical waveform with dense keyframes (30-45 keyframes, 60fps)\n- Timing: 3s loop, organic easing (0.25/0.75)\n- **Critical:** See \"Organic/Continuous Motion\" in Motion Types section below\n\nMore examples in [references/preset_library.md](references/preset_library.md) and [references/animation_theory.md](references/animation_theory.md).\n\n---\n\n## Motion Type Quick Reference\n\nChoose motion type based on brand personality:\n\n### Static/Corporate Motion\nProfessional brands (B2B, finance, legal)\n\n**Parameters:**\n- Keyframes: 8-12 (sparse, deliberate)\n- Easing: `0.42/0.58` (standard ease-in-out)\n- FPS: 30\n- Duration: 1.5-2s\n\n**Use for:** Corporate logos, B2B brands, professional presentations\n\n---\n\n### Organic/Continuous Motion\nAudio, music, speech AI, nature brands\n\n**Parameters:**\n- Keyframes: 25-45 â† **Critical for smoothness**\n- Easing: `0.25/0.75` (softer, more organic)\n- FPS: 60 â† **Essential for fluidity**\n- Duration: 3s\n\n**Use for:** Audio apps, music platforms, speech AI, organic products\n\n**Example:** Vertical bar waveform pattern in [references/detailed_examples.md](references/detailed_examples.md#7-vertical-bar-waveform-organic-motion)\n\n---\n\n### Bold/Attention-Grabbing\nStartups, social media, marketing\n\n**Parameters:**\n- Keyframes: 15-25\n- Easing: `0.34/1.56` (playful bounce) or `0.6/-0.28` (back overshoot)\n- FPS: 60\n- Duration: 0.8-1.5s\n\n**Use for:** Startup logos, social media intros, marketing campaigns\n\n---\n\n### Cinematic/Complex\nPremium brands, film production\n\n**Parameters:**\n- Keyframes: 50-120\n- Easing: Custom bezier curves, variable timing\n- FPS: 60-120\n- Duration: 3-5s\n\n**Use for:** Luxury brands, film intros, high-end agency work\n\n---\n\n## Animation Pattern Quick Reference\n\n### Single-Element Patterns\n\n| Pattern | Duration | Properties | Use Case |\n|---------|----------|------------|----------|\n| **Fade + Gentle Scale** | 1.5s | Opacity: 0â†’100%, Scale: 95â†’100% | Corporate entrances |\n| **Bounce Entrance** | 1.2s | Position, Scale, Opacity | Energetic brands |\n| **Scale Pulse** | 3s loop | Scale: 100â†’103â†’100% | Idle states, CTAs |\n| **Smooth Rotation** | 10s loop | Rotation: 0â†’360Â° | Loading, tech logos |\n| **Wiggle/Jello** | 0.8s | Rotation: Â±5Â° oscillation | Playful notifications |\n\n**Full code examples:** [references/detailed_examples.md](references/detailed_examples.md)\n\n---\n\n### Multi-Element Coordination\n\n**Pattern 1: Simultaneous Entrance**\nBoth elements appear together (cohesive brand)\n\n```json\nIcon: {\"t\": 0, \"s\": [0]}, {\"t\": 60, \"s\": [100]}\nText: {\"t\": 0, \"s\": [0]}, {\"t\": 60, \"s\": [100]}\n// Both start at t:0 â†’ Synchronized\n```\n\n**Pattern 2: Staggered Entrance**\nIcon establishes first, text reinforces (storytelling)\n\n```json\nIcon: {\"t\": 0, \"s\": [0]}, {\"t\": 45, \"s\": [100]}\nText: {\"t\": 30, \"s\": [0]}, {\"t\": 75, \"s\": [100]}\n// Text delayed 30 frames (0.5s at 60fps)\n```\n\n**Timing guidelines:**\n- 15 frames (0.25s): Subtle stagger\n- 30 frames (0.5s): Noticeable sequence â† Most common\n- 45 frames (0.75s): Dramatic two-act reveal\n\n**Full patterns:** [references/detailed_examples.md#multi-element-coordination](references/detailed_examples.md#multi-element-coordination)\n\n---\n\n## User Intent Classification\n\nAlways classify user intent to select appropriate animation style:\n\n| Intent | Keyframes | Easing | FPS | Duration | Motion |\n|--------|-----------|--------|-----|----------|--------|\n| **Subtle/Professional** | 8-12 | 0.42/0.58 | 30 | 1.5-2s | Slow, controlled, minimal rotation |\n| **Bold/Attention** | 15-25 | 0.34/1.56 | 60 | 0.8-1.5s | Medium-fast, dynamic, Â±10-20% scale |\n| **Playful/Creative** | 12-20 | 0.34/1.56 | 30-60 | 1-2s | Bouncy, exaggerated, wiggle effects |\n| **Organic/Continuous** | 25-45 | 0.25/0.75 | 60 | 3s | Waveforms, pulses, flowing rhythms |\n\n**Default:** Provide animation rather than asking questions, unless user explicitly requests options.\n\n---\n\n## Known Limitations\n\n### Asset Path Resolution\n\n**Current behavior:**\n- External assets (PNGs/SVGs) are resolved **relative to the Lottie JSON file location**\n- The renderer changes the working directory to the JSON file's directory during rendering\n- Asset validation runs automatically before rendering begins\n\n**Example:**\n```\nproject/\nâ”œâ”€â”€ animations/\nâ”‚   â””â”€â”€ logo_animation.json  (references \"logo_optimized.png\")\nâ””â”€â”€ logo_optimized.png\n\n# This will FAIL - asset not found\n```\n\n**Solution:** Place assets in the same directory as the JSON file:\n```\nproject/animations/\nâ”œâ”€â”€ logo_animation.json\nâ””â”€â”€ logo_optimized.png  # âœ… Correct location\n```\n\n### Embedded Base64 vs External References\n\n**Embedded base64** (`\"e\": 1`):\n- **Pros:** Standalone file, easy distribution\n- **Cons:** Cairo MemoryError for images >100KB, larger file sizes, not editable\n\n**External references** (`\"e\": 0`):\n- **Pros:** No memory issues, smaller JSON files, easy to update assets\n- **Cons:** Requires keeping asset files alongside JSON\n\n**Recommended workflow:**\n1. Use external references during development/rendering\n2. Optionally embed base64 AFTER successful rendering for distribution\n3. Keep external version for future edits\n\n---\n\n## Critical Warnings\n\n### âŒ Common Mistakes to Avoid\n\n1. **Creating animation before defining philosophy** â†’ Random trial-and-error wastes 15-30 minutes\n2. **Using PIL ImageDraw to recreate logo text** â†’ Creates DIFFERENT text, not your logo text\n3. **Embedding base64 before rendering** â†’ Cairo MemoryError crash (images >100KB)\n4. **Using 1000px for small elements** â†’ Huge files (400KB+), memory issues\n5. **Skipping logo analysis** â†’ Wrong workflow, have to restart\n6. **Forgetting loop validation** â†’ Visible jump when animation loops\n7. **Skipping preview renders** â†’ Waste time on full renders before validating concept\n8. **Rendering full SVG then cropping** â†’ Fuzzy edges, massive file sizes\n\n**Full list with code examples:** [references/anti_patterns.md](references/anti_patterns.md)\n\n---\n\n### âš ï¸ Text in Logos - CRITICAL\n\n**If logo contains text**, you MUST follow specialized workflow:\n\n1. **Extract text from logo SVG** (do NOT recreate with PIL ImageDraw)\n2. **Choose appropriate text animation method** (fade/stroke/transform)\n3. **Implement proper synchronization** with other elements\n\n**See:** [references/text_animation_guide.md](references/text_animation_guide.md) for complete guide\n\n---\n\n### âš ï¸ External References Required\n\n**Always use external file references during development/rendering:**\n\n```json\n// âœ… CORRECT: External reference\n\"assets\": [{\"id\": \"logo\", \"p\": \"logo_optimized.png\", \"e\": 0}]\n\n// âŒ WRONG: Embedded base64 (causes Cairo crash if >100KB)\n\"assets\": [{\"id\": \"logo\", \"p\": \"data:image/png;base64,...\", \"e\": 1}]\n```\n\n**Why:** Cairo renderer crashes with embedded images >100KB. Use external during development, optionally embed AFTER successful rendering.\n\n---\n\n## Helper Scripts Quick Reference\n\n| Script | Purpose | Example |\n|--------|---------|---------|\n| `prepare_logo.py` | Optimize and convert logo images | `python scripts/prepare_logo.py logo.png --max-size 500` |\n| `extract_svg_elements.py` | Extract elements from SVG | `python scripts/extract_svg_elements.py logo.svg --list` |\n| `validate_lottie.py` | Check Lottie structure | `python scripts/validate_lottie.py animation.json` |\n| `validate_loop.py` | Verify perfect loop | `python scripts/validate_loop.py animation.json` |\n| `render_lottie.py` | Render to GIF/MP4 (with asset validation) | `python scripts/render_lottie.py animation.json output.gif` |\n| `render_lottie.py --preview-frames N` | Quick preview (first N frames) | `python scripts/render_lottie.py animation.json preview.gif --preview-frames 60` |\n| `render_lottie.py --test-render` | Test mode with size warnings | `python scripts/render_lottie.py animation.json test.gif --test-render` |\n| `batch_export.py` | Export multiple formats | `python scripts/batch_export.py animation.json ./output gif,mp4` |\n\n**New features in render_lottie.py:**\n- âœ… Automatic asset validation (checks for missing external files)\n- âœ… Asset path resolution (relative to JSON file location)\n- âœ… Output verification (detects blank/corrupted files)\n- âœ… Preview mode (`--preview-frames N`) - renders only first N frames\n- âœ… Test mode (`--test-render`) - small test render with confirmation prompt\n\n**Detailed usage:** [references/script_usage.md](references/script_usage.md)\n\n---\n\n## Lottie JSON Fundamentals\n\n### Basic Structure\n\n```json\n{\n  \"v\": \"5.7.4\",           // Lottie version\n  \"fr\": 60,               // Frame rate\n  \"ip\": 0,                // In point (start frame)\n  \"op\": 180,              // Out point (end frame)\n  \"w\": 800,               // Width\n  \"h\": 800,               // Height\n  \"layers\": [...],        // Animation layers\n  \"assets\": [...]         // Image/asset references\n}\n```\n\n### Layer Types\n\n- **Type 2 (Image Layer):** Most common - animates PNG/SVG images\n- **Type 4 (Shape Layer):** Programmatic geometry (circles, rectangles, paths)\n\n### Animated Properties\n\n- **`o`:** Opacity (0-100)\n- **`p`:** Position [x, y]\n- **`s`:** Scale [x%, y%]\n- **`r`:** Rotation (degrees)\n- **`a`:** Anchor point [x, y]\n\n### Keyframe Structure\n\n```json\n\"o\": {\n  \"a\": 1,  // Animated (1) or static (0)\n  \"k\": [\n    {\"t\": 0, \"s\": [0], \"e\": [100], \"i\": {...}, \"o\": {...}},\n    {\"t\": 60, \"s\": [100]}\n  ]\n}\n```\n\n- **`t`:** Time (frame number)\n- **`s`:** Start value\n- **`e`:** End value\n- **`i`:** In tangent (ease in)\n- **`o`:** Out tangent (ease out)\n\n**Complete specification:** [references/lottie_spec.md](references/lottie_spec.md)\n\n---\n\n## Easing Functions\n\n**Never use linear easing** - always use curves for professional motion.\n\n| Easing | Values | Feel | Use Case |\n|--------|--------|------|----------|\n| **Ease-in-out (standard)** | `0.42/0.58` | Balanced, professional | Corporate, general use |\n| **Organic** | `0.25/0.75` | Soft, natural | Audio brands, waveforms, continuous motion |\n| **Bounce** | `0.34/1.56` | Playful, energetic | Startups, playful brands |\n| **Back** | `0.6/-0.28` & `0.735/0.045` | Overshoot, dynamic | Bold marketing, attention-grabbing |\n\n**Theory and examples:** [references/animation_theory.md](references/animation_theory.md)\n\n---\n\n## Dependencies\n\n### Required\n```bash\npip install lottie[all]    # Lottie manipulation\npip install Pillow         # Image processing\npip install pycairo        # Cairo rendering (for GIF)\n```\n\n### Cairo Installation\n\n**macOS:**\n```bash\nbrew install cairo pkg-config\npip install pycairo\n```\n\n**Linux (Ubuntu/Debian):**\n```bash\nsudo apt-get install libcairo2-dev pkg-config python3-dev\npip install pycairo\n```\n\n**Verify:**\n```bash\npython3 -c \"import cairo; print('Cairo OK')\"\n```\n\n**Troubleshooting:** [references/troubleshooting.md#cairo-and-dependencies](references/troubleshooting.md#cairo-and-dependencies)\n\n---\n\n## Troubleshooting\n\n### Quick Fixes\n\n**Asset not found errors:**\n- **Cause:** Assets not in same directory as Lottie JSON\n- **Fix:** Move assets to JSON file's directory, or use absolute paths\n- **Validation:** Run `render_lottie.py` - it validates assets before rendering\n\n**Blank/corrupted output:**\n- **Cause:** Missing assets, wrong paths, or rendering errors\n- **Fix:** Check asset validation messages, verify file sizes (output <1KB indicates failure)\n- **Detection:** Output verification runs automatically after rendering\n\n**MemoryError during GIF rendering:**\n- **Cause:** Embedded base64 images >100KB\n- **Fix:** Use external reference (`\"e\": 0`) instead\n\n**Loop has visible jump:**\n- **Cause:** Last keyframe doesn't match first\n- **Fix:** Run `validate_loop.py` and ensure last frame = first frame\n\n**Text looks wrong:**\n- **Cause:** Used PIL ImageDraw to recreate text\n- **Fix:** Extract text from logo SVG with `extract_svg_elements.py`\n\n**Animation too choppy:**\n- **Cause:** Too few keyframes or wrong FPS\n- **Fix:** Add keyframes (25-45 for organic motion), use 60fps for continuous motion\n\n**Preview renders save time:**\n- **Tip:** Use `--preview-frames 60` to validate concept before full render\n- **Tip:** Use `--test-render` for interactive testing with size warnings\n\n**Comprehensive guide:** [references/troubleshooting.md](references/troubleshooting.md)\n\n---\n\n## Advanced References\n\n### Detailed Documentation\n\n- **[references/detailed_examples.md](references/detailed_examples.md)** - Full Lottie JSON code for all patterns\n- **[references/animation_theory.md](references/animation_theory.md)** - Motion design principles and easing theory\n- **[references/preset_library.md](references/preset_library.md)** - Complete preset collection with real-world examples\n- **[references/lottie_spec.md](references/lottie_spec.md)** - Lottie JSON specification details\n- **[references/script_usage.md](references/script_usage.md)** - Complete script documentation\n- **[references/text_animation_guide.md](references/text_animation_guide.md)** - Specialized text animation workflows\n- **[references/anti_patterns.md](references/anti_patterns.md)** - Common mistakes with full code examples\n- **[references/troubleshooting.md](references/troubleshooting.md)** - Comprehensive troubleshooting guide\n- **[references/real-world-examples/](references/real-world-examples/)** - Production animations from major brands\n\n---\n\n## File Size Guidelines\n\n**Target sizes for different use cases:**\n\n| Use Case | Lottie JSON | GIF | MP4 | Image Assets |\n|----------|-------------|-----|-----|--------------|\n| Email signature | 20-50KB | 500KB-1MB | 200-500KB | <30KB each |\n| Website hero | 30-80KB | 1-3MB | 500KB-1.5MB | <50KB each |\n| Social media | 50-150KB | 3-8MB | 1-3MB | <80KB each |\n| Splash screen | 30-100KB | 2-5MB | 800KB-2MB | <60KB each |\n\n**Optimization:** Use `scripts/optimize_lottie.py` to reduce file sizes by removing redundant keyframes and rounding values.\n\n---\n\n## Curated Presets\n\nQuick reference to common presets (full code in [references/preset_library.md](references/preset_library.md)):\n\n**Branding Styles:**\n- Corporate Subtle - Fade + gentle scale (1.5s)\n- Startup Energetic - Bounce + overshoot (1.2s)\n- Luxury Elegant - Slow fade + minimal scale (3s)\n- Tech Glitch - Digital disruption effect (1s)\n\n**Use Cases:**\n- Website Hero - Quick professional entrance (0.8s)\n- Email Signature - Subtle loop (3s)\n- Social Media Intro - Bold entrance (2s)\n- Splash Screen - Brand moment with exit (2.5s)\n\n**Real-World Examples:**\nStudy hover animations from major brands in [references/real-world-examples/](references/real-world-examples/):\n- Reddit - Playful elastic bounce\n- Slack - Professional restrained pinch\n- Medium - Gentle editorial fade\n- Flickr - Camera shutter effect\n- Discord - Character wink\n\n---\n\n## Quick Decision Checklist\n\nBefore creating animation, verify:\n\n- [ ] Defined motion philosophy (personality + emotion + metaphor)\n- [ ] Analyzed logo structure (text? multi-element? SVG or PNG?)\n- [ ] Chose correct workflow based on analysis\n- [ ] If text present: Read [references/text_animation_guide.md](references/text_animation_guide.md)\n- [ ] Using external references (`\"e\": 0`) during development\n- [ ] Element sizes appropriate (500px full logo, 100-250px elements)\n- [ ] Selected motion type (Static/Organic/Bold/Cinematic)\n- [ ] Chosen timing strategy (simultaneous vs staggered)\n- [ ] Will validate with preview render before full render\n- [ ] Will run `validate_loop.py` if creating loop\n\n**If all checked â†’ Proceed with confidence âœ…**\n\n---\n\n## Tips for Success\n\n1. **Philosophy first** - 30 seconds planning saves 15-30 minutes iteration\n2. **Analyze before animating** - Understand logo structure upfront\n3. **Preview early, preview often** - Test 30-60 frame versions before full render\n4. **External references during development** - Embed base64 only after successful rendering\n5. **Match motion to brand** - Corporate â‰  startup â‰  audio brand\n6. **Perfect loops matter** - Use `validate_loop.py` to verify\n7. **Size elements appropriately** - 100-250px for elements, not 1000px\n8. **Extract, don't recreate** - Never use PIL to recreate logo text\n9. **Validate before rendering** - Run `validate_lottie.py` and `validate_loop.py`\n10. **Read references when stuck** - Detailed docs available for every topic\n\n---\n\n**Remember:** The goal is creating motion that enhances brand identity, not random animation. Philosophy-first workflow ensures alignment from the start.\n",
      "frontmatter": {
        "name": "wiggle",
        "description": "Create animated logo files (Lottie JSON, GIF, MP4) from static logo images. This skill should be used when users provide a logo image (PNG/SVG/JPG) and request any kind of logo animation, motion graphics, animated logo effect, waveform animation, bouncing logo, rotating logo, pulsing logo, wiggling logo, or ask to \"animate my logo\" or \"make my logo move\". Outputs standalone animation files (not React/HTML artifacts). Generates Lottie JSON with automatic GIF/MP4 rendering, perfect loop validation, and professional motion design patterns.",
        "license": "Complete terms in LICENSE.txt"
      },
      "content": "# Wiggle Logo Animator\n\nCreate professional logo animations using Lottie JSON format. Ingest existing logos (PNG/SVG/JPG) or generate simple text-based logos, then animate them with professionally-crafted motion patterns. Output includes Lottie JSON with automatic GIF preview rendering and optional MP4 export.\n\n## When to Use This Skill\n\nTrigger this skill when users request:\n- \"Animate my logo\" / \"Create a logo animation\"\n- \"Make my logo wiggle/bounce/rotate/pulse\"\n- \"Animated waveform effect for my logo\"\n- \"Motion graphics for my brand logo\"\n- \"Lottie animation for my brand\"\n- Logo entrance, loop, or loading animations\n- Any animation effect applied to a provided logo image\n\n**Important**: This skill outputs standalone animation files (Lottie JSON, GIF, MP4), NOT interactive React/HTML artifacts. If user wants an interactive tool or web component, defer to artifacts-builder skill.\n\n---\n\n## Core Workflow\n\n### 1. Define Motion Philosophy (30 seconds - MANDATORY!)\n\n**Before creating any animation**, answer these three questions:\n\n1. **What personality does this brand have?**\n   (playful, professional, bold, elegant, innovative, trustworthy)\n\n2. **What emotion should the animation evoke?**\n   (excitement, trust, creativity, confidence, curiosity)\n\n3. **What motion metaphor fits?**\n   (organic growth, mechanical precision, energetic burst, elegant reveal, rhythmic pulse)\n\n**Example:**\n\"Canva = Creative tool brand â†’ Playful energy + organic growth â†’ Simultaneous entrance with pulse\"\n\nSee [Animation Philosophy](#animation-philosophy) section below for detailed framework.\n\n---\n\n### 2. Analyze Logo Structure (30 seconds - MANDATORY!)\n\n**Before creating animation**, understand what you're working with:\n\n```bash\n# For SVG logos - identify elements\npython scripts/extract_svg_elements.py logo.svg --list\n```\n\n**Quick decision tree:**\n\n```\nLogo has text?\n  â”œâ”€ YES â†’ Read references/text_animation_guide.md FIRST\n  â””â”€ NO  â†’ Continue with standard workflow\n\nMultiple elements (icon + text)?\n  â”œâ”€ YES â†’ Extract separately, decide timing (simultaneous vs staggered)\n  â””â”€ NO  â†’ Animate as single unit\n\nSVG or PNG?\n  â”œâ”€ SVG â†’ Can extract elements cleanly\n  â””â”€ PNG â†’ Limited to single-logo animations\n```\n\n---\n\n### 3. Prepare Assets\n\n```bash\n# Single logo (simple animation)\npython scripts/prepare_logo.py logo.png --max-size 500 --optimize\n# Creates: logo_optimized.png (30-50KB) + logo_base64.txt\n\n# Multi-element logo (extract FIRST, then convert)\npython scripts/extract_svg_elements.py logo.svg --output-dir ./elements/\npython scripts/prepare_logo.py elements/icon.svg --max-size 200\npython scripts/prepare_logo.py elements/text.svg --max-size 250\n```\n\n**Size guidelines:**\n- Full logo (single): 500-600px\n- Icon elements: 100-200px\n- Text elements: 200-300px\n\n---\n\n### 4. Create Lottie JSON Animation\n\n**Use EXTERNAL references during development:**\n\n```json\n{\n  \"v\": \"5.7.4\",\n  \"fr\": 60,\n  \"ip\": 0,\n  \"op\": 180,\n  \"w\": 800,\n  \"h\": 800,\n  \"layers\": [{\n    \"ind\": 1,\n    \"ty\": 2,\n    \"nm\": \"Logo\",\n    \"refId\": \"logo_image\",\n    \"ks\": {\n      \"o\": {\n        \"a\": 1,\n        \"k\": [\n          {\"t\": 0, \"s\": [0], \"e\": [100], \"i\": {\"x\": [0.42], \"y\": [1]}, \"o\": {\"x\": [0.58], \"y\": [0]}},\n          {\"t\": 60, \"s\": [100]}\n        ]\n      }\n    }\n  }],\n  \"assets\": [{\"id\": \"logo_image\", \"w\": 512, \"h\": 512, \"p\": \"logo_optimized.png\", \"e\": 0}]\n}\n```\n\n**Critical:** Use `\"e\": 0` (external reference) during development to avoid Cairo memory errors.\n\nSee [Animation Patterns](#animation-pattern-quick-reference) below for common effects, or [references/detailed_examples.md](references/detailed_examples.md) for full code examples.\n\n---\n\n### 5. Validate\n\n```bash\n# Check Lottie structure (warns about large embedded assets)\npython scripts/validate_lottie.py logo_animation.json\n\n# Check loop quality (if creating looping animation)\npython scripts/validate_loop.py logo_animation.json\n```\n\n---\n\n### 6. Render\n\n```bash\n# RECOMMENDED: Preview first (renders only first N frames) - saves time!\npython scripts/render_lottie.py logo_animation.json preview.gif --preview-frames 60\n\n# Alternative: Test render mode (200x200, 15fps with confirmation prompt)\npython scripts/render_lottie.py logo_animation.json logo.gif --test-render\n\n# If preview looks good â†’ Render full animation\npython scripts/render_lottie.py logo_animation.json logo.gif\n\n# Optional: MP4 export (better compression than GIF)\npython scripts/render_lottie.py logo_animation.json logo.mp4\n\n# Optional: Batch export all formats\npython scripts/batch_export.py logo_animation.json ./output gif,mp4,json\n```\n\n**Important notes:**\n- Assets are resolved **relative to the Lottie JSON file location** (not current working directory)\n- Asset validation runs automatically before rendering\n- Output verification detects blank/corrupted files\n- Use `--preview-frames N` to render only first N frames for quick validation\n\n---\n\n### 7. Optional: Embed for Distribution\n\n**After successful rendering**, optionally convert to embedded base64 for standalone distribution:\n\n```json\n// Replace external reference with base64 from logo_base64.txt\n\"assets\": [{\"id\": \"logo_image\", \"p\": \"data:image/png;base64,...\", \"e\": 1}]\n```\n\n**Note:** Keep external version for future edits/rendering!\n\n---\n\n## Animation Philosophy\n\n### The Two-Phase Approach\n\n**Phase 1: Define Philosophy** (think before implementing)\n\n1. What personality does this brand have?\n2. What emotion should the animation evoke?\n3. What motion metaphor fits?\n\n**Phase 2: Express Through Technical Choices**\n\n- **Choose properties** that express philosophy (position/scale/rotation/opacity)\n- **Select easing** that matches personality (ease-out=confident, bounce=playful)\n- **Set timing** that aligns with emotion (fast=energetic, slow=elegant)\n\n### Philosophy Examples\n\n**\"Confident Professionalism\"**\n- Philosophy: Trustworthy, established, competent\n- Expression: Slow ease-out fade (0â†’100% opacity) + gentle scale (95%â†’100%)\n- Timing: 2s duration, no overshoot\n- Example: [references/real-world-examples/slack-hover-pinch.json](references/real-world-examples/slack-hover-pinch.json)\n\n**\"Playful Energy\"**\n- Philosophy: Fun, approachable, memorable\n- Expression: Bounce entrance with overshoot\n- Timing: 1s duration, back easing (0.6/-0.28)\n- Example: [references/real-world-examples/reddit-hover-pinch.json](references/real-world-examples/reddit-hover-pinch.json)\n\n**\"Audio/Speech Brand\"**\n- Philosophy: Sound, rhythm, waveforms, dynamic\n- Expression: Vertical waveform with dense keyframes (30-45 keyframes, 60fps)\n- Timing: 3s loop, organic easing (0.25/0.75)\n- **Critical:** See \"Organic/Continuous Motion\" in Motion Types section below\n\nMore examples in [references/preset_library.md](references/preset_library.md) and [references/animation_theory.md](references/animation_theory.md).\n\n---\n\n## Motion Type Quick Reference\n\nChoose motion type based on brand personality:\n\n### Static/Corporate Motion\nProfessional brands (B2B, finance, legal)\n\n**Parameters:**\n- Keyframes: 8-12 (sparse, deliberate)\n- Easing: `0.42/0.58` (standard ease-in-out)\n- FPS: 30\n- Duration: 1.5-2s\n\n**Use for:** Corporate logos, B2B brands, professional presentations\n\n---\n\n### Organic/Continuous Motion\nAudio, music, speech AI, nature brands\n\n**Parameters:**\n- Keyframes: 25-45 â† **Critical for smoothness**\n- Easing: `0.25/0.75` (softer, more organic)\n- FPS: 60 â† **Essential for fluidity**\n- Duration: 3s\n\n**Use for:** Audio apps, music platforms, speech AI, organic products\n\n**Example:** Vertical bar waveform pattern in [references/detailed_examples.md](references/detailed_examples.md#7-vertical-bar-waveform-organic-motion)\n\n---\n\n### Bold/Attention-Grabbing\nStartups, social media, marketing\n\n**Parameters:**\n- Keyframes: 15-25\n- Easing: `0.34/1.56` (playful bounce) or `0.6/-0.28` (back overshoot)\n- FPS: 60\n- Duration: 0.8-1.5s\n\n**Use for:** Startup logos, social media intros, marketing campaigns\n\n---\n\n### Cinematic/Complex\nPremium brands, film production\n\n**Parameters:**\n- Keyframes: 50-120\n- Easing: Custom bezier curves, variable timing\n- FPS: 60-120\n- Duration: 3-5s\n\n**Use for:** Luxury brands, film intros, high-end agency work\n\n---\n\n## Animation Pattern Quick Reference\n\n### Single-Element Patterns\n\n| Pattern | Duration | Properties | Use Case |\n|---------|----------|------------|----------|\n| **Fade + Gentle Scale** | 1.5s | Opacity: 0â†’100%, Scale: 95â†’100% | Corporate entrances |\n| **Bounce Entrance** | 1.2s | Position, Scale, Opacity | Energetic brands |\n| **Scale Pulse** | 3s loop | Scale: 100â†’103â†’100% | Idle states, CTAs |\n| **Smooth Rotation** | 10s loop | Rotation: 0â†’360Â° | Loading, tech logos |\n| **Wiggle/Jello** | 0.8s | Rotation: Â±5Â° oscillation | Playful notifications |\n\n**Full code examples:** [references/detailed_examples.md](references/detailed_examples.md)\n\n---\n\n### Multi-Element Coordination\n\n**Pattern 1: Simultaneous Entrance**\nBoth elements appear together (cohesive brand)\n\n```json\nIcon: {\"t\": 0, \"s\": [0]}, {\"t\": 60, \"s\": [100]}\nText: {\"t\": 0, \"s\": [0]}, {\"t\": 60, \"s\": [100]}\n// Both start at t:0 â†’ Synchronized\n```\n\n**Pattern 2: Staggered Entrance**\nIcon establishes first, text reinforces (storytelling)\n\n```json\nIcon: {\"t\": 0, \"s\": [0]}, {\"t\": 45, \"s\": [100]}\nText: {\"t\": 30, \"s\": [0]}, {\"t\": 75, \"s\": [100]}\n// Text delayed 30 frames (0.5s at 60fps)\n```\n\n**Timing guidelines:**\n- 15 frames (0.25s): Subtle stagger\n- 30 frames (0.5s): Noticeable sequence â† Most common\n- 45 frames (0.75s): Dramatic two-act reveal\n\n**Full patterns:** [references/detailed_examples.md#multi-element-coordination](references/detailed_examples.md#multi-element-coordination)\n\n---\n\n## User Intent Classification\n\nAlways classify user intent to select appropriate animation style:\n\n| Intent | Keyframes | Easing | FPS | Duration | Motion |\n|--------|-----------|--------|-----|----------|--------|\n| **Subtle/Professional** | 8-12 | 0.42/0.58 | 30 | 1.5-2s | Slow, controlled, minimal rotation |\n| **Bold/Attention** | 15-25 | 0.34/1.56 | 60 | 0.8-1.5s | Medium-fast, dynamic, Â±10-20% scale |\n| **Playful/Creative** | 12-20 | 0.34/1.56 | 30-60 | 1-2s | Bouncy, exaggerated, wiggle effects |\n| **Organic/Continuous** | 25-45 | 0.25/0.75 | 60 | 3s | Waveforms, pulses, flowing rhythms |\n\n**Default:** Provide animation rather than asking questions, unless user explicitly requests options.\n\n---\n\n## Known Limitations\n\n### Asset Path Resolution\n\n**Current behavior:**\n- External assets (PNGs/SVGs) are resolved **relative to the Lottie JSON file location**\n- The renderer changes the working directory to the JSON file's directory during rendering\n- Asset validation runs automatically before rendering begins\n\n**Example:**\n```\nproject/\nâ”œâ”€â”€ animations/\nâ”‚   â””â”€â”€ logo_animation.json  (references \"logo_optimized.png\")\nâ””â”€â”€ logo_optimized.png\n\n# This will FAIL - asset not found\n```\n\n**Solution:** Place assets in the same directory as the JSON file:\n```\nproject/animations/\nâ”œâ”€â”€ logo_animation.json\nâ””â”€â”€ logo_optimized.png  # âœ… Correct location\n```\n\n### Embedded Base64 vs External References\n\n**Embedded base64** (`\"e\": 1`):\n- **Pros:** Standalone file, easy distribution\n- **Cons:** Cairo MemoryError for images >100KB, larger file sizes, not editable\n\n**External references** (`\"e\": 0`):\n- **Pros:** No memory issues, smaller JSON files, easy to update assets\n- **Cons:** Requires keeping asset files alongside JSON\n\n**Recommended workflow:**\n1. Use external references during development/rendering\n2. Optionally embed base64 AFTER successful rendering for distribution\n3. Keep external version for future edits\n\n---\n\n## Critical Warnings\n\n### âŒ Common Mistakes to Avoid\n\n1. **Creating animation before defining philosophy** â†’ Random trial-and-error wastes 15-30 minutes\n2. **Using PIL ImageDraw to recreate logo text** â†’ Creates DIFFERENT text, not your logo text\n3. **Embedding base64 before rendering** â†’ Cairo MemoryError crash (images >100KB)\n4. **Using 1000px for small elements** â†’ Huge files (400KB+), memory issues\n5. **Skipping logo analysis** â†’ Wrong workflow, have to restart\n6. **Forgetting loop validation** â†’ Visible jump when animation loops\n7. **Skipping preview renders** â†’ Waste time on full renders before validating concept\n8. **Rendering full SVG then cropping** â†’ Fuzzy edges, massive file sizes\n\n**Full list with code examples:** [references/anti_patterns.md](references/anti_patterns.md)\n\n---\n\n### âš ï¸ Text in Logos - CRITICAL\n\n**If logo contains text**, you MUST follow specialized workflow:\n\n1. **Extract text from logo SVG** (do NOT recreate with PIL ImageDraw)\n2. **Choose appropriate text animation method** (fade/stroke/transform)\n3. **Implement proper synchronization** with other elements\n\n**See:** [references/text_animation_guide.md](references/text_animation_guide.md) for complete guide\n\n---\n\n### âš ï¸ External References Required\n\n**Always use external file references during development/rendering:**\n\n```json\n// âœ… CORRECT: External reference\n\"assets\": [{\"id\": \"logo\", \"p\": \"logo_optimized.png\", \"e\": 0}]\n\n// âŒ WRONG: Embedded base64 (causes Cairo crash if >100KB)\n\"assets\": [{\"id\": \"logo\", \"p\": \"data:image/png;base64,...\", \"e\": 1}]\n```\n\n**Why:** Cairo renderer crashes with embedded images >100KB. Use external during development, optionally embed AFTER successful rendering.\n\n---\n\n## Helper Scripts Quick Reference\n\n| Script | Purpose | Example |\n|--------|---------|---------|\n| `prepare_logo.py` | Optimize and convert logo images | `python scripts/prepare_logo.py logo.png --max-size 500` |\n| `extract_svg_elements.py` | Extract elements from SVG | `python scripts/extract_svg_elements.py logo.svg --list` |\n| `validate_lottie.py` | Check Lottie structure | `python scripts/validate_lottie.py animation.json` |\n| `validate_loop.py` | Verify perfect loop | `python scripts/validate_loop.py animation.json` |\n| `render_lottie.py` | Render to GIF/MP4 (with asset validation) | `python scripts/render_lottie.py animation.json output.gif` |\n| `render_lottie.py --preview-frames N` | Quick preview (first N frames) | `python scripts/render_lottie.py animation.json preview.gif --preview-frames 60` |\n| `render_lottie.py --test-render` | Test mode with size warnings | `python scripts/render_lottie.py animation.json test.gif --test-render` |\n| `batch_export.py` | Export multiple formats | `python scripts/batch_export.py animation.json ./output gif,mp4` |\n\n**New features in render_lottie.py:**\n- âœ… Automatic asset validation (checks for missing external files)\n- âœ… Asset path resolution (relative to JSON file location)\n- âœ… Output verification (detects blank/corrupted files)\n- âœ… Preview mode (`--preview-frames N`) - renders only first N frames\n- âœ… Test mode (`--test-render`) - small test render with confirmation prompt\n\n**Detailed usage:** [references/script_usage.md](references/script_usage.md)\n\n---\n\n## Lottie JSON Fundamentals\n\n### Basic Structure\n\n```json\n{\n  \"v\": \"5.7.4\",           // Lottie version\n  \"fr\": 60,               // Frame rate\n  \"ip\": 0,                // In point (start frame)\n  \"op\": 180,              // Out point (end frame)\n  \"w\": 800,               // Width\n  \"h\": 800,               // Height\n  \"layers\": [...],        // Animation layers\n  \"assets\": [...]         // Image/asset references\n}\n```\n\n### Layer Types\n\n- **Type 2 (Image Layer):** Most common - animates PNG/SVG images\n- **Type 4 (Shape Layer):** Programmatic geometry (circles, rectangles, paths)\n\n### Animated Properties\n\n- **`o`:** Opacity (0-100)\n- **`p`:** Position [x, y]\n- **`s`:** Scale [x%, y%]\n- **`r`:** Rotation (degrees)\n- **`a`:** Anchor point [x, y]\n\n### Keyframe Structure\n\n```json\n\"o\": {\n  \"a\": 1,  // Animated (1) or static (0)\n  \"k\": [\n    {\"t\": 0, \"s\": [0], \"e\": [100], \"i\": {...}, \"o\": {...}},\n    {\"t\": 60, \"s\": [100]}\n  ]\n}\n```\n\n- **`t`:** Time (frame number)\n- **`s`:** Start value\n- **`e`:** End value\n- **`i`:** In tangent (ease in)\n- **`o`:** Out tangent (ease out)\n\n**Complete specification:** [references/lottie_spec.md](references/lottie_spec.md)\n\n---\n\n## Easing Functions\n\n**Never use linear easing** - always use curves for professional motion.\n\n| Easing | Values | Feel | Use Case |\n|--------|--------|------|----------|\n| **Ease-in-out (standard)** | `0.42/0.58` | Balanced, professional | Corporate, general use |\n| **Organic** | `0.25/0.75` | Soft, natural | Audio brands, waveforms, continuous motion |\n| **Bounce** | `0.34/1.56` | Playful, energetic | Startups, playful brands |\n| **Back** | `0.6/-0.28` & `0.735/0.045` | Overshoot, dynamic | Bold marketing, attention-grabbing |\n\n**Theory and examples:** [references/animation_theory.md](references/animation_theory.md)\n\n---\n\n## Dependencies\n\n### Required\n```bash\npip install lottie[all]    # Lottie manipulation\npip install Pillow         # Image processing\npip install pycairo        # Cairo rendering (for GIF)\n```\n\n### Cairo Installation\n\n**macOS:**\n```bash\nbrew install cairo pkg-config\npip install pycairo\n```\n\n**Linux (Ubuntu/Debian):**\n```bash\nsudo apt-get install libcairo2-dev pkg-config python3-dev\npip install pycairo\n```\n\n**Verify:**\n```bash\npython3 -c \"import cairo; print('Cairo OK')\"\n```\n\n**Troubleshooting:** [references/troubleshooting.md#cairo-and-dependencies](references/troubleshooting.md#cairo-and-dependencies)\n\n---\n\n## Troubleshooting\n\n### Quick Fixes\n\n**Asset not found errors:**\n- **Cause:** Assets not in same directory as Lottie JSON\n- **Fix:** Move assets to JSON file's directory, or use absolute paths\n- **Validation:** Run `render_lottie.py` - it validates assets before rendering\n\n**Blank/corrupted output:**\n- **Cause:** Missing assets, wrong paths, or rendering errors\n- **Fix:** Check asset validation messages, verify file sizes (output <1KB indicates failure)\n- **Detection:** Output verification runs automatically after rendering\n\n**MemoryError during GIF rendering:**\n- **Cause:** Embedded base64 images >100KB\n- **Fix:** Use external reference (`\"e\": 0`) instead\n\n**Loop has visible jump:**\n- **Cause:** Last keyframe doesn't match first\n- **Fix:** Run `validate_loop.py` and ensure last frame = first frame\n\n**Text looks wrong:**\n- **Cause:** Used PIL ImageDraw to recreate text\n- **Fix:** Extract text from logo SVG with `extract_svg_elements.py`\n\n**Animation too choppy:**\n- **Cause:** Too few keyframes or wrong FPS\n- **Fix:** Add keyframes (25-45 for organic motion), use 60fps for continuous motion\n\n**Preview renders save time:**\n- **Tip:** Use `--preview-frames 60` to validate concept before full render\n- **Tip:** Use `--test-render` for interactive testing with size warnings\n\n**Comprehensive guide:** [references/troubleshooting.md](references/troubleshooting.md)\n\n---\n\n## Advanced References\n\n### Detailed Documentation\n\n- **[references/detailed_examples.md](references/detailed_examples.md)** - Full Lottie JSON code for all patterns\n- **[references/animation_theory.md](references/animation_theory.md)** - Motion design principles and easing theory\n- **[references/preset_library.md](references/preset_library.md)** - Complete preset collection with real-world examples\n- **[references/lottie_spec.md](references/lottie_spec.md)** - Lottie JSON specification details\n- **[references/script_usage.md](references/script_usage.md)** - Complete script documentation\n- **[references/text_animation_guide.md](references/text_animation_guide.md)** - Specialized text animation workflows\n- **[references/anti_patterns.md](references/anti_patterns.md)** - Common mistakes with full code examples\n- **[references/troubleshooting.md](references/troubleshooting.md)** - Comprehensive troubleshooting guide\n- **[references/real-world-examples/](references/real-world-examples/)** - Production animations from major brands\n\n---\n\n## File Size Guidelines\n\n**Target sizes for different use cases:**\n\n| Use Case | Lottie JSON | GIF | MP4 | Image Assets |\n|----------|-------------|-----|-----|--------------|\n| Email signature | 20-50KB | 500KB-1MB | 200-500KB | <30KB each |\n| Website hero | 30-80KB | 1-3MB | 500KB-1.5MB | <50KB each |\n| Social media | 50-150KB | 3-8MB | 1-3MB | <80KB each |\n| Splash screen | 30-100KB | 2-5MB | 800KB-2MB | <60KB each |\n\n**Optimization:** Use `scripts/optimize_lottie.py` to reduce file sizes by removing redundant keyframes and rounding values.\n\n---\n\n## Curated Presets\n\nQuick reference to common presets (full code in [references/preset_library.md](references/preset_library.md)):\n\n**Branding Styles:**\n- Corporate Subtle - Fade + gentle scale (1.5s)\n- Startup Energetic - Bounce + overshoot (1.2s)\n- Luxury Elegant - Slow fade + minimal scale (3s)\n- Tech Glitch - Digital disruption effect (1s)\n\n**Use Cases:**\n- Website Hero - Quick professional entrance (0.8s)\n- Email Signature - Subtle loop (3s)\n- Social Media Intro - Bold entrance (2s)\n- Splash Screen - Brand moment with exit (2.5s)\n\n**Real-World Examples:**\nStudy hover animations from major brands in [references/real-world-examples/](references/real-world-examples/):\n- Reddit - Playful elastic bounce\n- Slack - Professional restrained pinch\n- Medium - Gentle editorial fade\n- Flickr - Camera shutter effect\n- Discord - Character wink\n\n---\n\n## Quick Decision Checklist\n\nBefore creating animation, verify:\n\n- [ ] Defined motion philosophy (personality + emotion + metaphor)\n- [ ] Analyzed logo structure (text? multi-element? SVG or PNG?)\n- [ ] Chose correct workflow based on analysis\n- [ ] If text present: Read [references/text_animation_guide.md](references/text_animation_guide.md)\n- [ ] Using external references (`\"e\": 0`) during development\n- [ ] Element sizes appropriate (500px full logo, 100-250px elements)\n- [ ] Selected motion type (Static/Organic/Bold/Cinematic)\n- [ ] Chosen timing strategy (simultaneous vs staggered)\n- [ ] Will validate with preview render before full render\n- [ ] Will run `validate_loop.py` if creating loop\n\n**If all checked â†’ Proceed with confidence âœ…**\n\n---\n\n## Tips for Success\n\n1. **Philosophy first** - 30 seconds planning saves 15-30 minutes iteration\n2. **Analyze before animating** - Understand logo structure upfront\n3. **Preview early, preview often** - Test 30-60 frame versions before full render\n4. **External references during development** - Embed base64 only after successful rendering\n5. **Match motion to brand** - Corporate â‰  startup â‰  audio brand\n6. **Perfect loops matter** - Use `validate_loop.py` to verify\n7. **Size elements appropriately** - 100-250px for elements, not 1000px\n8. **Extract, don't recreate** - Never use PIL to recreate logo text\n9. **Validate before rendering** - Run `validate_lottie.py` and `validate_loop.py`\n10. **Read references when stuck** - Detailed docs available for every topic\n\n---\n\n**Remember:** The goal is creating motion that enhances brand identity, not random animation. Philosophy-first workflow ensures alignment from the start."
    },
    "marketplace": {
      "hasMarketplaceJson": false
    },
    "internal": {
      "syncedAt": "2026-01-08T00:26:04.960Z",
      "version": 1
    }
  },
  "bayramannakov-claude-reflect": {
    "id": "bayramannakov-claude-reflect",
    "name": "claude-reflect",
    "description": "Self-learning system that captures corrections during sessions and reminds users to run /reflect to update CLAUDE.md. Use when discussing learnings, corrections, or when the user mentions remembering something for future sessions.",
    "repo": {
      "owner": "BayramAnnakov",
      "name": "claude-reflect",
      "fullName": "BayramAnnakov/claude-reflect",
      "url": "https://github.com/BayramAnnakov/claude-reflect",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 265,
      "forks": 12,
      "language": "Python",
      "topics": [
        "claude-code",
        "claude-skills",
        "memory",
        "productivity",
        "self-learning"
      ],
      "createdAt": "2026-01-03T20:51:36Z",
      "updatedAt": "2026-01-07T23:46:34Z",
      "pushedAt": "2026-01-07T06:18:02Z",
      "license": "MIT License"
    },
    "category": "tools",
    "tags": [
      "claude-code",
      "claude-skills",
      "memory",
      "productivity",
      "self-learning",
      "corrections",
      "sessions",
      "when",
      "self",
      "learning"
    ],
    "skillMd": {
      "raw": "---\nname: claude-reflect\ndescription: Self-learning system that captures corrections during sessions and reminds users to run /reflect to update CLAUDE.md. Use when discussing learnings, corrections, or when the user mentions remembering something for future sessions.\n---\n\n# Claude Reflect - Self-Learning System\n\nA two-stage system that helps Claude Code learn from user corrections.\n\n## How It Works\n\n**Stage 1: Capture (Automatic)**\nHooks detect correction patterns (\"no, use X\", \"actually...\", \"use X not Y\") and queue them to `~/.claude/learnings-queue.json`.\n\n**Stage 2: Process (Manual)**\nUser runs `/reflect` to review and apply queued learnings to CLAUDE.md files.\n\n## Available Commands\n\n| Command | Purpose |\n|---------|---------|\n| `/reflect` | Process queued learnings with human review |\n| `/reflect --scan-history` | Scan past sessions for missed learnings |\n| `/reflect --dry-run` | Preview changes without applying |\n| `/skip-reflect` | Discard all queued learnings |\n| `/view-queue` | View pending learnings without processing |\n\n## When to Remind Users\n\nRemind users about `/reflect` when:\n- They complete a feature or meaningful work unit\n- They make corrections you should remember for future sessions\n- They explicitly say \"remember this\" or similar\n- Context is about to compact and queue has items\n\n## Correction Detection Patterns\n\nHigh-confidence corrections:\n- Tool rejections (user stops an action with guidance)\n- \"no, use X\" / \"don't use Y\"\n- \"actually...\" / \"I meant...\"\n- \"use X not Y\" / \"X instead of Y\"\n- \"remember:\" (explicit marker)\n\n## CLAUDE.md Destinations\n\n- `~/.claude/CLAUDE.md` - Global learnings (model names, general patterns)\n- `./CLAUDE.md` - Project-specific learnings (conventions, tools, structure)\n\n## Example Interaction\n\n```\nUser: no, use gpt-5.1 not gpt-5 for reasoning tasks\nClaude: Got it, I'll use gpt-5.1 for reasoning tasks.\n\n[Hook captures this correction to queue]\n\nUser: /reflect\nClaude: Found 1 learning queued. \"Use gpt-5.1 for reasoning tasks\"\n        Scope: global\n        Apply to ~/.claude/CLAUDE.md? [y/n]\n```\n",
      "frontmatter": {
        "name": "claude-reflect",
        "description": "Self-learning system that captures corrections during sessions and reminds users to run /reflect to update CLAUDE.md. Use when discussing learnings, corrections, or when the user mentions remembering something for future sessions."
      },
      "content": "# Claude Reflect - Self-Learning System\n\nA two-stage system that helps Claude Code learn from user corrections.\n\n## How It Works\n\n**Stage 1: Capture (Automatic)**\nHooks detect correction patterns (\"no, use X\", \"actually...\", \"use X not Y\") and queue them to `~/.claude/learnings-queue.json`.\n\n**Stage 2: Process (Manual)**\nUser runs `/reflect` to review and apply queued learnings to CLAUDE.md files.\n\n## Available Commands\n\n| Command | Purpose |\n|---------|---------|\n| `/reflect` | Process queued learnings with human review |\n| `/reflect --scan-history` | Scan past sessions for missed learnings |\n| `/reflect --dry-run` | Preview changes without applying |\n| `/skip-reflect` | Discard all queued learnings |\n| `/view-queue` | View pending learnings without processing |\n\n## When to Remind Users\n\nRemind users about `/reflect` when:\n- They complete a feature or meaningful work unit\n- They make corrections you should remember for future sessions\n- They explicitly say \"remember this\" or similar\n- Context is about to compact and queue has items\n\n## Correction Detection Patterns\n\nHigh-confidence corrections:\n- Tool rejections (user stops an action with guidance)\n- \"no, use X\" / \"don't use Y\"\n- \"actually...\" / \"I meant...\"\n- \"use X not Y\" / \"X instead of Y\"\n- \"remember:\" (explicit marker)\n\n## CLAUDE.md Destinations\n\n- `~/.claude/CLAUDE.md` - Global learnings (model names, general patterns)\n- `./CLAUDE.md` - Project-specific learnings (conventions, tools, structure)\n\n## Example Interaction\n\n```\nUser: no, use gpt-5.1 not gpt-5 for reasoning tasks\nClaude: Got it, I'll use gpt-5.1 for reasoning tasks.\n\n[Hook captures this correction to queue]\n\nUser: /reflect\nClaude: Found 1 learning queued. \"Use gpt-5.1 for reasoning tasks\"\n        Scope: global\n        Apply to ~/.claude/CLAUDE.md? [y/n]\n```"
    },
    "marketplace": {
      "hasMarketplaceJson": false
    },
    "internal": {
      "syncedAt": "2026-01-08T00:30:10.789Z",
      "version": 1
    }
  },
  "ait88-claude-workflow-toolkit": {
    "id": "ait88-claude-workflow-toolkit",
    "name": "claude-workflow-toolkit",
    "description": "Reusable workflow optimization toolkit for Claude Code agents.",
    "repo": {
      "owner": "ait88",
      "name": "claude-workflow-toolkit",
      "fullName": "ait88/claude-workflow-toolkit",
      "url": "https://github.com/ait88/claude-workflow-toolkit",
      "defaultBranch": "master"
    },
    "metadata": {
      "stars": 1,
      "forks": 1,
      "language": "Shell",
      "topics": [
        "agentic-workflow",
        "claude-code",
        "claude-skills",
        "gh-cli",
        "token-optimization"
      ],
      "createdAt": "2026-01-06T10:15:03Z",
      "updatedAt": "2026-01-07T02:14:55Z",
      "pushedAt": "2026-01-07T02:13:53Z",
      "license": "MIT License"
    },
    "category": "tools",
    "tags": [
      "agentic-workflow",
      "claude-code",
      "claude-skills",
      "gh-cli",
      "token-optimization",
      "reusable",
      "workflow",
      "optimization",
      "toolkit",
      "claude"
    ],
    "skillMd": {
      "raw": "# Claude Workflow Toolkit\n\n## Purpose\n\nThis toolkit provides templates and patterns for optimizing Claude Code agent workflows in any repository. Use it to apply consistent, efficient workflow automation to a target project.\n\n## When to Use This Toolkit\n\nUse this toolkit when:\n- Setting up a new project for Claude Code agent collaboration\n- Optimizing an existing project's agent workflow\n- Updating a project to latest workflow best practices\n\n## How to Apply This Toolkit\n\n### Step 1: Understand the Target Project\n\nBefore applying, gather information about the target project:\n- **Language/Framework**: What tech stack? (PHP, Node, Python, Bash, etc.)\n- **Package Manager**: composer, npm, pip, none?\n- **Test Command**: How are tests run?\n- **Lint Command**: How is code style checked?\n- **Branch Naming**: Any existing conventions?\n- **Label Scheme**: Existing GitHub labels?\n- **Directory Structure**: Where does source code live?\n\n### Step 2: Select a Profile\n\nChoose the closest matching profile from `/profiles/`:\n- `default.yaml` - Generic, works for any project\n- `php-composer.yaml` - PHP projects using Composer\n- `bash-cli.yaml` - Bash CLI tools and scripts\n- `node-npm.yaml` - Node.js/TypeScript projects\n- `python-poetry.yaml` - Python projects\n\n### Step 3: Customize Variables\n\nEach template uses `{{VARIABLE}}` placeholders. Common variables:\n\n| Variable | Description | Example |\n|----------|-------------|---------|\n| `{{PROJECT_NAME}}` | Repository name | `my-project` |\n| `{{REPO_OWNER}}` | GitHub owner/org | `username` |\n| `{{DEFAULT_BRANCH}}` | Main branch name | `main` |\n| `{{TEST_COMMAND}}` | How to run tests | `composer test` |\n| `{{LINT_COMMAND}}` | How to check style | `npm run lint` |\n| `{{PHASE_PREFIX}}` | Label prefix for phases | `phase-` |\n| `{{PHASE_COUNT}}` | Number of phases (0-indexed) | `6` |\n| `{{SKILLS_DIR}}` | Where skills live | `.claude/skills` |\n| `{{DOCS_DIR}}` | Documentation directory | `docs` |\n\n### Step 4: Generate Files\n\nFor each template:\n1. Read the template file\n2. Replace all `{{VARIABLE}}` placeholders with project-specific values\n3. Write to the target project location\n4. Make skill scripts executable (`chmod +x`)\n\n### Step 5: Verify Installation\n\nAfter applying:\n1. Verify skills are executable: `ls -la {{SKILLS_DIR}}/`\n2. Test claim-issue with a test issue number\n3. Run check-workflow to verify it detects current state\n4. Review generated documentation for accuracy\n\n## Template Reference\n\n### Skills Templates\n\n#### `claim-issue.sh.template`\nAtomically claims a GitHub issue and creates a feature branch.\n- Removes `agent-ready` label\n- Adds `in-progress` label\n- Creates branch: `<issue>-<title-slug>`\n- Single operation prevents inconsistent state\n\n#### `check-workflow.sh.template`\nValidates current workflow state using GraphQL (1 API call vs 4-5 REST calls).\n- Extracts issue number from branch name\n- Validates labels match workflow stage\n- Provides fix commands for any issues\n- Color-coded output for quick scanning\n\n#### `submit-pr.sh.template`\nCreates PR and updates labels atomically.\n- Pushes current branch\n- Creates PR with \"Closes #X\"\n- Removes `in-progress`, adds `needs-review`\n\n### Documentation Templates\n\n#### `QUICK-REFERENCE.md.template`\nFast navigation document for agents - \"where do I find X?\"\n- Quick start paths for common tasks\n- Pre-flight checklists\n- Coding conventions summary\n- Links to detailed docs\n\n#### `FAQ-AGENTS.md.template`\nPre-answered common questions to reduce repeated lookups.\n- Project-specific Q&A format\n- Reduces tokens spent re-discovering information\n\n#### `CODEBASE-MAP.md.template`\nVisual directory structure with annotations.\n- What each directory/file does\n- Entry points for common tasks\n- Dependency relationships\n\n## Optimization Principles\n\nThese skills are designed around key principles:\n\n1. **Minimize API Calls**: GraphQL over REST where possible\n2. **Atomic Operations**: Prevent inconsistent state\n3. **Self-Documenting**: Skills output what they're doing\n4. **Graceful Errors**: Clear messages, actionable fixes\n5. **Idempotent Where Possible**: Safe to re-run\n\n## Application Workflow\n\nWhen applying this toolkit to a target project:\n\n```\n1. Clone/access target project\n2. Gather project information (language, test command, etc.)\n3. Select appropriate profile\n4. For each template:\n   a. Read template content\n   b. Substitute {{VARIABLES}} with project values\n   c. Write to target path\n   d. Set permissions (chmod +x for scripts)\n5. Update target project's .gitignore if needed\n6. Test the generated skills\n7. Commit changes to target project\n```\n\n## Output Structure\n\nAfter applying, the target project will have:\n\n```\ntarget-project/\nâ”œâ”€â”€ .claude/\nâ”‚   â”œâ”€â”€ skills/\nâ”‚   â”‚   â”œâ”€â”€ claim-issue      # Claim issue + create branch\nâ”‚   â”‚   â”œâ”€â”€ check-workflow   # Validate workflow state\nâ”‚   â”‚   â”œâ”€â”€ submit-pr        # Create PR + update labels\nâ”‚   â”‚   â””â”€â”€ README.md        # Skills documentation\nâ”‚   â””â”€â”€ settings.local.json  # Pre-configured Claude Code permissions\nâ””â”€â”€ {{DOCS_DIR}}/\n    â”œâ”€â”€ QUICK-REFERENCE.md   # Navigation hub\n    â”œâ”€â”€ FAQ-AGENTS.md        # Pre-answered questions\n    â””â”€â”€ CODEBASE-MAP.md      # Annotated directory structure\n```\n\n### Optional: Setup GitHub Labels\n\nBefore using the workflow, the target repository needs the required labels. Run the setup script from this toolkit in the target repo:\n\n```bash\n# From target project directory\n/path/to/claude-workflow-toolkit/scripts/setup-labels.sh\n```\n\nThis creates: `agent-ready`, `in-progress`, `needs-review`, `blocked`, `phase-0` through `phase-6`, and type labels.\n\n## Maintenance\n\nWhen updating the toolkit:\n1. Update templates in this repo\n2. For projects using this toolkit, re-run the application process\n3. Projects can diff changes and selectively adopt updates\n\n## Troubleshooting\n\n### \"Permission denied\" when running skills\n```bash\nchmod +x {{SKILLS_DIR}}/*\n```\n\n### \"gh: command not found\"\nInstall GitHub CLI: https://cli.github.com/\n\n### \"jq: command not found\"\nInstall jq:\n- macOS: `brew install jq`\n- Ubuntu/Debian: `sudo apt-get install jq`\n- RHEL/CentOS: `sudo yum install jq`\n\n### Skills not appearing as slash commands\nSkills should be in `{{SKILLS_DIR}}/` directory. Verify:\n1. Files exist and are executable\n2. Files have no `.sh` extension (just `claim-issue`, not `claim-issue.sh`)\n3. Claude Code is restarted after adding skills\n\n### SSH authentication fails\nThe skill templates include automatic SSH/HTTPS fallback. If SSH fails, they'll attempt to use `gh auth setup-git` to configure HTTPS authentication. Ensure:\n1. GitHub CLI is authenticated: `gh auth status`\n2. If using SSH, keys are properly configured: `ssh -T git@github.com`\n",
      "frontmatter": {},
      "content": "# Claude Workflow Toolkit\n\n## Purpose\n\nThis toolkit provides templates and patterns for optimizing Claude Code agent workflows in any repository. Use it to apply consistent, efficient workflow automation to a target project.\n\n## When to Use This Toolkit\n\nUse this toolkit when:\n- Setting up a new project for Claude Code agent collaboration\n- Optimizing an existing project's agent workflow\n- Updating a project to latest workflow best practices\n\n## How to Apply This Toolkit\n\n### Step 1: Understand the Target Project\n\nBefore applying, gather information about the target project:\n- **Language/Framework**: What tech stack? (PHP, Node, Python, Bash, etc.)\n- **Package Manager**: composer, npm, pip, none?\n- **Test Command**: How are tests run?\n- **Lint Command**: How is code style checked?\n- **Branch Naming**: Any existing conventions?\n- **Label Scheme**: Existing GitHub labels?\n- **Directory Structure**: Where does source code live?\n\n### Step 2: Select a Profile\n\nChoose the closest matching profile from `/profiles/`:\n- `default.yaml` - Generic, works for any project\n- `php-composer.yaml` - PHP projects using Composer\n- `bash-cli.yaml` - Bash CLI tools and scripts\n- `node-npm.yaml` - Node.js/TypeScript projects\n- `python-poetry.yaml` - Python projects\n\n### Step 3: Customize Variables\n\nEach template uses `{{VARIABLE}}` placeholders. Common variables:\n\n| Variable | Description | Example |\n|----------|-------------|---------|\n| `{{PROJECT_NAME}}` | Repository name | `my-project` |\n| `{{REPO_OWNER}}` | GitHub owner/org | `username` |\n| `{{DEFAULT_BRANCH}}` | Main branch name | `main` |\n| `{{TEST_COMMAND}}` | How to run tests | `composer test` |\n| `{{LINT_COMMAND}}` | How to check style | `npm run lint` |\n| `{{PHASE_PREFIX}}` | Label prefix for phases | `phase-` |\n| `{{PHASE_COUNT}}` | Number of phases (0-indexed) | `6` |\n| `{{SKILLS_DIR}}` | Where skills live | `.claude/skills` |\n| `{{DOCS_DIR}}` | Documentation directory | `docs` |\n\n### Step 4: Generate Files\n\nFor each template:\n1. Read the template file\n2. Replace all `{{VARIABLE}}` placeholders with project-specific values\n3. Write to the target project location\n4. Make skill scripts executable (`chmod +x`)\n\n### Step 5: Verify Installation\n\nAfter applying:\n1. Verify skills are executable: `ls -la {{SKILLS_DIR}}/`\n2. Test claim-issue with a test issue number\n3. Run check-workflow to verify it detects current state\n4. Review generated documentation for accuracy\n\n## Template Reference\n\n### Skills Templates\n\n#### `claim-issue.sh.template`\nAtomically claims a GitHub issue and creates a feature branch.\n- Removes `agent-ready` label\n- Adds `in-progress` label\n- Creates branch: `<issue>-<title-slug>`\n- Single operation prevents inconsistent state\n\n#### `check-workflow.sh.template`\nValidates current workflow state using GraphQL (1 API call vs 4-5 REST calls).\n- Extracts issue number from branch name\n- Validates labels match workflow stage\n- Provides fix commands for any issues\n- Color-coded output for quick scanning\n\n#### `submit-pr.sh.template`\nCreates PR and updates labels atomically.\n- Pushes current branch\n- Creates PR with \"Closes #X\"\n- Removes `in-progress`, adds `needs-review`\n\n### Documentation Templates\n\n#### `QUICK-REFERENCE.md.template`\nFast navigation document for agents - \"where do I find X?\"\n- Quick start paths for common tasks\n- Pre-flight checklists\n- Coding conventions summary\n- Links to detailed docs\n\n#### `FAQ-AGENTS.md.template`\nPre-answered common questions to reduce repeated lookups.\n- Project-specific Q&A format\n- Reduces tokens spent re-discovering information\n\n#### `CODEBASE-MAP.md.template`\nVisual directory structure with annotations.\n- What each directory/file does\n- Entry points for common tasks\n- Dependency relationships\n\n## Optimization Principles\n\nThese skills are designed around key principles:\n\n1. **Minimize API Calls**: GraphQL over REST where possible\n2. **Atomic Operations**: Prevent inconsistent state\n3. **Self-Documenting**: Skills output what they're doing\n4. **Graceful Errors**: Clear messages, actionable fixes\n5. **Idempotent Where Possible**: Safe to re-run\n\n## Application Workflow\n\nWhen applying this toolkit to a target project:\n\n```\n1. Clone/access target project\n2. Gather project information (language, test command, etc.)\n3. Select appropriate profile\n4. For each template:\n   a. Read template content\n   b. Substitute {{VARIABLES}} with project values\n   c. Write to target path\n   d. Set permissions (chmod +x for scripts)\n5. Update target project's .gitignore if needed\n6. Test the generated skills\n7. Commit changes to target project\n```\n\n## Output Structure\n\nAfter applying, the target project will have:\n\n```\ntarget-project/\nâ”œâ”€â”€ .claude/\nâ”‚   â”œâ”€â”€ skills/\nâ”‚   â”‚   â”œâ”€â”€ claim-issue      # Claim issue + create branch\nâ”‚   â”‚   â”œâ”€â”€ check-workflow   # Validate workflow state\nâ”‚   â”‚   â”œâ”€â”€ submit-pr        # Create PR + update labels\nâ”‚   â”‚   â””â”€â”€ README.md        # Skills documentation\nâ”‚   â””â”€â”€ settings.local.json  # Pre-configured Claude Code permissions\nâ””â”€â”€ {{DOCS_DIR}}/\n    â”œâ”€â”€ QUICK-REFERENCE.md   # Navigation hub\n    â”œâ”€â”€ FAQ-AGENTS.md        # Pre-answered questions\n    â””â”€â”€ CODEBASE-MAP.md      # Annotated directory structure\n```\n\n### Optional: Setup GitHub Labels\n\nBefore using the workflow, the target repository needs the required labels. Run the setup script from this toolkit in the target repo:\n\n```bash\n# From target project directory\n/path/to/claude-workflow-toolkit/scripts/setup-labels.sh\n```\n\nThis creates: `agent-ready`, `in-progress`, `needs-review`, `blocked`, `phase-0` through `phase-6`, and type labels.\n\n## Maintenance\n\nWhen updating the toolkit:\n1. Update templates in this repo\n2. For projects using this toolkit, re-run the application process\n3. Projects can diff changes and selectively adopt updates\n\n## Troubleshooting\n\n### \"Permission denied\" when running skills\n```bash\nchmod +x {{SKILLS_DIR}}/*\n```\n\n### \"gh: command not found\"\nInstall GitHub CLI: https://cli.github.com/\n\n### \"jq: command not found\"\nInstall jq:\n- macOS: `brew install jq`\n- Ubuntu/Debian: `sudo apt-get install jq`\n- RHEL/CentOS: `sudo yum install jq`\n\n### Skills not appearing as slash commands\nSkills should be in `{{SKILLS_DIR}}/` directory. Verify:\n1. Files exist and are executable\n2. Files have no `.sh` extension (just `claim-issue`, not `claim-issue.sh`)\n3. Claude Code is restarted after adding skills\n\n### SSH authentication fails\nThe skill templates include automatic SSH/HTTPS fallback. If SSH fails, they'll attempt to use `gh auth setup-git` to configure HTTPS authentication. Ensure:\n1. GitHub CLI is authenticated: `gh auth status`\n2. If using SSH, keys are properly configured: `ssh -T git@github.com`"
    },
    "marketplace": {
      "hasMarketplaceJson": false
    },
    "internal": {
      "syncedAt": "2026-01-08T00:30:44.239Z",
      "version": 1
    }
  },
  "santiago-afonso-codex-sandbox-preflight": {
    "id": "santiago-afonso-codex-sandbox-preflight",
    "name": "codex-sandbox-preflight",
    "description": "Use at the start of a Codex session (especially sandboxed) to run `scripts/codex-sandbox-preflight.sh` and interpret network + writable_roots constraints.",
    "repo": {
      "owner": "santiago-afonso",
      "name": "codex-sandbox-preflight",
      "fullName": "santiago-afonso/codex-sandbox-preflight",
      "url": "https://github.com/santiago-afonso/codex-sandbox-preflight",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 0,
      "forks": 0,
      "language": "Shell",
      "topics": [
        "claude-skills",
        "codex",
        "skills"
      ],
      "createdAt": "2025-12-27T20:57:48Z",
      "updatedAt": "2026-01-06T15:04:26Z",
      "pushedAt": "2026-01-06T15:04:22Z"
    },
    "category": "development",
    "tags": [
      "claude-skills",
      "codex",
      "skills",
      "start",
      "session",
      "especially",
      "sandboxed"
    ],
    "skillMd": {
      "raw": "---\nname: codex-sandbox-preflight\ndescription: \"Use at the start of a Codex session (especially sandboxed) to run `scripts/codex-sandbox-preflight.sh` and interpret network + writable_roots constraints.\"\n---\n\n# Codex sandbox preflight\n\n## When to use\n- Start of a new Codex session (default).\n- You see sandbox-ish errors like `PermissionError: [Errno 1] Operation not permitted`, `seccomp`, or unexpected â€œPermission deniedâ€ when paths look writable.\n- You need to know if network is disabled in the tool sandbox before attempting auth, installs, `git push`, etc.\n\n## Workflow\n1) Run the preflight helper:\n```bash\nscripts/codex-sandbox-preflight.sh\n```\n\n2) If youâ€™re in a normal shell and want to see what happens when network is enabled inside the sandbox:\n```bash\nscripts/codex-sandbox-preflight.sh --with-network\n```\n\n3) Summarize results (donâ€™t paste the full output unless asked):\n- Tool sandbox network: `socket()` allowed vs blocked (and DNS if allowed).\n- Writable roots: whether `~/.config/wbg-auth` is writable inside the sandbox.\n- Config drift: whether `~/.codex/config.toml` is symlinked to dotfiles or has diverged.\n\n## Interpretation cheatsheet\n- `INFO- socket() syscall blocked`:\n  - Tool sandbox has network disabled (expected in many sandboxed sessions).\n  - Avoid network-dependent commands/tools inside the sandbox.\n  - To allow sandbox network, start Codex with `-c sandbox_workspace_write.network_access=true` (still sandboxed, but with egress).\n- `WARN missing_writable_root=$HOME/.config/wbg-auth` (or similar) / sandbox write fails for `~/.config/wbg-auth`:\n  - `wbg-auth` will crash on startup due to log file creation.\n  - Fix by adding `~/.config/wbg-auth` to `sandbox_workspace_write.writable_roots` in `~/.codex/config.toml`.\n\n## Notes / pitfalls\n- Running this from inside an already-restricted tool sandbox cannot â€œproveâ€ that enabling network would work; outer seccomp will still block `socket()`. Use `--with-network` from a normal shell for that.\n- This helper must never print secrets; it only checks tool presence, config linkage, writability, and basic DNS.\n",
      "frontmatter": {
        "name": "codex-sandbox-preflight",
        "description": "Use at the start of a Codex session (especially sandboxed) to run `scripts/codex-sandbox-preflight.sh` and interpret network + writable_roots constraints."
      },
      "content": "# Codex sandbox preflight\n\n## When to use\n- Start of a new Codex session (default).\n- You see sandbox-ish errors like `PermissionError: [Errno 1] Operation not permitted`, `seccomp`, or unexpected â€œPermission deniedâ€ when paths look writable.\n- You need to know if network is disabled in the tool sandbox before attempting auth, installs, `git push`, etc.\n\n## Workflow\n1) Run the preflight helper:\n```bash\nscripts/codex-sandbox-preflight.sh\n```\n\n2) If youâ€™re in a normal shell and want to see what happens when network is enabled inside the sandbox:\n```bash\nscripts/codex-sandbox-preflight.sh --with-network\n```\n\n3) Summarize results (donâ€™t paste the full output unless asked):\n- Tool sandbox network: `socket()` allowed vs blocked (and DNS if allowed).\n- Writable roots: whether `~/.config/wbg-auth` is writable inside the sandbox.\n- Config drift: whether `~/.codex/config.toml` is symlinked to dotfiles or has diverged.\n\n## Interpretation cheatsheet\n- `INFO- socket() syscall blocked`:\n  - Tool sandbox has network disabled (expected in many sandboxed sessions).\n  - Avoid network-dependent commands/tools inside the sandbox.\n  - To allow sandbox network, start Codex with `-c sandbox_workspace_write.network_access=true` (still sandboxed, but with egress).\n- `WARN missing_writable_root=$HOME/.config/wbg-auth` (or similar) / sandbox write fails for `~/.config/wbg-auth`:\n  - `wbg-auth` will crash on startup due to log file creation.\n  - Fix by adding `~/.config/wbg-auth` to `sandbox_workspace_write.writable_roots` in `~/.codex/config.toml`.\n\n## Notes / pitfalls\n- Running this from inside an already-restricted tool sandbox cannot â€œproveâ€ that enabling network would work; outer seccomp will still block `socket()`. Use `--with-network` from a normal shell for that.\n- This helper must never print secrets; it only checks tool presence, config linkage, writability, and basic DNS."
    },
    "marketplace": {
      "hasMarketplaceJson": false
    },
    "internal": {
      "syncedAt": "2026-01-08T00:33:11.282Z",
      "version": 1
    }
  },
  "blacktop-ipsw-skill": {
    "id": "blacktop-ipsw-skill",
    "name": "ipsw",
    "description": "Apple firmware and binary reverse engineering with the ipsw CLI tool. Use when analyzing iOS/macOS binaries, disassembling functions in dyld_shared_cache, dumping Objective-C headers from private frameworks, downloading IPSWs or kernelcaches, extracting entitlements, analyzing Mach-O files, or researching Apple security. Triggers on requests involving Apple RE, iOS internals, kernel analysis, KEXT extraction, or vulnerability research on Apple platforms.",
    "repo": {
      "owner": "blacktop",
      "name": "ipsw-skill",
      "fullName": "blacktop/ipsw-skill",
      "url": "https://github.com/blacktop/ipsw-skill",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 30,
      "forks": 1,
      "language": null,
      "topics": [
        "apple",
        "claude-code",
        "claude-skills",
        "codex",
        "ipsw",
        "reverse-engineering",
        "skill",
        "vulnerability-research"
      ],
      "createdAt": "2025-12-24T17:59:06Z",
      "updatedAt": "2026-01-05T03:48:38Z",
      "pushedAt": "2026-01-04T19:16:12Z",
      "license": "MIT License"
    },
    "category": "tools",
    "tags": [
      "apple",
      "claude-code",
      "claude-skills",
      "codex",
      "ipsw",
      "reverse-engineering",
      "skill",
      "vulnerability-research",
      "analyzing",
      "firmware"
    ],
    "skillMd": {
      "raw": "---\nname: ipsw\ndescription: Apple firmware and binary reverse engineering with the ipsw CLI tool. Use when analyzing iOS/macOS binaries, disassembling functions in dyld_shared_cache, dumping Objective-C headers from private frameworks, downloading IPSWs or kernelcaches, extracting entitlements, analyzing Mach-O files, or researching Apple security. Triggers on requests involving Apple RE, iOS internals, kernel analysis, KEXT extraction, or vulnerability research on Apple platforms.\n---\n\n# IPSW - Apple Reverse Engineering Toolkit\n\n**Install:** `brew install blacktop/tap/ipsw`\n\n## Choose Your Workflow\n\n| Goal | Start Here |\n|------|------------|\n| Download/extract firmware | [Firmware Acquisition](#firmware-acquisition) |\n| Reverse engineer userspace | [Userspace RE](#userspace-re-dyld_shared_cache) |\n| Analyze kernel/KEXTs | [Kernel Analysis](#kernel-analysis) |\n| Research entitlements | [Entitlements](#entitlements) |\n| Dump private API headers | [Class Dump](#class-dump) |\n| Analyze standalone binary | [Mach-O Analysis](#mach-o-analysis) |\n\n---\n\n## Firmware Acquisition\n\n```bash\n# Download latest IPSW for device\nipsw download ipsw --device iPhone16,1 --latest\n\n# Download with automatic kernel/DSC extraction\nipsw download ipsw --device iPhone16,1 --latest --kernel --dyld\n\n# Extract components from local IPSW\nipsw extract --kernel iPhone16,1_18.0_Restore.ipsw\nipsw extract --dyld --dyld-arch arm64e iPhone16,1_18.0_Restore.ipsw\n\n# Remote extraction (no full download)\nipsw extract --kernel --remote <IPSW_URL>\n```\n\nSee [references/download.md](references/download.md) for device identifiers and advanced options.\n\n---\n\n## Userspace RE (dyld_shared_cache)\n\n**macOS DSC:** `/System/Volumes/Preboot/Cryptexes/OS/System/Library/dyld/dyld_shared_cache_arm64e`\n\n### Essential Commands\n\n| Command | Purpose |\n|---------|---------|\n| `dyld a2s <DSC> <ADDR>` | Address â†’ symbol (triage crash LR/PC) |\n| `dyld symaddr <DSC> <SYM> --image <DYLIB>` | Symbol â†’ address |\n| `dyld disass <DSC> --vaddr <ADDR>` | Disassemble at address |\n| `dyld disass <DSC> --symbol <SYM> --image <DYLIB>` | Disassemble by symbol |\n| `dyld xref <DSC> <ADDR> --all` | Find all references to address |\n| `dyld dump <DSC> <ADDR> --size 256` | Dump raw bytes at address |\n| `dyld str <DSC> \"pattern\" --image <DYLIB>` | Search strings |\n| `dyld objc --class <DSC> --image <DYLIB>` | List ObjC classes |\n| `dyld extract <DSC> <DYLIB> -o ./out/` | Extract dylib for external tools |\n\n### Common Workflow\n\n```bash\n# 1. Resolve address from crash/trace\nipsw dyld a2s $DSC 0x1bc39e1e0\n# â†’ -[SomeClass someMethod:] + 0x40\n\n# 2. Disassemble around that address\nipsw dyld disass $DSC --vaddr 0x1bc39e1e0\n\n# 3. Find who calls this function\nipsw dyld xref $DSC 0x1bc39e1a0 --all\n\n# 4. Extract string/data referenced in disassembly\nipsw dyld dump $DSC 0x1bc39e200 --size 64\n```\n\n**Tip:** Always use `--image <DYLIB>` - it's 10x+ faster.\n\nSee [references/dyld.md](references/dyld.md) for complete DSC commands.\n\n---\n\n## Kernel Analysis\n\n```bash\n# List all KEXTs\nipsw kernel kexts kernelcache.release.iPhone16,1\n\n# Extract specific KEXT\nipsw kernel extract kernelcache sandbox --output ./kexts/\n\n# Dump syscalls\nipsw kernel syscall kernelcache\n\n# Diff KEXTs between versions\nipsw kernel kexts --diff kernelcache_17.0 kernelcache_18.0\n```\n\nSee [references/kernel.md](references/kernel.md) for KEXT extraction and kernel analysis.\n\n---\n\n## Entitlements\n\n```bash\n# Single binary entitlements\nipsw macho info --ent /path/to/binary\n\n# Build searchable database from IPSW\nipsw ent --sqlite ent.db --ipsw iOS18.ipsw\n\n# Query database\nipsw ent --sqlite ent.db --key \"com.apple.private.security.no-sandbox\"\nipsw ent --sqlite ent.db --key \"platform-application\"\nipsw ent --sqlite ent.db --key \"com.apple.private.tcc.manager\"\n```\n\nSee [references/entitlements.md](references/entitlements.md) for common entitlements and query patterns.\n\n---\n\n## Class Dump\n\nDump Objective-C headers from binaries or dyld_shared_cache:\n\n```bash\n# Dump all headers from framework in DSC\nipsw class-dump $DSC SpringBoardServices --headers -o ./headers/\n\n# Dump specific class\nipsw class-dump $DSC Security --class SecKey\n\n# Filter by pattern\nipsw class-dump $DSC UIKit --class 'UIApplication.*' --headers -o ./headers/\n\n# Include runtime addresses (for hooking)\nipsw class-dump $DSC Security --re\n```\n\nSee [references/class-dump.md](references/class-dump.md) for filtering and output options.\n\n---\n\n## Mach-O Analysis\n\n```bash\n# Full binary info\nipsw macho info /path/to/binary\n\n# Disassemble function\nipsw macho disass /path/to/binary --symbol _main\n\n# Get entitlements and signature\nipsw macho info --ent /path/to/binary\nipsw macho info --sig /path/to/binary\n```\n\nSee [references/macho.md](references/macho.md) for complete Mach-O commands.\n\n---\n\n## Reference Files\n\n- [references/download.md](references/download.md) - Firmware download, device IDs, extraction\n- [references/dyld.md](references/dyld.md) - Complete DSC commands (a2s, xref, dump, str, extract)\n- [references/kernel.md](references/kernel.md) - Kernel and KEXT analysis\n- [references/entitlements.md](references/entitlements.md) - Entitlements database and queries\n- [references/class-dump.md](references/class-dump.md) - ObjC header dumping\n- [references/macho.md](references/macho.md) - Mach-O binary analysis\n\n## Tips\n\n1. **Symbol caching:** First `a2s`/`symaddr` creates `.a2s` cache - subsequent lookups are instant\n2. **Use --image flag:** Specifying dylib is 10x+ faster for DSC operations\n3. **JSON output:** Most commands support `--json` for scripting\n4. **Device IDs:** Use `ipsw device-list` to find device identifiers\n",
      "frontmatter": {
        "name": "ipsw",
        "description": "Apple firmware and binary reverse engineering with the ipsw CLI tool. Use when analyzing iOS/macOS binaries, disassembling functions in dyld_shared_cache, dumping Objective-C headers from private frameworks, downloading IPSWs or kernelcaches, extracting entitlements, analyzing Mach-O files, or researching Apple security. Triggers on requests involving Apple RE, iOS internals, kernel analysis, KEXT extraction, or vulnerability research on Apple platforms."
      },
      "content": "# IPSW - Apple Reverse Engineering Toolkit\n\n**Install:** `brew install blacktop/tap/ipsw`\n\n## Choose Your Workflow\n\n| Goal | Start Here |\n|------|------------|\n| Download/extract firmware | [Firmware Acquisition](#firmware-acquisition) |\n| Reverse engineer userspace | [Userspace RE](#userspace-re-dyld_shared_cache) |\n| Analyze kernel/KEXTs | [Kernel Analysis](#kernel-analysis) |\n| Research entitlements | [Entitlements](#entitlements) |\n| Dump private API headers | [Class Dump](#class-dump) |\n| Analyze standalone binary | [Mach-O Analysis](#mach-o-analysis) |\n\n---\n\n## Firmware Acquisition\n\n```bash\n# Download latest IPSW for device\nipsw download ipsw --device iPhone16,1 --latest\n\n# Download with automatic kernel/DSC extraction\nipsw download ipsw --device iPhone16,1 --latest --kernel --dyld\n\n# Extract components from local IPSW\nipsw extract --kernel iPhone16,1_18.0_Restore.ipsw\nipsw extract --dyld --dyld-arch arm64e iPhone16,1_18.0_Restore.ipsw\n\n# Remote extraction (no full download)\nipsw extract --kernel --remote <IPSW_URL>\n```\n\nSee [references/download.md](references/download.md) for device identifiers and advanced options.\n\n---\n\n## Userspace RE (dyld_shared_cache)\n\n**macOS DSC:** `/System/Volumes/Preboot/Cryptexes/OS/System/Library/dyld/dyld_shared_cache_arm64e`\n\n### Essential Commands\n\n| Command | Purpose |\n|---------|---------|\n| `dyld a2s <DSC> <ADDR>` | Address â†’ symbol (triage crash LR/PC) |\n| `dyld symaddr <DSC> <SYM> --image <DYLIB>` | Symbol â†’ address |\n| `dyld disass <DSC> --vaddr <ADDR>` | Disassemble at address |\n| `dyld disass <DSC> --symbol <SYM> --image <DYLIB>` | Disassemble by symbol |\n| `dyld xref <DSC> <ADDR> --all` | Find all references to address |\n| `dyld dump <DSC> <ADDR> --size 256` | Dump raw bytes at address |\n| `dyld str <DSC> \"pattern\" --image <DYLIB>` | Search strings |\n| `dyld objc --class <DSC> --image <DYLIB>` | List ObjC classes |\n| `dyld extract <DSC> <DYLIB> -o ./out/` | Extract dylib for external tools |\n\n### Common Workflow\n\n```bash\n# 1. Resolve address from crash/trace\nipsw dyld a2s $DSC 0x1bc39e1e0\n# â†’ -[SomeClass someMethod:] + 0x40\n\n# 2. Disassemble around that address\nipsw dyld disass $DSC --vaddr 0x1bc39e1e0\n\n# 3. Find who calls this function\nipsw dyld xref $DSC 0x1bc39e1a0 --all\n\n# 4. Extract string/data referenced in disassembly\nipsw dyld dump $DSC 0x1bc39e200 --size 64\n```\n\n**Tip:** Always use `--image <DYLIB>` - it's 10x+ faster.\n\nSee [references/dyld.md](references/dyld.md) for complete DSC commands.\n\n---\n\n## Kernel Analysis\n\n```bash\n# List all KEXTs\nipsw kernel kexts kernelcache.release.iPhone16,1\n\n# Extract specific KEXT\nipsw kernel extract kernelcache sandbox --output ./kexts/\n\n# Dump syscalls\nipsw kernel syscall kernelcache\n\n# Diff KEXTs between versions\nipsw kernel kexts --diff kernelcache_17.0 kernelcache_18.0\n```\n\nSee [references/kernel.md](references/kernel.md) for KEXT extraction and kernel analysis.\n\n---\n\n## Entitlements\n\n```bash\n# Single binary entitlements\nipsw macho info --ent /path/to/binary\n\n# Build searchable database from IPSW\nipsw ent --sqlite ent.db --ipsw iOS18.ipsw\n\n# Query database\nipsw ent --sqlite ent.db --key \"com.apple.private.security.no-sandbox\"\nipsw ent --sqlite ent.db --key \"platform-application\"\nipsw ent --sqlite ent.db --key \"com.apple.private.tcc.manager\"\n```\n\nSee [references/entitlements.md](references/entitlements.md) for common entitlements and query patterns.\n\n---\n\n## Class Dump\n\nDump Objective-C headers from binaries or dyld_shared_cache:\n\n```bash\n# Dump all headers from framework in DSC\nipsw class-dump $DSC SpringBoardServices --headers -o ./headers/\n\n# Dump specific class\nipsw class-dump $DSC Security --class SecKey\n\n# Filter by pattern\nipsw class-dump $DSC UIKit --class 'UIApplication.*' --headers -o ./headers/\n\n# Include runtime addresses (for hooking)\nipsw class-dump $DSC Security --re\n```\n\nSee [references/class-dump.md](references/class-dump.md) for filtering and output options.\n\n---\n\n## Mach-O Analysis\n\n```bash\n# Full binary info\nipsw macho info /path/to/binary\n\n# Disassemble function\nipsw macho disass /path/to/binary --symbol _main\n\n# Get entitlements and signature\nipsw macho info --ent /path/to/binary\nipsw macho info --sig /path/to/binary\n```\n\nSee [references/macho.md](references/macho.md) for complete Mach-O commands.\n\n---\n\n## Reference Files\n\n- [references/download.md](references/download.md) - Firmware download, device IDs, extraction\n- [references/dyld.md](references/dyld.md) - Complete DSC commands (a2s, xref, dump, str, extract)\n- [references/kernel.md](references/kernel.md) - Kernel and KEXT analysis\n- [references/entitlements.md](references/entitlements.md) - Entitlements database and queries\n- [references/class-dump.md](references/class-dump.md) - ObjC header dumping\n- [references/macho.md](references/macho.md) - Mach-O binary analysis\n\n## Tips\n\n1. **Symbol caching:** First `a2s`/`symaddr` creates `.a2s` cache - subsequent lookups are instant\n2. **Use --image flag:** Specifying dylib is 10x+ faster for DSC operations\n3. **JSON output:** Most commands support `--json` for scripting\n4. **Device IDs:** Use `ipsw device-list` to find device identifiers"
    },
    "marketplace": {
      "hasMarketplaceJson": false
    },
    "internal": {
      "syncedAt": "2026-01-08T00:37:42.119Z",
      "version": 1
    }
  },
  "jjmartres-reachy-mini-sdk-skill": {
    "id": "jjmartres-reachy-mini-sdk-skill",
    "name": "reachy-mini-sdk",
    "description": "Programming guide for Reachy Mini robot using Python SDK v1.2.6 and REST API. Use when controlling Reachy Mini robots, programming movements (head/antennas/body), accessing sensors (camera/microphone/IMU), recording motions, building AI applications, deploying to Hugging Face, or using the daemon REST API. Covers SDK patterns, coordinate systems, interpolation methods, app management, and OpenAPI client generation.",
    "repo": {
      "owner": "jjmartres",
      "name": "reachy-mini-sdk-skill",
      "fullName": "jjmartres/reachy-mini-sdk-skill",
      "url": "https://github.com/jjmartres/reachy-mini-sdk-skill",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 3,
      "forks": 1,
      "language": "Python",
      "topics": [
        "agentskills",
        "claude-code",
        "claude-skills"
      ],
      "createdAt": "2026-01-04T08:49:31Z",
      "updatedAt": "2026-01-06T20:29:23Z",
      "pushedAt": "2026-01-04T11:05:43Z",
      "license": "MIT License"
    },
    "category": "development",
    "tags": [
      "agentskills",
      "claude-code",
      "claude-skills",
      "programming",
      "reachy",
      "mini",
      "using",
      "rest"
    ],
    "skillMd": {
      "raw": "---\nname: reachy-mini-sdk\ndescription: Programming guide for Reachy Mini robot using Python SDK v1.2.6 and REST API. Use when controlling Reachy Mini robots, programming movements (head/antennas/body), accessing sensors (camera/microphone/IMU), recording motions, building AI applications, deploying to Hugging Face, or using the daemon REST API. Covers SDK patterns, coordinate systems, interpolation methods, app management, and OpenAPI client generation.\nlicense: MIT (see LICENSE.txt)\n---\n\n# Reachy Mini SDK\n\nProgramming guide for Reachy Mini - an open-source desktop humanoid robot with 6-DOF head, expressive antennas, and AI integration.\n\n## Hardware\n\n- **Head**: 6-DOF Stewart platform (X,Y,Z + roll,pitch,yaw)\n- **Antennas**: 2 servos\n- **Body**: 360Â° yaw rotation\n- **Sensors**: Camera, microphone, IMU (Wireless only)\n\n**Daemon**: FastAPI on port 8000 (REST + WebSocket)\n\n## Quick Start\n\n### Installation\n\nSee `references/installation.md` for complete setup (uv/pip, platform-specific configs, permissions).\n\n### Basic Connection\n\n```python\nfrom reachy_mini import ReachyMini\n\n# Local\nwith ReachyMini() as mini:\n    pass\n\n# Remote (Wireless)\nwith ReachyMini(localhost_only=False) as mini:\n    pass\n```\n\n## Movement\n\nSee `references/movement_control.md` for complete guide (450+ lines with all patterns).\n\n### goto_target (Smooth Interpolation)\n\n```python\nfrom reachy_mini.utils import create_head_pose\nimport numpy as np\n\nmini.goto_target(\n    head=create_head_pose(z=10, roll=15, degrees=True, mm=True),\n    antennas=np.deg2rad([45, 45]),\n    body_yaw=np.deg2rad(30),\n    duration=2.0,\n    method=\"minjerk\"  # linear, ease, cartoon\n)\n```\n\n### set_target (Direct Control)\n\nFor high-frequency control (>30Hz):\n\n```python\nmini.set_target(\n    head=create_head_pose(z=5, mm=True),\n    antennas=[0.5, -0.5]\n)\n```\n\n### Coordinates\n\n- **Head**: Position in meters, orientation in radians\n- **Antennas**: Radians (Â±1.5)\n- **Body**: Radians (full 360Â°)\n\n## Sensors\n\nSee `references/sensors.md` for camera, audio, IMU details.\n\n```python\n# Camera (BGR numpy array)\nframe = mini.media.get_frame()\n\n# Audio (16kHz stereo)\nsamples = mini.media.get_audio_sample()\nmini.media.push_audio_sample(samples)  # Non-blocking\n\n# IMU (Wireless only)\nif hasattr(mini, 'imu'):\n    data = mini.imu.get_data()\n```\n\n## Motion Recording\n\n```python\nmini.start_recording()\n# Move robot\nmotion = mini.stop_recording()\nmotion.save(\"demo.pkl\")\n\n# Replay\nmotion.play()\n```\n\n## REST API\n\nSee `references/daemon_api.md` for all 25+ endpoints.\nSee `references/openapi_usage.md` for client generation.\n\n### Direct HTTP Control\n\n```python\nimport requests\n\n# Move via API\nrequests.post(\"http://localhost:8000/api/goto\", json={\n    \"head_pose\": {\"x\": 0, \"y\": 0, \"z\": 0.01, \"roll\": 0, \"pitch\": 0, \"yaw\": 0},\n    \"duration\": 2.0,\n    \"interpolation\": \"minjerk\"\n})\n\n# Get state\nstate = requests.get(\"http://localhost:8000/api/state/full-state\").json()\n```\n\n### Generate Clients\n\nSee `references/openapi_schema.json` for OpenAPI v3.1.0 spec.\n\n```bash\n# Python\nopenapi-generator-cli generate -i openapi_schema.json -g python -o client/\n\n# TypeScript\nopenapi-typescript openapi_schema.json -o types.ts\n\n# Go, Rust, Java, etc. (50+ languages supported)\n```\n\n## App Management\n\n```python\n# Install app\nrequests.post(\"http://localhost:8000/api/apps/install\", json={\n    \"name\": \"hand_tracker\",\n    \"source\": \"hf_space\",\n    \"space_id\": \"pollen-robotics/hand_tracker_v2\"\n})\n\n# Start app\nrequests.post(\"http://localhost:8000/api/apps/start-app/hand_tracker\")\n```\n\n## Motor Modes\n\n```python\n# Compliant (manual movement)\nrequests.post(\"http://localhost:8000/api/motors/set-mode\", \n              json={\"mode\": \"disabled\"})\n\n# Active control\nrequests.post(\"http://localhost:8000/api/motors/set-mode\", \n              json={\"mode\": \"enabled\"})\n\n# Gravity compensation\nrequests.post(\"http://localhost:8000/api/motors/set-mode\", \n              json={\"mode\": \"gravity_compensation\"})\n```\n\n## AI Integration\n\nSee `references/ai_integration.md` for LLM patterns, vision models, multimodal apps, and HuggingFace deployment.\n\n### Example: Object Detection\n\n```python\nfrom transformers import pipeline\n\ndetector = pipeline(\"object-detection\")\nframe = mini.media.get_frame()\nresults = detector(frame)\n\n# React to detections\nfor obj in results:\n    if obj['label'] == 'person':\n        mini.goto_target(antennas=np.deg2rad([45, 45]), duration=0.5)\n```\n\n## Common Patterns\n\n### Greeting Sequence\n\n```python\ndef greet():\n    mini.goto_target(head=create_head_pose(z=5, mm=True), duration=0.5)\n    mini.goto_target(antennas=np.deg2rad([45, 45]), duration=0.5)\n    for _ in range(2):\n        mini.goto_target(head=create_head_pose(pitch=-10, degrees=True), duration=0.3)\n        mini.goto_target(head=create_head_pose(pitch=10, degrees=True), duration=0.3)\n```\n\n### Scanning Motion\n\n```python\nfor angle in [-60, -30, 0, 30, 60]:\n    mini.goto_target(\n        body_yaw=np.deg2rad(angle),\n        head=create_head_pose(z=5, mm=True),\n        duration=1.0\n    )\n```\n\n## Reference Files\n\n- **installation.md** - Setup for Wireless/Lite/Simulation\n- **movement_control.md** - Complete movement guide (450+ lines)\n- **sensors.md** - Camera, microphone, IMU access\n- **ai_integration.md** - AI models, LLMs, apps, deployment\n- **daemon_api.md** - REST API reference (500+ lines, 25+ endpoints)\n- **openapi_schema.json** - OpenAPI v3.1.0 spec for client generation\n- **openapi_usage.md** - Using OpenAPI for automation\n- **api_quick_reference.md** - Quick reference card\n\n## Platform Notes\n\n- **Wireless**: Raspberry Pi, WiFi, includes IMU, use `localhost_only=False` from PC\n- **Lite**: USB connection, no IMU, use `localhost_only=True`\n- **Simulation**: MuJoCo-based, no hardware needed\n\n## Safety\n\n- SDK enforces limits automatically\n- Test in simulation first\n- Use appropriate durations (0.5-2.0s typically)\n- Always use context managers (`with ReachyMini()`)\n\n## Version\n\nSDK v1.2.6, OpenAPI v3.1.0\n\nSource: https://github.com/pollen-robotics/reachy_mini/tree/1.2.6\n",
      "frontmatter": {
        "name": "reachy-mini-sdk",
        "description": "Programming guide for Reachy Mini robot using Python SDK v1.2.6 and REST API. Use when controlling Reachy Mini robots, programming movements (head/antennas/body), accessing sensors (camera/microphone/IMU), recording motions, building AI applications, deploying to Hugging Face, or using the daemon REST API. Covers SDK patterns, coordinate systems, interpolation methods, app management, and OpenAPI client generation.",
        "license": "MIT (see LICENSE.txt)"
      },
      "content": "# Reachy Mini SDK\n\nProgramming guide for Reachy Mini - an open-source desktop humanoid robot with 6-DOF head, expressive antennas, and AI integration.\n\n## Hardware\n\n- **Head**: 6-DOF Stewart platform (X,Y,Z + roll,pitch,yaw)\n- **Antennas**: 2 servos\n- **Body**: 360Â° yaw rotation\n- **Sensors**: Camera, microphone, IMU (Wireless only)\n\n**Daemon**: FastAPI on port 8000 (REST + WebSocket)\n\n## Quick Start\n\n### Installation\n\nSee `references/installation.md` for complete setup (uv/pip, platform-specific configs, permissions).\n\n### Basic Connection\n\n```python\nfrom reachy_mini import ReachyMini\n\n# Local\nwith ReachyMini() as mini:\n    pass\n\n# Remote (Wireless)\nwith ReachyMini(localhost_only=False) as mini:\n    pass\n```\n\n## Movement\n\nSee `references/movement_control.md` for complete guide (450+ lines with all patterns).\n\n### goto_target (Smooth Interpolation)\n\n```python\nfrom reachy_mini.utils import create_head_pose\nimport numpy as np\n\nmini.goto_target(\n    head=create_head_pose(z=10, roll=15, degrees=True, mm=True),\n    antennas=np.deg2rad([45, 45]),\n    body_yaw=np.deg2rad(30),\n    duration=2.0,\n    method=\"minjerk\"  # linear, ease, cartoon\n)\n```\n\n### set_target (Direct Control)\n\nFor high-frequency control (>30Hz):\n\n```python\nmini.set_target(\n    head=create_head_pose(z=5, mm=True),\n    antennas=[0.5, -0.5]\n)\n```\n\n### Coordinates\n\n- **Head**: Position in meters, orientation in radians\n- **Antennas**: Radians (Â±1.5)\n- **Body**: Radians (full 360Â°)\n\n## Sensors\n\nSee `references/sensors.md` for camera, audio, IMU details.\n\n```python\n# Camera (BGR numpy array)\nframe = mini.media.get_frame()\n\n# Audio (16kHz stereo)\nsamples = mini.media.get_audio_sample()\nmini.media.push_audio_sample(samples)  # Non-blocking\n\n# IMU (Wireless only)\nif hasattr(mini, 'imu'):\n    data = mini.imu.get_data()\n```\n\n## Motion Recording\n\n```python\nmini.start_recording()\n# Move robot\nmotion = mini.stop_recording()\nmotion.save(\"demo.pkl\")\n\n# Replay\nmotion.play()\n```\n\n## REST API\n\nSee `references/daemon_api.md` for all 25+ endpoints.\nSee `references/openapi_usage.md` for client generation.\n\n### Direct HTTP Control\n\n```python\nimport requests\n\n# Move via API\nrequests.post(\"http://localhost:8000/api/goto\", json={\n    \"head_pose\": {\"x\": 0, \"y\": 0, \"z\": 0.01, \"roll\": 0, \"pitch\": 0, \"yaw\": 0},\n    \"duration\": 2.0,\n    \"interpolation\": \"minjerk\"\n})\n\n# Get state\nstate = requests.get(\"http://localhost:8000/api/state/full-state\").json()\n```\n\n### Generate Clients\n\nSee `references/openapi_schema.json` for OpenAPI v3.1.0 spec.\n\n```bash\n# Python\nopenapi-generator-cli generate -i openapi_schema.json -g python -o client/\n\n# TypeScript\nopenapi-typescript openapi_schema.json -o types.ts\n\n# Go, Rust, Java, etc. (50+ languages supported)\n```\n\n## App Management\n\n```python\n# Install app\nrequests.post(\"http://localhost:8000/api/apps/install\", json={\n    \"name\": \"hand_tracker\",\n    \"source\": \"hf_space\",\n    \"space_id\": \"pollen-robotics/hand_tracker_v2\"\n})\n\n# Start app\nrequests.post(\"http://localhost:8000/api/apps/start-app/hand_tracker\")\n```\n\n## Motor Modes\n\n```python\n# Compliant (manual movement)\nrequests.post(\"http://localhost:8000/api/motors/set-mode\", \n              json={\"mode\": \"disabled\"})\n\n# Active control\nrequests.post(\"http://localhost:8000/api/motors/set-mode\", \n              json={\"mode\": \"enabled\"})\n\n# Gravity compensation\nrequests.post(\"http://localhost:8000/api/motors/set-mode\", \n              json={\"mode\": \"gravity_compensation\"})\n```\n\n## AI Integration\n\nSee `references/ai_integration.md` for LLM patterns, vision models, multimodal apps, and HuggingFace deployment.\n\n### Example: Object Detection\n\n```python\nfrom transformers import pipeline\n\ndetector = pipeline(\"object-detection\")\nframe = mini.media.get_frame()\nresults = detector(frame)\n\n# React to detections\nfor obj in results:\n    if obj['label'] == 'person':\n        mini.goto_target(antennas=np.deg2rad([45, 45]), duration=0.5)\n```\n\n## Common Patterns\n\n### Greeting Sequence\n\n```python\ndef greet():\n    mini.goto_target(head=create_head_pose(z=5, mm=True), duration=0.5)\n    mini.goto_target(antennas=np.deg2rad([45, 45]), duration=0.5)\n    for _ in range(2):\n        mini.goto_target(head=create_head_pose(pitch=-10, degrees=True), duration=0.3)\n        mini.goto_target(head=create_head_pose(pitch=10, degrees=True), duration=0.3)\n```\n\n### Scanning Motion\n\n```python\nfor angle in [-60, -30, 0, 30, 60]:\n    mini.goto_target(\n        body_yaw=np.deg2rad(angle),\n        head=create_head_pose(z=5, mm=True),\n        duration=1.0\n    )\n```\n\n## Reference Files\n\n- **installation.md** - Setup for Wireless/Lite/Simulation\n- **movement_control.md** - Complete movement guide (450+ lines)\n- **sensors.md** - Camera, microphone, IMU access\n- **ai_integration.md** - AI models, LLMs, apps, deployment\n- **daemon_api.md** - REST API reference (500+ lines, 25+ endpoints)\n- **openapi_schema.json** - OpenAPI v3.1.0 spec for client generation\n- **openapi_usage.md** - Using OpenAPI for automation\n- **api_quick_reference.md** - Quick reference card\n\n## Platform Notes\n\n- **Wireless**: Raspberry Pi, WiFi, includes IMU, use `localhost_only=False` from PC\n- **Lite**: USB connection, no IMU, use `localhost_only=True`\n- **Simulation**: MuJoCo-based, no hardware needed\n\n## Safety\n\n- SDK enforces limits automatically\n- Test in simulation first\n- Use appropriate durations (0.5-2.0s typically)\n- Always use context managers (`with ReachyMini()`)\n\n## Version\n\nSDK v1.2.6, OpenAPI v3.1.0\n\nSource: https://github.com/pollen-robotics/reachy_mini/tree/1.2.6"
    },
    "marketplace": {
      "hasMarketplaceJson": false
    },
    "internal": {
      "syncedAt": "2026-01-08T00:38:53.188Z",
      "version": 1
    }
  },
  "texmeijin-video-to-gif-skill": {
    "id": "texmeijin-video-to-gif-skill",
    "name": "video-to-gif",
    "description": "Convert multiple video files (MOV/MP4) into a single merged GIF with customizable speed per segment.\nUse this skill when users want to:\n- Merge multiple videos into one GIF\n- Create demo GIFs from screen recordings\n- Combine video clips with different playback speeds\n- Convert videos to optimized GIFs with compression\nTriggers: \"create GIF from videos\", \"merge videos to GIF\", \"convert MOV to GIF\", \"combine videos into animated GIF\"\n",
    "repo": {
      "owner": "TeXmeijin",
      "name": "video-to-gif-skill",
      "fullName": "TeXmeijin/video-to-gif-skill",
      "url": "https://github.com/TeXmeijin/video-to-gif-skill",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 0,
      "forks": 0,
      "language": "Shell",
      "topics": [
        "claude-code-skill",
        "claude-skills"
      ],
      "createdAt": "2026-01-04T08:58:02Z",
      "updatedAt": "2026-01-04T09:01:47Z",
      "pushedAt": "2026-01-04T09:00:58Z"
    },
    "category": "development",
    "tags": [
      "claude-code-skill",
      "claude-skills",
      "videos",
      "convert",
      "into",
      "with",
      "multiple"
    ],
    "skillMd": {
      "raw": "---\nname: video-to-gif\ndescription: |\n  Convert multiple video files (MOV/MP4) into a single merged GIF with customizable speed per segment.\n  Use this skill when users want to:\n  - Merge multiple videos into one GIF\n  - Create demo GIFs from screen recordings\n  - Combine video clips with different playback speeds\n  - Convert videos to optimized GIFs with compression\n  Triggers: \"create GIF from videos\", \"merge videos to GIF\", \"convert MOV to GIF\", \"combine videos into animated GIF\"\n---\n\n# Video to GIF Converter\n\nMerge multiple video files into a single optimized GIF with per-segment speed control.\n\n## Quick Start\n\n```bash\n.claude/skills/video-to-gif/scripts/merge_videos_to_gif.sh -o output.gif video1.mov:2 video2.mov:4.75 video3.mov:4.75\n```\n\n## Script Usage\n\n```\n.claude/skills/video-to-gif/scripts/merge_videos_to_gif.sh -o output.gif [options] video1:speed1 video2:speed2 ...\n```\n\n### Options\n\n| Option | Default | Description |\n|--------|---------|-------------|\n| `-o FILE` | (required) | Output GIF file path |\n| `-w WIDTH` | 800 | Output width in pixels |\n| `-h HEIGHT` | 338 | Output height in pixels |\n| `-f FPS` | 8 | Frames per second (lower = smaller file) |\n| `-c COLORS` | 128 | Max colors (64-256, lower = smaller file) |\n| `-l LOSSY` | 80 | Lossy compression 0-200 (higher = smaller file, more artifacts) |\n\n### Video Format\n\n`path/to/video.mov:speed_multiplier`\n\n- `1` = original speed\n- `2` = 2x faster (video plays in half the time)\n- `4.75` = 4.75x faster\n- `0.5` = half speed (slower playback)\n\n## Examples\n\n### Basic: Merge 3 videos with different speeds\n\nFirst video slower (2x), others fast (4.75x):\n\n```bash\n.claude/skills/video-to-gif/scripts/merge_videos_to_gif.sh -o demo.gif \\\n  ~/Desktop/intro.mov:2 \\\n  ~/Desktop/action.mov:4.75 \\\n  ~/Desktop/outro.mov:4.75\n```\n\n### Custom resolution and compression\n\nCreate a smaller GIF (640x360, 64 colors, high compression):\n\n```bash\n.claude/skills/video-to-gif/scripts/merge_videos_to_gif.sh -o small.gif -w 640 -h 360 -c 64 -l 120 \\\n  video1.mov:3 video2.mov:3\n```\n\n### Higher quality GIF\n\nMore colors and lower compression:\n\n```bash\n.claude/skills/video-to-gif/scripts/merge_videos_to_gif.sh -o hq.gif -f 10 -c 256 -l 40 \\\n  video.mov:2\n```\n\n## Dependencies\n\nRequired tools (install via Homebrew on macOS):\n\n```bash\nbrew install ffmpeg gifsicle\n```\n\n## Tips\n\n- **File size too large?** Reduce FPS (`-f 6`), colors (`-c 64`), or increase lossy (`-l 100`)\n- **Video looks choppy?** Increase FPS (`-f 12`) or reduce speed multiplier\n- **Black bars appearing?** Videos with different aspect ratios get padded to fit target dimensions\n- **First segment too fast?** Use a lower speed multiplier (e.g., `:1.5` instead of `:4`)\n",
      "frontmatter": {
        "name": "video-to-gif",
        "description": "Convert multiple video files (MOV/MP4) into a single merged GIF with customizable speed per segment.\nUse this skill when users want to:\n- Merge multiple videos into one GIF\n- Create demo GIFs from screen recordings\n- Combine video clips with different playback speeds\n- Convert videos to optimized GIFs with compression\nTriggers: \"create GIF from videos\", \"merge videos to GIF\", \"convert MOV to GIF\", \"combine videos into animated GIF\"\n"
      },
      "content": "# Video to GIF Converter\n\nMerge multiple video files into a single optimized GIF with per-segment speed control.\n\n## Quick Start\n\n```bash\n.claude/skills/video-to-gif/scripts/merge_videos_to_gif.sh -o output.gif video1.mov:2 video2.mov:4.75 video3.mov:4.75\n```\n\n## Script Usage\n\n```\n.claude/skills/video-to-gif/scripts/merge_videos_to_gif.sh -o output.gif [options] video1:speed1 video2:speed2 ...\n```\n\n### Options\n\n| Option | Default | Description |\n|--------|---------|-------------|\n| `-o FILE` | (required) | Output GIF file path |\n| `-w WIDTH` | 800 | Output width in pixels |\n| `-h HEIGHT` | 338 | Output height in pixels |\n| `-f FPS` | 8 | Frames per second (lower = smaller file) |\n| `-c COLORS` | 128 | Max colors (64-256, lower = smaller file) |\n| `-l LOSSY` | 80 | Lossy compression 0-200 (higher = smaller file, more artifacts) |\n\n### Video Format\n\n`path/to/video.mov:speed_multiplier`\n\n- `1` = original speed\n- `2` = 2x faster (video plays in half the time)\n- `4.75` = 4.75x faster\n- `0.5` = half speed (slower playback)\n\n## Examples\n\n### Basic: Merge 3 videos with different speeds\n\nFirst video slower (2x), others fast (4.75x):\n\n```bash\n.claude/skills/video-to-gif/scripts/merge_videos_to_gif.sh -o demo.gif \\\n  ~/Desktop/intro.mov:2 \\\n  ~/Desktop/action.mov:4.75 \\\n  ~/Desktop/outro.mov:4.75\n```\n\n### Custom resolution and compression\n\nCreate a smaller GIF (640x360, 64 colors, high compression):\n\n```bash\n.claude/skills/video-to-gif/scripts/merge_videos_to_gif.sh -o small.gif -w 640 -h 360 -c 64 -l 120 \\\n  video1.mov:3 video2.mov:3\n```\n\n### Higher quality GIF\n\nMore colors and lower compression:\n\n```bash\n.claude/skills/video-to-gif/scripts/merge_videos_to_gif.sh -o hq.gif -f 10 -c 256 -l 40 \\\n  video.mov:2\n```\n\n## Dependencies\n\nRequired tools (install via Homebrew on macOS):\n\n```bash\nbrew install ffmpeg gifsicle\n```\n\n## Tips\n\n- **File size too large?** Reduce FPS (`-f 6`), colors (`-c 64`), or increase lossy (`-l 100`)\n- **Video looks choppy?** Increase FPS (`-f 12`) or reduce speed multiplier\n- **Black bars appearing?** Videos with different aspect ratios get padded to fit target dimensions\n- **First segment too fast?** Use a lower speed multiplier (e.g., `:1.5` instead of `:4`)"
    },
    "marketplace": {
      "hasMarketplaceJson": false
    },
    "internal": {
      "syncedAt": "2026-01-08T00:39:12.182Z",
      "version": 1
    }
  },
  "finelagusaz-proofread-ja": {
    "id": "finelagusaz-proofread-ja",
    "name": "proofread-ja",
    "description": "æ—¥æœ¬èªžãƒ†ã‚­ã‚¹ãƒˆã®èª¤å­—è„±å­—ãƒã‚§ãƒƒã‚¯ã€è¡¨è¨˜æºã‚Œã®æ¤œå‡ºã¨ä¿®æ­£ã€‚å°èª¬ã€æŠ€è¡“æ–‡æ›¸ã€ãƒ–ãƒ­ã‚°è¨˜äº‹ãªã©ã§ä½¿ç”¨ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã€Œèª¤å­—è„±å­—ã‚’ãƒã‚§ãƒƒã‚¯ã€ã€Œè¡¨è¨˜ã®æºã‚Œã‚’ç¢ºèªã€ã€Œæ ¡æ­£ã—ã¦ã€ãªã©ã¨ä¾é ¼ã—ãŸæ™‚ã€ã¾ãŸã¯æ–‡æ›¸ã®å“è³ªå‘ä¸ŠãŒå¿…è¦ãªæ™‚ã«ä½¿ç”¨ã€‚",
    "repo": {
      "owner": "finelagusaz",
      "name": "proofread-ja",
      "fullName": "finelagusaz/proofread-ja",
      "url": "https://github.com/finelagusaz/proofread-ja",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 1,
      "forks": 0,
      "language": null,
      "topics": [
        "claude-code-skill",
        "claude-skills"
      ],
      "createdAt": "2025-11-13T00:39:53Z",
      "updatedAt": "2026-01-03T09:12:17Z",
      "pushedAt": "2026-01-03T03:53:59Z"
    },
    "category": "development",
    "tags": [
      "claude-code-skill",
      "claude-skills"
    ],
    "skillMd": {
      "raw": "---\nname: proofread-ja\ndescription: æ—¥æœ¬èªžãƒ†ã‚­ã‚¹ãƒˆã®èª¤å­—è„±å­—ãƒã‚§ãƒƒã‚¯ã€è¡¨è¨˜æºã‚Œã®æ¤œå‡ºã¨ä¿®æ­£ã€‚å°èª¬ã€æŠ€è¡“æ–‡æ›¸ã€ãƒ–ãƒ­ã‚°è¨˜äº‹ãªã©ã§ä½¿ç”¨ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã€Œèª¤å­—è„±å­—ã‚’ãƒã‚§ãƒƒã‚¯ã€ã€Œè¡¨è¨˜ã®æºã‚Œã‚’ç¢ºèªã€ã€Œæ ¡æ­£ã—ã¦ã€ãªã©ã¨ä¾é ¼ã—ãŸæ™‚ã€ã¾ãŸã¯æ–‡æ›¸ã®å“è³ªå‘ä¸ŠãŒå¿…è¦ãªæ™‚ã«ä½¿ç”¨ã€‚\n---\n\n# æ—¥æœ¬èªžæ ¡æ­£ã‚¹ã‚­ãƒ«\n\n## ã‚¹ã‚³ãƒ¼ãƒ—ã¨åˆ¶é™\n\n### å‡¦ç†å¯èƒ½ãªæ–‡å­—æ•°\n\n| æ–‡å­—æ•°          | å‡¦ç†æ–¹é‡                                                                 |\n|-----------------|--------------------------------------------------------------------------|\n| ã€œ3,000å­—       | å…¨æ–‡ã‚’ä¸€æ‹¬ãƒã‚§ãƒƒã‚¯                                                       |\n| 3,000ã€œ10,000å­— | ã‚»ã‚¯ã‚·ãƒ§ãƒ³å˜ä½ã§é †æ¬¡ãƒã‚§ãƒƒã‚¯ã€æœ€å¾Œã«å…¨ä½“ã®è¡¨è¨˜æºã‚Œã‚’ç¢ºèª                 |\n| 10,000å­—ã€œ      | ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«åˆ†å‰²ã‚’ææ¡ˆã€é‡ç‚¹ç®‡æ‰€ã®æŒ‡å®šã€ã¾ãŸã¯ã€Œè¡¨è¨˜æºã‚Œã®ã¿å…¨æ–‡ãƒã‚§ãƒƒã‚¯ã€ |\n\n### å‡¦ç†ãƒ¢ãƒ¼ãƒ‰\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¾é ¼ã«å¿œã˜ã¦ãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠžã™ã‚‹ã€‚\n\n| ãƒ¢ãƒ¼ãƒ‰       | ãƒˆãƒªã‚¬ãƒ¼ä¾‹                                         | å‡ºåŠ›                                  |\n|--------------|----------------------------------------------------|---------------------------------------|\n| è©³ç´°         | ã€Œã—ã£ã‹ã‚Šæ ¡æ­£ã—ã¦ã€ã€Œå…¨éƒ¨ãƒã‚§ãƒƒã‚¯ã€               | å…¨ã‚«ãƒ†ã‚´ãƒªã®æŒ‡æ‘˜ + ä¿®æ­£ç‰ˆãƒ†ã‚­ã‚¹ãƒˆ     |\n| æ¨™æº–         | ã€Œæ ¡æ­£ã—ã¦ã€ã€Œãƒã‚§ãƒƒã‚¯ã—ã¦ã€                       | ç¢ºä¿¡åº¦ï¼šé«˜ãƒ»ä¸­ã®æŒ‡æ‘˜ + ä¿®æ­£ç‰ˆãƒ†ã‚­ã‚¹ãƒˆ |\n| ç°¡æ˜“         | ã€Œè»½ããƒã‚§ãƒƒã‚¯ã€ã€Œã–ã£ã¨è¦‹ã¦ã€ã€Œæ˜Žã‚‰ã‹ãªèª¤å­—ã ã‘ã€ | ç¢ºä¿¡åº¦ï¼šé«˜ã®ã¿ã€ç®‡æ¡æ›¸ãã§å ±å‘Š        |\n| è¡¨è¨˜çµ±ä¸€ã®ã¿ | ã€Œè¡¨è¨˜æºã‚Œã ã‘ç¢ºèªã€                               | è¡¨è¨˜æºã‚Œã®ä¸€è¦§ã®ã¿                    |\n\n## ãƒã‚§ãƒƒã‚¯é …ç›®\n\n### 1. èª¤å­—è„±å­—ã®æ¤œå‡º\n\n- åŒéŸ³ç•°ç¾©èªžã®èª¤ç”¨ï¼ˆä¾‹ï¼šä»¥å¤–/æ„å¤–ã€ä½œã‚‹/é€ ã‚‹/å‰µã‚‹ï¼‰\n- ã‚¿ã‚¤ãƒï¼ˆã‚­ãƒ¼ãƒœãƒ¼ãƒ‰é…ç½®ã«ã‚ˆã‚‹èª¤å…¥åŠ›ï¼‰\n- é€ã‚Šä»®åã®èª¤ã‚Š\n- å¤‰æ›ãƒŸã‚¹\n\n### 2. è¡¨è¨˜ã®æºã‚Œãƒã‚§ãƒƒã‚¯\n\n- æ¼¢å­—/ã²ã‚‰ãŒãªè¡¨è¨˜ï¼ˆä¾‹ï¼šã€Œäº‹ã€ã¨ã€Œã“ã¨ã€ï¼‰\n- ã‚«ã‚¿ã‚«ãƒŠè¡¨è¨˜ï¼ˆä¾‹ï¼šã€Œã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã€ã¨ã€Œã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã€ï¼‰\n- è‹±æ•°å­—ã®å…¨è§’/åŠè§’\n- å¥èª­ç‚¹ã®ç¨®é¡žï¼ˆã€‚ã€ã¨ï¼Žï¼Œï¼‰\n- é•·éŸ³ç¬¦ã®æœ‰ç„¡ï¼ˆã‚µãƒ¼ãƒãƒ¼/ã‚µãƒ¼ãƒï¼‰\n\n### 3. æ–‡ä½“ã®ä¸€è²«æ€§\n\n- æ•¬ä½“ï¼ˆã§ã™ãƒ»ã¾ã™ï¼‰/å¸¸ä½“ï¼ˆã ãƒ»ã§ã‚ã‚‹ï¼‰ã®æ··åœ¨\n- ä¸€äººç§°ã®çµ±ä¸€\n- æŽ¥ç¶šè©žã®ä½¿ç”¨é »åº¦\n\n### 4. æ–‡æ³•ãƒ»èªžæ³•ã®èª¤ã‚Š\n\n- ã‚‰æŠœãè¨€è‘‰ï¼ˆè¦‹ã‚Œã‚‹â†’è¦‹ã‚‰ã‚Œã‚‹ï¼‰\n- ã•å…¥ã‚Œè¨€è‘‰ï¼ˆèª­ã¾ã•ã›ã¦â†’èª­ã¾ã›ã¦ï¼‰\n- é‡è¤‡è¡¨ç¾ï¼ˆé ­ç—›ãŒç—›ã„ï¼‰\n- åŠ©è©žã®èª¤ç”¨ãƒ»é‡è¤‡\n\n## ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼\n\n### 1. ãƒ†ã‚­ã‚¹ãƒˆã®åˆ†æž\n\næ–‡æ›¸ã®ç¨®é¡žã‚’åˆ¤å®šã—ã€é©åˆ‡ãªãƒã‚§ãƒƒã‚¯åŸºæº–ã‚’é¸æŠžã™ã‚‹ã€‚\n\n| æ–‡æ›¸ã‚¿ã‚¤ãƒ—     | åˆ¤å®šã®æ‰‹ãŒã‹ã‚Š               | ãƒã‚§ãƒƒã‚¯æ–¹é‡                                               |\n|----------------|------------------------------|------------------------------------------------------------|\n| å°èª¬ãƒ»ã‚¨ãƒƒã‚»ã‚¤ | ä¸€äººç§°èªžã‚Šã€æƒ…æ™¯æå†™ã€ä¼šè©±æ–‡ | ã²ã‚‰ãŒãªå¤šã‚è¨±å®¹ã€æ–‡ä½“ã®ä¸€è²«æ€§é‡è¦–ã€ä¼šè©±æ–‡å†…ã¯æ–¹è¨€ãƒ»å£èªžOK |\n| æŠ€è¡“æ–‡æ›¸       | ã‚³ãƒ¼ãƒ‰ã€APIã€æ‰‹é †èª¬æ˜Ž        | æ¼¢å­—å¤šã‚ã€ç”¨èªžã®çµ±ä¸€é‡è¦–ã€é•·éŸ³ç¬¦ã®çµ±ä¸€                     |\n| ãƒ–ãƒ­ã‚°ãƒ»è¨˜äº‹   | è¦‹å‡ºã—ã€èª­è€…ã¸ã®å‘¼ã³ã‹ã‘     | æ–‡ä½“ã®æ··åœ¨ã¯æ–‡è„ˆæ¬¡ç¬¬ã§è¨±å®¹                                 |\n| ãƒ“ã‚¸ãƒã‚¹æ–‡æ›¸   | æ•¬èªžã€å®›åã€ç½²å             | æ•¬èªžã®æ­£ç¢ºã•ã€äºŒé‡æ•¬èªžãƒã‚§ãƒƒã‚¯                             |\n\n### 2. è©³ç´°ãƒã‚§ãƒƒã‚¯ã®å®Ÿè¡Œ\n\nä»¥ä¸‹ã®ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§ã—ã¦ãƒã‚§ãƒƒã‚¯ã‚’è¡Œã†ï¼š\n\n1. **references/common-errors.md** - å…¸åž‹çš„ãªèª¤ã‚Šã®ãƒ‘ã‚¿ãƒ¼ãƒ³\n2. **references/consistency-rules.md** - è¡¨è¨˜çµ±ä¸€ã®åŸºæº–\n3. **references/custom-terms.md** - ãƒ¦ãƒ¼ã‚¶ãƒ¼å›ºæœ‰ã®ç”¨èªžï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰\n\nåˆ¤æ–­ã«è¿·ã†å ´åˆã¯ **examples/boundary-cases.md** ã‚’å‚ç…§ã€‚\n\n### 3. çµæžœã®å ±å‘Š\n\n#### ç¢ºä¿¡åº¦ã®å®šç¾©\n\n| ç¢ºä¿¡åº¦ | åŸºæº–                                             | ä¾‹                                                 |\n|--------|--------------------------------------------------|----------------------------------------------------|\n| é«˜     | æ˜Žã‚‰ã‹ãªèª¤ã‚Šã€‚æ–‡è„ˆã«é–¢ã‚ã‚‰ãšä¿®æ­£ã™ã¹ã           | ã€Œä»¥å¤–ã¨ç°¡å˜ã€â†’ã€Œæ„å¤–ã¨ã€ã€ã€Œè¦‹ã‚Œã‚‹ã€â†’ã€Œè¦‹ã‚‰ã‚Œã‚‹ã€ |\n| ä¸­     | è¡¨è¨˜æºã‚Œã€ã¾ãŸã¯æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã«ã‚ˆã£ã¦åˆ¤æ–­ãŒåˆ†ã‹ã‚Œã‚‹ | ã€Œã“ã¨ã€ã¨ã€Œäº‹ã€ã®æ··åœ¨ã€é•·éŸ³ç¬¦ã®ä¸çµ±ä¸€             |\n| ä½Ž     | æ„å›³çš„ãªå¯èƒ½æ€§ãŒã‚ã‚‹ã€ã¾ãŸã¯è¨±å®¹ã•ã‚Œã‚‹å ´åˆã‚‚ã‚ã‚‹ | ä¼šè©±æ–‡ä¸­ã®ã€Œã‚‰æŠœãã€ã€å£èªžçš„ãªã€Œå…¨ç„¶ã„ã„ã€         |\n\n#### å ±å‘Šãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆï¼ˆæ¨™æº–ãƒ¢ãƒ¼ãƒ‰ï¼‰\n\n```markdown\n## æ ¡æ­£çµæžœ\n\n### ä¿®æ­£ææ¡ˆï¼ˆç¢ºä¿¡åº¦ï¼šé«˜ï¼‰\n\n| ç®‡æ‰€    | ç¾åœ¨ã®è¡¨è¨˜ | ä¿®æ­£æ¡ˆ   | ç†ç”±                               |\n|---------|------------|----------|------------------------------------|\n| 2æ®µè½ç›® | ä»¥å¤–ã¨     | æ„å¤–ã¨   | åŒéŸ³ç•°ç¾©èªžï¼ˆã€Œæ€ã„ãŒã‘ãšã€ã®æ„å‘³ï¼‰ |\n| 5æ®µè½ç›® | è¦‹ã‚Œã‚‹     | è¦‹ã‚‰ã‚Œã‚‹ | ã‚‰æŠœãè¨€è‘‰                         |\n\n### è¡¨è¨˜æºã‚Œï¼ˆç¢ºä¿¡åº¦ï¼šä¸­ï¼‰\n\næ–‡æ›¸å†…ã§ä»¥ä¸‹ã®è¡¨è¨˜ãŒæ··åœ¨ã—ã¦ã„ã¾ã™ã€‚ã©ã¡ã‚‰ã‹ã«çµ±ä¸€ã—ã¦ãã ã•ã„ã€‚\n\n| è¡¨è¨˜A    | å‡ºç¾  | è¡¨è¨˜B  | å‡ºç¾  | æŽ¨å¥¨                           |\n|----------|-------|--------|-------|--------------------------------|\n| ã“ã¨     | 8ç®‡æ‰€ | äº‹     | 3ç®‡æ‰€ | ã€Œã“ã¨ã€ï¼ˆå½¢å¼åè©žã¯ã²ã‚‰ãŒãªï¼‰ |\n| ãƒ¦ãƒ¼ã‚¶ãƒ¼ | 5ç®‡æ‰€ | ãƒ¦ãƒ¼ã‚¶ | 2ç®‡æ‰€ | ã©ã¡ã‚‰ã§ã‚‚å¯ï¼ˆçµ±ä¸€ãŒå¿…è¦ï¼‰     |\n\n### è¦ç¢ºèªï¼ˆç¢ºä¿¡åº¦ï¼šä½Žï¼‰\n\n| ç®‡æ‰€   | è¡¨è¨˜           | ç¢ºèªäº‹é …                             |\n|--------|----------------|--------------------------------------|\n| ä¼šè©±æ–‡ | ã€Œå…¨ç„¶å¤§ä¸ˆå¤«ã€ | å£èªžè¡¨ç¾ã¨ã—ã¦æ„å›³çš„ã§ã‚ã‚Œã°å•é¡Œãªã— |\n\n---\n\n## ä¿®æ­£ç‰ˆ\n\nï¼ˆç¢ºä¿¡åº¦ï¼šé«˜ã®é …ç›®ã‚’åæ˜ ã€è¡¨è¨˜æºã‚Œã¯å¤šæ•°æ´¾ã«çµ±ä¸€ï¼‰\n\n[ä¿®æ­£å¾Œã®å…¨æ–‡ã‚’ã“ã“ã«å‡ºåŠ›]\n```\n\n#### å ±å‘Šãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆï¼ˆç°¡æ˜“ãƒ¢ãƒ¼ãƒ‰ï¼‰\n\n```markdown\n## æ ¡æ­£çµæžœï¼ˆç°¡æ˜“ãƒã‚§ãƒƒã‚¯ï¼‰\n\nä»¥ä¸‹ã®æ˜Žã‚‰ã‹ãªèª¤ã‚Šã‚’æ¤œå‡ºã—ã¾ã—ãŸï¼š\n\n- 2æ®µè½ç›®ã€Œä»¥å¤–ã¨ã€â†’ã€Œæ„å¤–ã¨ã€\n- 5æ®µè½ç›®ã€Œè¦‹ã‚Œã‚‹ã€â†’ã€Œè¦‹ã‚‰ã‚Œã‚‹ã€\n- 8æ®µè½ç›®ã€Œé ­ç—›ãŒç—›ã„ã€â†’ã€Œé ­ãŒç—›ã„ã€\n\nä¿®æ­£ç‰ˆãŒå¿…è¦ãªå ´åˆã¯ãŠçŸ¥ã‚‰ã›ãã ã•ã„ã€‚\n```\n\n### 4. ä¿®æ­£ç‰ˆã®æä¾›\n\n1. ç¢ºä¿¡åº¦ã€Œé«˜ã€ã®é …ç›®ã‚’åæ˜ \n2. è¡¨è¨˜æºã‚Œã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒé¸æŠžã—ãŸæ–¹é‡ã€ã¾ãŸã¯å¤šæ•°æ´¾ã«çµ±ä¸€\n3. ç¢ºä¿¡åº¦ã€Œä½Žã€ã®é …ç›®ã¯å…ƒã®ã¾ã¾ç¶­æŒï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæŒ‡ç¤ºã—ãŸå ´åˆã®ã¿ä¿®æ­£ï¼‰\n\nä¿®æ­£ç®‡æ‰€ã‚’æ˜Žç¤ºã™ã‚‹å ´åˆã¯ `ã€åŽŸæ–‡ï¼šã€œã€‘` å½¢å¼ã§ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã‚³ãƒ¡ãƒ³ãƒˆã‚’ä»˜ã‘ã‚‹ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå¸Œæœ›ã—ãŸå ´åˆã®ã¿ï¼‰ã€‚\n\n## åˆ¤æ–­åŸºæº–\n\n### ä¿®æ­£ã™ã¹ãï¼ˆç¢ºä¿¡åº¦ï¼šé«˜ï¼‰\n\n- æ˜Žã‚‰ã‹ãªèª¤å­—è„±å­—ï¼ˆå¤‰æ›ãƒŸã‚¹ã€ã‚¿ã‚¤ãƒï¼‰\n- åŒéŸ³ç•°ç¾©èªžã®èª¤ç”¨ã§æ„å‘³ãŒé€šã‚‰ãªã„\n- æ–‡æ³•çš„ãªèª¤ã‚Šï¼ˆæ´»ç”¨å½¢ã®èª¤ã‚Šï¼‰\n- äºŒé‡æ•¬èªžï¼ˆãƒ“ã‚¸ãƒã‚¹æ–‡æ›¸ã®å ´åˆï¼‰\n- æ„å‘³ãŒå¤‰ã‚ã‚‹èª¤ã‚Š\n\n### ç¢ºèªãŒå¿…è¦ï¼ˆç¢ºä¿¡åº¦ï¼šä¸­ï¼‰\n\n- è¡¨è¨˜æºã‚Œï¼ˆã©ã¡ã‚‰ã‚‚æ­£ã—ã„å ´åˆï¼‰\n- æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã«ã‚ˆã£ã¦åˆ¤æ–­ãŒåˆ†ã‹ã‚Œã‚‹ã‚‚ã®\n- æ¼¢å­—/ã²ã‚‰ãŒãªã®é¸æŠž\n\n### æŒ‡æ‘˜ã®ã¿ï¼ˆç¢ºä¿¡åº¦ï¼šä½Žï¼‰\n\n- ä¼šè©±æ–‡ãƒ»å°è©žå†…ã®å£èªžè¡¨ç¾\n- æ–‡ä½“ã®æ„å›³çš„ãªæ··åœ¨ã®å¯èƒ½æ€§\n- ä½œè€…ã®å€‹æ€§ã¨ã—ã¦è¨±å®¹ã•ã‚Œã‚‹è¡¨ç¾\n\n### ä¿®æ­£ã—ãªã„\n\n- å¼•ç”¨æ–‡ä¸­ã®è¡¨è¨˜ï¼ˆåŽŸæ–‡ãƒžãƒžï¼‰\n- å›ºæœ‰åè©žï¼ˆäººåã€ä½œå“åã€å•†å“åï¼‰\n- æ˜Žç¤ºçš„ã«ã€Œè¨±å®¹ã€ã¨è¨­å®šã•ã‚ŒãŸè¡¨ç¾ï¼ˆcustom-terms.mdï¼‰\n- å°èª¬ã®ç™»å ´äººç‰©ã®å£èª¿ãƒ»æ–¹è¨€\n\n## å„ªå…ˆé †ä½\n\nå¤§é‡ã®å•é¡ŒãŒã‚ã‚‹å ´åˆã®å ±å‘Šé †åºï¼š\n\n1. **ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«**: æ„å‘³ãŒå¤‰ã‚ã‚‹èª¤å­—ï¼ˆåŒéŸ³ç•°ç¾©èªžï¼‰\n2. **é‡è¦**: æ–‡æ³•ã‚¨ãƒ©ãƒ¼ï¼ˆã‚‰æŠœãã€ã•å…¥ã‚Œã€æ´»ç”¨å½¢ï¼‰\n3. **ä¸­ç¨‹åº¦**: è¡¨è¨˜æºã‚Œï¼ˆçµ±ä¸€ãŒå¿…è¦ãªç®‡æ‰€ï¼‰\n4. **è»½å¾®**: ã‚¹ã‚¿ã‚¤ãƒ«ã®å•é¡Œï¼ˆå†—é•·è¡¨ç¾ã€å¥èª­ç‚¹ï¼‰\n\næŒ‡æ‘˜ãŒ20ä»¶ã‚’è¶…ãˆã‚‹å ´åˆã¯ã€ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ»é‡è¦ã‚’å„ªå…ˆã—ã€è»½å¾®ãªå•é¡Œã¯ã€Œä»–ã«â—‹ä»¶ã®è»½å¾®ãªæŒ‡æ‘˜ãŒã‚ã‚Šã¾ã™ã€ã¨ã¾ã¨ã‚ã‚‹ã€‚\n\n## å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«\n\n| ãƒ•ã‚¡ã‚¤ãƒ«                        | å†…å®¹                                   |\n|---------------------------------|----------------------------------------|\n| references/common-errors.md     | ã‚ˆãã‚ã‚‹èª¤å­—è„±å­—ã®ãƒ‘ã‚¿ãƒ¼ãƒ³é›†           |\n| references/consistency-rules.md | è¡¨è¨˜çµ±ä¸€ã®åˆ¤æ–­åŸºæº–ã¨ãƒ«ãƒ¼ãƒ«             |\n| references/custom-terms.md      | ãƒ¦ãƒ¼ã‚¶ãƒ¼å›ºæœ‰ã®ç”¨èªžé›†ï¼ˆç·¨é›†å¯èƒ½ï¼‰       |\n| examples/boundary-cases.md      | åˆ¤æ–­ã®å¢ƒç•Œä¾‹ï¼ˆç›´ã™/ç¢ºèª/ç¶­æŒã®åˆ†å²ç‚¹ï¼‰ |\n| examples/output-sample.md       | å‡ºåŠ›å½¢å¼ã®å…·ä½“ä¾‹                       |\n",
      "frontmatter": {
        "name": "proofread-ja",
        "description": "æ—¥æœ¬èªžãƒ†ã‚­ã‚¹ãƒˆã®èª¤å­—è„±å­—ãƒã‚§ãƒƒã‚¯ã€è¡¨è¨˜æºã‚Œã®æ¤œå‡ºã¨ä¿®æ­£ã€‚å°èª¬ã€æŠ€è¡“æ–‡æ›¸ã€ãƒ–ãƒ­ã‚°è¨˜äº‹ãªã©ã§ä½¿ç”¨ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã€Œèª¤å­—è„±å­—ã‚’ãƒã‚§ãƒƒã‚¯ã€ã€Œè¡¨è¨˜ã®æºã‚Œã‚’ç¢ºèªã€ã€Œæ ¡æ­£ã—ã¦ã€ãªã©ã¨ä¾é ¼ã—ãŸæ™‚ã€ã¾ãŸã¯æ–‡æ›¸ã®å“è³ªå‘ä¸ŠãŒå¿…è¦ãªæ™‚ã«ä½¿ç”¨ã€‚"
      },
      "content": "# æ—¥æœ¬èªžæ ¡æ­£ã‚¹ã‚­ãƒ«\n\n## ã‚¹ã‚³ãƒ¼ãƒ—ã¨åˆ¶é™\n\n### å‡¦ç†å¯èƒ½ãªæ–‡å­—æ•°\n\n| æ–‡å­—æ•°          | å‡¦ç†æ–¹é‡                                                                 |\n|-----------------|--------------------------------------------------------------------------|\n| ã€œ3,000å­—       | å…¨æ–‡ã‚’ä¸€æ‹¬ãƒã‚§ãƒƒã‚¯                                                       |\n| 3,000ã€œ10,000å­— | ã‚»ã‚¯ã‚·ãƒ§ãƒ³å˜ä½ã§é †æ¬¡ãƒã‚§ãƒƒã‚¯ã€æœ€å¾Œã«å…¨ä½“ã®è¡¨è¨˜æºã‚Œã‚’ç¢ºèª                 |\n| 10,000å­—ã€œ      | ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«åˆ†å‰²ã‚’ææ¡ˆã€é‡ç‚¹ç®‡æ‰€ã®æŒ‡å®šã€ã¾ãŸã¯ã€Œè¡¨è¨˜æºã‚Œã®ã¿å…¨æ–‡ãƒã‚§ãƒƒã‚¯ã€ |\n\n### å‡¦ç†ãƒ¢ãƒ¼ãƒ‰\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¾é ¼ã«å¿œã˜ã¦ãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠžã™ã‚‹ã€‚\n\n| ãƒ¢ãƒ¼ãƒ‰       | ãƒˆãƒªã‚¬ãƒ¼ä¾‹                                         | å‡ºåŠ›                                  |\n|--------------|----------------------------------------------------|---------------------------------------|\n| è©³ç´°         | ã€Œã—ã£ã‹ã‚Šæ ¡æ­£ã—ã¦ã€ã€Œå…¨éƒ¨ãƒã‚§ãƒƒã‚¯ã€               | å…¨ã‚«ãƒ†ã‚´ãƒªã®æŒ‡æ‘˜ + ä¿®æ­£ç‰ˆãƒ†ã‚­ã‚¹ãƒˆ     |\n| æ¨™æº–         | ã€Œæ ¡æ­£ã—ã¦ã€ã€Œãƒã‚§ãƒƒã‚¯ã—ã¦ã€                       | ç¢ºä¿¡åº¦ï¼šé«˜ãƒ»ä¸­ã®æŒ‡æ‘˜ + ä¿®æ­£ç‰ˆãƒ†ã‚­ã‚¹ãƒˆ |\n| ç°¡æ˜“         | ã€Œè»½ããƒã‚§ãƒƒã‚¯ã€ã€Œã–ã£ã¨è¦‹ã¦ã€ã€Œæ˜Žã‚‰ã‹ãªèª¤å­—ã ã‘ã€ | ç¢ºä¿¡åº¦ï¼šé«˜ã®ã¿ã€ç®‡æ¡æ›¸ãã§å ±å‘Š        |\n| è¡¨è¨˜çµ±ä¸€ã®ã¿ | ã€Œè¡¨è¨˜æºã‚Œã ã‘ç¢ºèªã€                               | è¡¨è¨˜æºã‚Œã®ä¸€è¦§ã®ã¿                    |\n\n## ãƒã‚§ãƒƒã‚¯é …ç›®\n\n### 1. èª¤å­—è„±å­—ã®æ¤œå‡º\n\n- åŒéŸ³ç•°ç¾©èªžã®èª¤ç”¨ï¼ˆä¾‹ï¼šä»¥å¤–/æ„å¤–ã€ä½œã‚‹/é€ ã‚‹/å‰µã‚‹ï¼‰\n- ã‚¿ã‚¤ãƒï¼ˆã‚­ãƒ¼ãƒœãƒ¼ãƒ‰é…ç½®ã«ã‚ˆã‚‹èª¤å…¥åŠ›ï¼‰\n- é€ã‚Šä»®åã®èª¤ã‚Š\n- å¤‰æ›ãƒŸã‚¹\n\n### 2. è¡¨è¨˜ã®æºã‚Œãƒã‚§ãƒƒã‚¯\n\n- æ¼¢å­—/ã²ã‚‰ãŒãªè¡¨è¨˜ï¼ˆä¾‹ï¼šã€Œäº‹ã€ã¨ã€Œã“ã¨ã€ï¼‰\n- ã‚«ã‚¿ã‚«ãƒŠè¡¨è¨˜ï¼ˆä¾‹ï¼šã€Œã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã€ã¨ã€Œã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã€ï¼‰\n- è‹±æ•°å­—ã®å…¨è§’/åŠè§’\n- å¥èª­ç‚¹ã®ç¨®é¡žï¼ˆã€‚ã€ã¨ï¼Žï¼Œï¼‰\n- é•·éŸ³ç¬¦ã®æœ‰ç„¡ï¼ˆã‚µãƒ¼ãƒãƒ¼/ã‚µãƒ¼ãƒï¼‰\n\n### 3. æ–‡ä½“ã®ä¸€è²«æ€§\n\n- æ•¬ä½“ï¼ˆã§ã™ãƒ»ã¾ã™ï¼‰/å¸¸ä½“ï¼ˆã ãƒ»ã§ã‚ã‚‹ï¼‰ã®æ··åœ¨\n- ä¸€äººç§°ã®çµ±ä¸€\n- æŽ¥ç¶šè©žã®ä½¿ç”¨é »åº¦\n\n### 4. æ–‡æ³•ãƒ»èªžæ³•ã®èª¤ã‚Š\n\n- ã‚‰æŠœãè¨€è‘‰ï¼ˆè¦‹ã‚Œã‚‹â†’è¦‹ã‚‰ã‚Œã‚‹ï¼‰\n- ã•å…¥ã‚Œè¨€è‘‰ï¼ˆèª­ã¾ã•ã›ã¦â†’èª­ã¾ã›ã¦ï¼‰\n- é‡è¤‡è¡¨ç¾ï¼ˆé ­ç—›ãŒç—›ã„ï¼‰\n- åŠ©è©žã®èª¤ç”¨ãƒ»é‡è¤‡\n\n## ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼\n\n### 1. ãƒ†ã‚­ã‚¹ãƒˆã®åˆ†æž\n\næ–‡æ›¸ã®ç¨®é¡žã‚’åˆ¤å®šã—ã€é©åˆ‡ãªãƒã‚§ãƒƒã‚¯åŸºæº–ã‚’é¸æŠžã™ã‚‹ã€‚\n\n| æ–‡æ›¸ã‚¿ã‚¤ãƒ—     | åˆ¤å®šã®æ‰‹ãŒã‹ã‚Š               | ãƒã‚§ãƒƒã‚¯æ–¹é‡                                               |\n|----------------|------------------------------|------------------------------------------------------------|\n| å°èª¬ãƒ»ã‚¨ãƒƒã‚»ã‚¤ | ä¸€äººç§°èªžã‚Šã€æƒ…æ™¯æå†™ã€ä¼šè©±æ–‡ | ã²ã‚‰ãŒãªå¤šã‚è¨±å®¹ã€æ–‡ä½“ã®ä¸€è²«æ€§é‡è¦–ã€ä¼šè©±æ–‡å†…ã¯æ–¹è¨€ãƒ»å£èªžOK |\n| æŠ€è¡“æ–‡æ›¸       | ã‚³ãƒ¼ãƒ‰ã€APIã€æ‰‹é †èª¬æ˜Ž        | æ¼¢å­—å¤šã‚ã€ç”¨èªžã®çµ±ä¸€é‡è¦–ã€é•·éŸ³ç¬¦ã®çµ±ä¸€                     |\n| ãƒ–ãƒ­ã‚°ãƒ»è¨˜äº‹   | è¦‹å‡ºã—ã€èª­è€…ã¸ã®å‘¼ã³ã‹ã‘     | æ–‡ä½“ã®æ··åœ¨ã¯æ–‡è„ˆæ¬¡ç¬¬ã§è¨±å®¹                                 |\n| ãƒ“ã‚¸ãƒã‚¹æ–‡æ›¸   | æ•¬èªžã€å®›åã€ç½²å             | æ•¬èªžã®æ­£ç¢ºã•ã€äºŒé‡æ•¬èªžãƒã‚§ãƒƒã‚¯                             |\n\n### 2. è©³ç´°ãƒã‚§ãƒƒã‚¯ã®å®Ÿè¡Œ\n\nä»¥ä¸‹ã®ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§ã—ã¦ãƒã‚§ãƒƒã‚¯ã‚’è¡Œã†ï¼š\n\n1. **references/common-errors.md** - å…¸åž‹çš„ãªèª¤ã‚Šã®ãƒ‘ã‚¿ãƒ¼ãƒ³\n2. **references/consistency-rules.md** - è¡¨è¨˜çµ±ä¸€ã®åŸºæº–\n3. **references/custom-terms.md** - ãƒ¦ãƒ¼ã‚¶ãƒ¼å›ºæœ‰ã®ç”¨èªžï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰\n\nåˆ¤æ–­ã«è¿·ã†å ´åˆã¯ **examples/boundary-cases.md** ã‚’å‚ç…§ã€‚\n\n### 3. çµæžœã®å ±å‘Š\n\n#### ç¢ºä¿¡åº¦ã®å®šç¾©\n\n| ç¢ºä¿¡åº¦ | åŸºæº–                                             | ä¾‹                                                 |\n|--------|--------------------------------------------------|----------------------------------------------------|\n| é«˜     | æ˜Žã‚‰ã‹ãªèª¤ã‚Šã€‚æ–‡è„ˆã«é–¢ã‚ã‚‰ãšä¿®æ­£ã™ã¹ã           | ã€Œä»¥å¤–ã¨ç°¡å˜ã€â†’ã€Œæ„å¤–ã¨ã€ã€ã€Œè¦‹ã‚Œã‚‹ã€â†’ã€Œè¦‹ã‚‰ã‚Œã‚‹ã€ |\n| ä¸­     | è¡¨è¨˜æºã‚Œã€ã¾ãŸã¯æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã«ã‚ˆã£ã¦åˆ¤æ–­ãŒåˆ†ã‹ã‚Œã‚‹ | ã€Œã“ã¨ã€ã¨ã€Œäº‹ã€ã®æ··åœ¨ã€é•·éŸ³ç¬¦ã®ä¸çµ±ä¸€             |\n| ä½Ž     | æ„å›³çš„ãªå¯èƒ½æ€§ãŒã‚ã‚‹ã€ã¾ãŸã¯è¨±å®¹ã•ã‚Œã‚‹å ´åˆã‚‚ã‚ã‚‹ | ä¼šè©±æ–‡ä¸­ã®ã€Œã‚‰æŠœãã€ã€å£èªžçš„ãªã€Œå…¨ç„¶ã„ã„ã€         |\n\n#### å ±å‘Šãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆï¼ˆæ¨™æº–ãƒ¢ãƒ¼ãƒ‰ï¼‰\n\n```markdown\n## æ ¡æ­£çµæžœ\n\n### ä¿®æ­£ææ¡ˆï¼ˆç¢ºä¿¡åº¦ï¼šé«˜ï¼‰\n\n| ç®‡æ‰€    | ç¾åœ¨ã®è¡¨è¨˜ | ä¿®æ­£æ¡ˆ   | ç†ç”±                               |\n|---------|------------|----------|------------------------------------|\n| 2æ®µè½ç›® | ä»¥å¤–ã¨     | æ„å¤–ã¨   | åŒéŸ³ç•°ç¾©èªžï¼ˆã€Œæ€ã„ãŒã‘ãšã€ã®æ„å‘³ï¼‰ |\n| 5æ®µè½ç›® | è¦‹ã‚Œã‚‹     | è¦‹ã‚‰ã‚Œã‚‹ | ã‚‰æŠœãè¨€è‘‰                         |\n\n### è¡¨è¨˜æºã‚Œï¼ˆç¢ºä¿¡åº¦ï¼šä¸­ï¼‰\n\næ–‡æ›¸å†…ã§ä»¥ä¸‹ã®è¡¨è¨˜ãŒæ··åœ¨ã—ã¦ã„ã¾ã™ã€‚ã©ã¡ã‚‰ã‹ã«çµ±ä¸€ã—ã¦ãã ã•ã„ã€‚\n\n| è¡¨è¨˜A    | å‡ºç¾  | è¡¨è¨˜B  | å‡ºç¾  | æŽ¨å¥¨                           |\n|----------|-------|--------|-------|--------------------------------|\n| ã“ã¨     | 8ç®‡æ‰€ | äº‹     | 3ç®‡æ‰€ | ã€Œã“ã¨ã€ï¼ˆå½¢å¼åè©žã¯ã²ã‚‰ãŒãªï¼‰ |\n| ãƒ¦ãƒ¼ã‚¶ãƒ¼ | 5ç®‡æ‰€ | ãƒ¦ãƒ¼ã‚¶ | 2ç®‡æ‰€ | ã©ã¡ã‚‰ã§ã‚‚å¯ï¼ˆçµ±ä¸€ãŒå¿…è¦ï¼‰     |\n\n### è¦ç¢ºèªï¼ˆç¢ºä¿¡åº¦ï¼šä½Žï¼‰\n\n| ç®‡æ‰€   | è¡¨è¨˜           | ç¢ºèªäº‹é …                             |\n|--------|----------------|--------------------------------------|\n| ä¼šè©±æ–‡ | ã€Œå…¨ç„¶å¤§ä¸ˆå¤«ã€ | å£èªžè¡¨ç¾ã¨ã—ã¦æ„å›³çš„ã§ã‚ã‚Œã°å•é¡Œãªã— |\n\n---\n\n## ä¿®æ­£ç‰ˆ\n\nï¼ˆç¢ºä¿¡åº¦ï¼šé«˜ã®é …ç›®ã‚’åæ˜ ã€è¡¨è¨˜æºã‚Œã¯å¤šæ•°æ´¾ã«çµ±ä¸€ï¼‰\n\n[ä¿®æ­£å¾Œã®å…¨æ–‡ã‚’ã“ã“ã«å‡ºåŠ›]\n```\n\n#### å ±å‘Šãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆï¼ˆç°¡æ˜“ãƒ¢ãƒ¼ãƒ‰ï¼‰\n\n```markdown\n## æ ¡æ­£çµæžœï¼ˆç°¡æ˜“ãƒã‚§ãƒƒã‚¯ï¼‰\n\nä»¥ä¸‹ã®æ˜Žã‚‰ã‹ãªèª¤ã‚Šã‚’æ¤œå‡ºã—ã¾ã—ãŸï¼š\n\n- 2æ®µè½ç›®ã€Œä»¥å¤–ã¨ã€â†’ã€Œæ„å¤–ã¨ã€\n- 5æ®µè½ç›®ã€Œè¦‹ã‚Œã‚‹ã€â†’ã€Œè¦‹ã‚‰ã‚Œã‚‹ã€\n- 8æ®µè½ç›®ã€Œé ­ç—›ãŒç—›ã„ã€â†’ã€Œé ­ãŒç—›ã„ã€\n\nä¿®æ­£ç‰ˆãŒå¿…è¦ãªå ´åˆã¯ãŠçŸ¥ã‚‰ã›ãã ã•ã„ã€‚\n```\n\n### 4. ä¿®æ­£ç‰ˆã®æä¾›\n\n1. ç¢ºä¿¡åº¦ã€Œé«˜ã€ã®é …ç›®ã‚’åæ˜ \n2. è¡¨è¨˜æºã‚Œã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒé¸æŠžã—ãŸæ–¹é‡ã€ã¾ãŸã¯å¤šæ•°æ´¾ã«çµ±ä¸€\n3. ç¢ºä¿¡åº¦ã€Œä½Žã€ã®é …ç›®ã¯å…ƒã®ã¾ã¾ç¶­æŒï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæŒ‡ç¤ºã—ãŸå ´åˆã®ã¿ä¿®æ­£ï¼‰\n\nä¿®æ­£ç®‡æ‰€ã‚’æ˜Žç¤ºã™ã‚‹å ´åˆã¯ `ã€åŽŸæ–‡ï¼šã€œã€‘` å½¢å¼ã§ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã‚³ãƒ¡ãƒ³ãƒˆã‚’ä»˜ã‘ã‚‹ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå¸Œæœ›ã—ãŸå ´åˆã®ã¿ï¼‰ã€‚\n\n## åˆ¤æ–­åŸºæº–\n\n### ä¿®æ­£ã™ã¹ãï¼ˆç¢ºä¿¡åº¦ï¼šé«˜ï¼‰\n\n- æ˜Žã‚‰ã‹ãªèª¤å­—è„±å­—ï¼ˆå¤‰æ›ãƒŸã‚¹ã€ã‚¿ã‚¤ãƒï¼‰\n- åŒéŸ³ç•°ç¾©èªžã®èª¤ç”¨ã§æ„å‘³ãŒé€šã‚‰ãªã„\n- æ–‡æ³•çš„ãªèª¤ã‚Šï¼ˆæ´»ç”¨å½¢ã®èª¤ã‚Šï¼‰\n- äºŒé‡æ•¬èªžï¼ˆãƒ“ã‚¸ãƒã‚¹æ–‡æ›¸ã®å ´åˆï¼‰\n- æ„å‘³ãŒå¤‰ã‚ã‚‹èª¤ã‚Š\n\n### ç¢ºèªãŒå¿…è¦ï¼ˆç¢ºä¿¡åº¦ï¼šä¸­ï¼‰\n\n- è¡¨è¨˜æºã‚Œï¼ˆã©ã¡ã‚‰ã‚‚æ­£ã—ã„å ´åˆï¼‰\n- æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã«ã‚ˆã£ã¦åˆ¤æ–­ãŒåˆ†ã‹ã‚Œã‚‹ã‚‚ã®\n- æ¼¢å­—/ã²ã‚‰ãŒãªã®é¸æŠž\n\n### æŒ‡æ‘˜ã®ã¿ï¼ˆç¢ºä¿¡åº¦ï¼šä½Žï¼‰\n\n- ä¼šè©±æ–‡ãƒ»å°è©žå†…ã®å£èªžè¡¨ç¾\n- æ–‡ä½“ã®æ„å›³çš„ãªæ··åœ¨ã®å¯èƒ½æ€§\n- ä½œè€…ã®å€‹æ€§ã¨ã—ã¦è¨±å®¹ã•ã‚Œã‚‹è¡¨ç¾\n\n### ä¿®æ­£ã—ãªã„\n\n- å¼•ç”¨æ–‡ä¸­ã®è¡¨è¨˜ï¼ˆåŽŸæ–‡ãƒžãƒžï¼‰\n- å›ºæœ‰åè©žï¼ˆäººåã€ä½œå“åã€å•†å“åï¼‰\n- æ˜Žç¤ºçš„ã«ã€Œè¨±å®¹ã€ã¨è¨­å®šã•ã‚ŒãŸè¡¨ç¾ï¼ˆcustom-terms.mdï¼‰\n- å°èª¬ã®ç™»å ´äººç‰©ã®å£èª¿ãƒ»æ–¹è¨€\n\n## å„ªå…ˆé †ä½\n\nå¤§é‡ã®å•é¡ŒãŒã‚ã‚‹å ´åˆã®å ±å‘Šé †åºï¼š\n\n1. **ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«**: æ„å‘³ãŒå¤‰ã‚ã‚‹èª¤å­—ï¼ˆåŒéŸ³ç•°ç¾©èªžï¼‰\n2. **é‡è¦**: æ–‡æ³•ã‚¨ãƒ©ãƒ¼ï¼ˆã‚‰æŠœãã€ã•å…¥ã‚Œã€æ´»ç”¨å½¢ï¼‰\n3. **ä¸­ç¨‹åº¦**: è¡¨è¨˜æºã‚Œï¼ˆçµ±ä¸€ãŒå¿…è¦ãªç®‡æ‰€ï¼‰\n4. **è»½å¾®**: ã‚¹ã‚¿ã‚¤ãƒ«ã®å•é¡Œï¼ˆå†—é•·è¡¨ç¾ã€å¥èª­ç‚¹ï¼‰\n\næŒ‡æ‘˜ãŒ20ä»¶ã‚’è¶…ãˆã‚‹å ´åˆã¯ã€ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ»é‡è¦ã‚’å„ªå…ˆã—ã€è»½å¾®ãªå•é¡Œã¯ã€Œä»–ã«â—‹ä»¶ã®è»½å¾®ãªæŒ‡æ‘˜ãŒã‚ã‚Šã¾ã™ã€ã¨ã¾ã¨ã‚ã‚‹ã€‚\n\n## å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«\n\n| ãƒ•ã‚¡ã‚¤ãƒ«                        | å†…å®¹                                   |\n|---------------------------------|----------------------------------------|\n| references/common-errors.md     | ã‚ˆãã‚ã‚‹èª¤å­—è„±å­—ã®ãƒ‘ã‚¿ãƒ¼ãƒ³é›†           |\n| references/consistency-rules.md | è¡¨è¨˜çµ±ä¸€ã®åˆ¤æ–­åŸºæº–ã¨ãƒ«ãƒ¼ãƒ«             |\n| references/custom-terms.md      | ãƒ¦ãƒ¼ã‚¶ãƒ¼å›ºæœ‰ã®ç”¨èªžé›†ï¼ˆç·¨é›†å¯èƒ½ï¼‰       |\n| examples/boundary-cases.md      | åˆ¤æ–­ã®å¢ƒç•Œä¾‹ï¼ˆç›´ã™/ç¢ºèª/ç¶­æŒã®åˆ†å²ç‚¹ï¼‰ |\n| examples/output-sample.md       | å‡ºåŠ›å½¢å¼ã®å…·ä½“ä¾‹                       |"
    },
    "marketplace": {
      "hasMarketplaceJson": false
    },
    "internal": {
      "syncedAt": "2026-01-08T00:41:28.041Z",
      "version": 1
    }
  },
  "abra5umente-radarr-skill": {
    "id": "abra5umente-radarr-skill",
    "name": "radarr",
    "description": "Manage movies via Radarr - search, add, monitor downloads, check wanted list. Use when user asks about movies to download, checking download queue, adding films to library, or managing their Radarr instance. Triggers on mentions of Radarr, movie downloads, adding movies, download queue, wanted movies.",
    "repo": {
      "owner": "abra5umente",
      "name": "radarr-skill",
      "fullName": "abra5umente/radarr-skill",
      "url": "https://github.com/abra5umente/radarr-skill",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 0,
      "forks": 0,
      "language": "Python",
      "topics": [
        "ai",
        "claude",
        "claude-skills",
        "claude-skills-library",
        "homelab",
        "radarr"
      ],
      "createdAt": "2026-01-02T05:16:20Z",
      "updatedAt": "2026-01-02T05:30:14Z",
      "pushedAt": "2026-01-02T05:30:11Z",
      "license": "MIT License"
    },
    "category": "data-ai",
    "tags": [
      "ai",
      "claude",
      "claude-skills",
      "claude-skills-library",
      "homelab",
      "radarr",
      "movies",
      "download",
      "downloads",
      "wanted"
    ],
    "skillMd": {
      "raw": "---\nname: radarr\ndescription: Manage movies via Radarr - search, add, monitor downloads, check wanted list. Use when user asks about movies to download, checking download queue, adding films to library, or managing their Radarr instance. Triggers on mentions of Radarr, movie downloads, adding movies, download queue, wanted movies.\n---\n\n# Radarr Skill\n\nManage user's Radarr movie library via proxy. Large results save to disk (metadata only returned) to preserve context.\n\n## Scripts\n\nAll at `/mnt/skills/user/radarr/scripts/`\n\n### radarr.py\n\nMain interface for all Radarr operations.\n\n**Large results (movies, releases, queue, wanted) return metadata only. Full data saved to file.**\n\n```bash\n# Search for movies (returns full results - typically small)\npython3 radarr.py search \"The Matrix\" 1999\npython3 radarr.py search \"Interstellar\"\n\n# List library (returns count + path only)\npython3 radarr.py movies                    # All movies\npython3 radarr.py movies true               # Monitored only\n\n# Then grep the saved file to find specific movies\ngrep -i \"hotel\" /home/claude/radarr/movies_*.json\n\n# Get movie details (returns full result)\npython3 radarr.py movie 123\n\n# Add movie by TMDB ID (get ID from search results)\npython3 radarr.py add 603                   # The Matrix\n\n# Search for releases (returns count + path only)\npython3 radarr.py releases 123\n# Then grep for quality/size\ngrep -i \"1080p\\|bluray\" /home/claude/radarr/releases_*.json\n\n# Download a release\npython3 radarr.py download \"release-guid\" 123\n\n# Check download queue (returns count + path only)\npython3 radarr.py queue\ngrep -i \"title\" /home/claude/radarr/queue_*.json\n\n# Get wanted/missing movies (returns count + path only)\npython3 radarr.py wanted\n\n# System status (returns full result)\npython3 radarr.py status\n```\n\n### storage.py\n\nManage cached results.\n\n```bash\npython3 storage.py list              # Show all cached results\npython3 storage.py get <filename>    # Load specific result\npython3 storage.py clear             # Clear cache\n```\n\n## Cache Location\n\n```\n/home/claude/radarr/\nâ”œâ”€â”€ search_*.json\nâ”œâ”€â”€ movies_*.json\nâ”œâ”€â”€ releases_*.json\nâ”œâ”€â”€ queue_*.json\nâ””â”€â”€ manifest.json\n```\n\n## Typical Workflows\n\n### Check if a movie is in library\n```bash\n# 1. Fetch library (saves to file, returns count only)\npython3 radarr.py movies\n# 2. Grep the file for the movie\ngrep -i \"hotel transylvania\" /home/claude/radarr/movies_*.json\n```\n\n### Find and add a movie\n```bash\n# 1. Search (returns full results)\npython3 radarr.py search \"Dune\" 2021\n# 2. Note the tmdb_id from results\n# 3. Add it\npython3 radarr.py add 438631\n```\n\n### Check what's downloading\n```bash\npython3 radarr.py queue\n# If items exist, grep for details\ngrep -i \"title\\|progress\" /home/claude/radarr/queue_*.json\n```\n\n### Find missing movies\n```bash\npython3 radarr.py wanted\ngrep -i \"title\" /home/claude/radarr/wanted_*.json\n```\n\n### Manually grab a release\n```bash\n# 1. Get movie ID from library\npython3 radarr.py movies\ngrep -i \"movie name\" /home/claude/radarr/movies_*.json | grep '\"id\"'\n# 2. Search releases\npython3 radarr.py releases 123\n# 3. Find a good release\ngrep -i \"1080p\" /home/claude/radarr/releases_*.json | head -20\n# 4. Download preferred release (get guid from grep output)\npython3 radarr.py download \"release-guid-here\" 123\n```\n\n## Notes\n\n- **Large results (movies, releases, queue, wanted) return metadata only** - full data in file\n- **Small results (search, status, movie details, add) return full data**\n- Grep the saved files directly, don't pipe stdout\n- TMDB IDs are used for adding movies (shown in search results)\n- Movie IDs (internal Radarr IDs) are used for releases/details\n- Quality profiles and root folders use Radarr defaults\n- Files are timestamped, use `*.json` glob to find latest\n",
      "frontmatter": {
        "name": "radarr",
        "description": "Manage movies via Radarr - search, add, monitor downloads, check wanted list. Use when user asks about movies to download, checking download queue, adding films to library, or managing their Radarr instance. Triggers on mentions of Radarr, movie downloads, adding movies, download queue, wanted movies."
      },
      "content": "# Radarr Skill\n\nManage user's Radarr movie library via proxy. Large results save to disk (metadata only returned) to preserve context.\n\n## Scripts\n\nAll at `/mnt/skills/user/radarr/scripts/`\n\n### radarr.py\n\nMain interface for all Radarr operations.\n\n**Large results (movies, releases, queue, wanted) return metadata only. Full data saved to file.**\n\n```bash\n# Search for movies (returns full results - typically small)\npython3 radarr.py search \"The Matrix\" 1999\npython3 radarr.py search \"Interstellar\"\n\n# List library (returns count + path only)\npython3 radarr.py movies                    # All movies\npython3 radarr.py movies true               # Monitored only\n\n# Then grep the saved file to find specific movies\ngrep -i \"hotel\" /home/claude/radarr/movies_*.json\n\n# Get movie details (returns full result)\npython3 radarr.py movie 123\n\n# Add movie by TMDB ID (get ID from search results)\npython3 radarr.py add 603                   # The Matrix\n\n# Search for releases (returns count + path only)\npython3 radarr.py releases 123\n# Then grep for quality/size\ngrep -i \"1080p\\|bluray\" /home/claude/radarr/releases_*.json\n\n# Download a release\npython3 radarr.py download \"release-guid\" 123\n\n# Check download queue (returns count + path only)\npython3 radarr.py queue\ngrep -i \"title\" /home/claude/radarr/queue_*.json\n\n# Get wanted/missing movies (returns count + path only)\npython3 radarr.py wanted\n\n# System status (returns full result)\npython3 radarr.py status\n```\n\n### storage.py\n\nManage cached results.\n\n```bash\npython3 storage.py list              # Show all cached results\npython3 storage.py get <filename>    # Load specific result\npython3 storage.py clear             # Clear cache\n```\n\n## Cache Location\n\n```\n/home/claude/radarr/\nâ”œâ”€â”€ search_*.json\nâ”œâ”€â”€ movies_*.json\nâ”œâ”€â”€ releases_*.json\nâ”œâ”€â”€ queue_*.json\nâ””â”€â”€ manifest.json\n```\n\n## Typical Workflows\n\n### Check if a movie is in library\n```bash\n# 1. Fetch library (saves to file, returns count only)\npython3 radarr.py movies\n# 2. Grep the file for the movie\ngrep -i \"hotel transylvania\" /home/claude/radarr/movies_*.json\n```\n\n### Find and add a movie\n```bash\n# 1. Search (returns full results)\npython3 radarr.py search \"Dune\" 2021\n# 2. Note the tmdb_id from results\n# 3. Add it\npython3 radarr.py add 438631\n```\n\n### Check what's downloading\n```bash\npython3 radarr.py queue\n# If items exist, grep for details\ngrep -i \"title\\|progress\" /home/claude/radarr/queue_*.json\n```\n\n### Find missing movies\n```bash\npython3 radarr.py wanted\ngrep -i \"title\" /home/claude/radarr/wanted_*.json\n```\n\n### Manually grab a release\n```bash\n# 1. Get movie ID from library\npython3 radarr.py movies\ngrep -i \"movie name\" /home/claude/radarr/movies_*.json | grep '\"id\"'\n# 2. Search releases\npython3 radarr.py releases 123\n# 3. Find a good release\ngrep -i \"1080p\" /home/claude/radarr/releases_*.json | head -20\n# 4. Download preferred release (get guid from grep output)\npython3 radarr.py download \"release-guid-here\" 123\n```\n\n## Notes\n\n- **Large results (movies, releases, queue, wanted) return metadata only** - full data in file\n- **Small results (search, status, movie details, add) return full data**\n- Grep the saved files directly, don't pipe stdout\n- TMDB IDs are used for adding movies (shown in search results)\n- Movie IDs (internal Radarr IDs) are used for releases/details\n- Quality profiles and root folders use Radarr defaults\n- Files are timestamped, use `*.json` glob to find latest"
    },
    "marketplace": {
      "hasMarketplaceJson": false
    },
    "internal": {
      "syncedAt": "2026-01-08T00:41:42.987Z",
      "version": 1
    }
  },
  "anthropics-skills-algorithmic-art": {
    "id": "anthropics-skills-algorithmic-art",
    "name": "algorithmic-art",
    "description": "Creating algorithmic art using p5.js with seeded randomness and interactive parameter exploration. Use this when users request creating art using code, generative art, algorithmic art, flow fields, or particle systems. Create original algorithmic art rather than copying existing artists' work to avoid copyright violations.",
    "repo": {
      "owner": "anthropics",
      "name": "skills",
      "fullName": "anthropics/skills",
      "url": "https://github.com/anthropics/skills/tree/main/skills/algorithmic-art",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 35374,
      "forks": 3202,
      "language": "TypeScript",
      "topics": [],
      "updatedAt": "2026-01-08T06:00:29Z",
      "pushedAt": "2025-12-20T18:09:45Z"
    },
    "category": "official",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: algorithmic-art\ndescription: Creating algorithmic art using p5.js with seeded randomness and interactive parameter exploration. Use this when users request creating art using code, generative art, algorithmic art, flow fields, or particle systems. Create original algorithmic art rather than copying existing artists' work to avoid copyright violations.\nlicense: Complete terms in LICENSE.txt\n---\n\nAlgorithmic philosophies are computational aesthetic movements that are then expressed through code. Output .md files (philosophy), .html files (interactive viewer), and .js files (generative algorithms).\n\nThis happens in two steps:\n1. Algorithmic Philosophy Creation (.md file)\n2. Express by creating p5.js generative art (.html + .js files)\n\nFirst, undertake this task:\n\n## ALGORITHMIC PHILOSOPHY CREATION\n\nTo begin, create an ALGORITHMIC PHILOSOPHY (not static images or templates) that will be interpreted through:\n- Computational processes, emergent behavior, mathematical beauty\n- Seeded randomness, noise fields, organic systems\n- Particles, flows, fields, forces\n- Parametric variation and controlled chaos\n\n### THE CRITICAL UNDERSTANDING\n- What is received: Some subtle input or instructions by the user to take into account, but use as a foundation; it should not constrain creative freedom.\n- What is created: An algorithmic philosophy/generative aesthetic movement.\n- What happens next: The same version receives the philosophy and EXPRESSES IT IN CODE - creating p5.js sketches that are 90% algorithmic generation, 10% essential parameters.\n\nConsider this approach:\n- Write a manifesto for a generative art movement\n- The next phase involves writing the algorithm that brings it to life\n\nThe philosophy must emphasize: Algorithmic expression. Emergent behavior. Computational beauty. Seeded variation.\n\n### HOW TO GENERATE AN ALGORITHMIC PHILOSOPHY\n\n**Name the movement** (1-2 words): \"Organic Turbulence\" / \"Quantum Harmonics\" / \"Emergent Stillness\"\n\n**Articulate the philosophy** (4-6 paragraphs - concise but complete):\n\nTo capture the ALGORITHMIC essence, express how this philosophy manifests through:\n- Computational processes and mathematical relationships?\n- Noise functions and randomness patterns?\n- Particle behaviors and field dynamics?\n- Temporal evolution and system states?\n- Parametric variation and emergent complexity?\n\n**CRITICAL GUIDELINES:**\n- **Avoid redundancy**: Each algorithmic aspect should be mentioned once. Avoid repeating concepts about noise theory, particle dynamics, or mathematical principles unless adding new depth.\n- **Emphasize craftsmanship REPEATEDLY**: The philosophy MUST stress multiple times that the final algorithm should appear as though it took countless hours to develop, was refined with care, and comes from someone at the absolute top of their field. This framing is essential - repeat phrases like \"meticulously crafted algorithm,\" \"the product of deep computational expertise,\" \"painstaking optimization,\" \"master-level implementation.\"\n- **Leave creative space**: Be specific about the algorithmic direction, but concise enough that the next Claude has room to make interpretive implementation choices at an extremely high level of craftsmanship.\n\nThe philosophy must guide the next version to express ideas ALGORITHMICALLY, not through static images. Beauty lives in the process, not the final frame.\n\n### PHILOSOPHY EXAMPLES\n\n**\"Organic Turbulence\"**\nPhilosophy: Chaos constrained by natural law, order emerging from disorder.\nAlgorithmic expression: Flow fields driven by layered Perlin noise. Thousands of particles following vector forces, their trails accumulating into organic density maps. Multiple noise octaves create turbulent regions and calm zones. Color emerges from velocity and density - fast particles burn bright, slow ones fade to shadow. The algorithm runs until equilibrium - a meticulously tuned balance where every parameter was refined through countless iterations by a master of computational aesthetics.\n\n**\"Quantum Harmonics\"**\nPhilosophy: Discrete entities exhibiting wave-like interference patterns.\nAlgorithmic expression: Particles initialized on a grid, each carrying a phase value that evolves through sine waves. When particles are near, their phases interfere - constructive interference creates bright nodes, destructive creates voids. Simple harmonic motion generates complex emergent mandalas. The result of painstaking frequency calibration where every ratio was carefully chosen to produce resonant beauty.\n\n**\"Recursive Whispers\"**\nPhilosophy: Self-similarity across scales, infinite depth in finite space.\nAlgorithmic expression: Branching structures that subdivide recursively. Each branch slightly randomized but constrained by golden ratios. L-systems or recursive subdivision generate tree-like forms that feel both mathematical and organic. Subtle noise perturbations break perfect symmetry. Line weights diminish with each recursion level. Every branching angle the product of deep mathematical exploration.\n\n**\"Field Dynamics\"**\nPhilosophy: Invisible forces made visible through their effects on matter.\nAlgorithmic expression: Vector fields constructed from mathematical functions or noise. Particles born at edges, flowing along field lines, dying when they reach equilibrium or boundaries. Multiple fields can attract, repel, or rotate particles. The visualization shows only the traces - ghost-like evidence of invisible forces. A computational dance meticulously choreographed through force balance.\n\n**\"Stochastic Crystallization\"**\nPhilosophy: Random processes crystallizing into ordered structures.\nAlgorithmic expression: Randomized circle packing or Voronoi tessellation. Start with random points, let them evolve through relaxation algorithms. Cells push apart until equilibrium. Color based on cell size, neighbor count, or distance from center. The organic tiling that emerges feels both random and inevitable. Every seed produces unique crystalline beauty - the mark of a master-level generative algorithm.\n\n*These are condensed examples. The actual algorithmic philosophy should be 4-6 substantial paragraphs.*\n\n### ESSENTIAL PRINCIPLES\n- **ALGORITHMIC PHILOSOPHY**: Creating a computational worldview to be expressed through code\n- **PROCESS OVER PRODUCT**: Always emphasize that beauty emerges from the algorithm's execution - each run is unique\n- **PARAMETRIC EXPRESSION**: Ideas communicate through mathematical relationships, forces, behaviors - not static composition\n- **ARTISTIC FREEDOM**: The next Claude interprets the philosophy algorithmically - provide creative implementation room\n- **PURE GENERATIVE ART**: This is about making LIVING ALGORITHMS, not static images with randomness\n- **EXPERT CRAFTSMANSHIP**: Repeatedly emphasize the final algorithm must feel meticulously crafted, refined through countless iterations, the product of deep expertise by someone at the absolute top of their field in computational aesthetics\n\n**The algorithmic philosophy should be 4-6 paragraphs long.** Fill it with poetic computational philosophy that brings together the intended vision. Avoid repeating the same points. Output this algorithmic philosophy as a .md file.\n\n---\n\n## DEDUCING THE CONCEPTUAL SEED\n\n**CRITICAL STEP**: Before implementing the algorithm, identify the subtle conceptual thread from the original request.\n\n**THE ESSENTIAL PRINCIPLE**:\nThe concept is a **subtle, niche reference embedded within the algorithm itself** - not always literal, always sophisticated. Someone familiar with the subject should feel it intuitively, while others simply experience a masterful generative composition. The algorithmic philosophy provides the computational language. The deduced concept provides the soul - the quiet conceptual DNA woven invisibly into parameters, behaviors, and emergence patterns.\n\nThis is **VERY IMPORTANT**: The reference must be so refined that it enhances the work's depth without announcing itself. Think like a jazz musician quoting another song through algorithmic harmony - only those who know will catch it, but everyone appreciates the generative beauty.\n\n---\n\n## P5.JS IMPLEMENTATION\n\nWith the philosophy AND conceptual framework established, express it through code. Pause to gather thoughts before proceeding. Use only the algorithmic philosophy created and the instructions below.\n\n### âš ï¸ STEP 0: READ THE TEMPLATE FIRST âš ï¸\n\n**CRITICAL: BEFORE writing any HTML:**\n\n1. **Read** `templates/viewer.html` using the Read tool\n2. **Study** the exact structure, styling, and Anthropic branding\n3. **Use that file as the LITERAL STARTING POINT** - not just inspiration\n4. **Keep all FIXED sections exactly as shown** (header, sidebar structure, Anthropic colors/fonts, seed controls, action buttons)\n5. **Replace only the VARIABLE sections** marked in the file's comments (algorithm, parameters, UI controls for parameters)\n\n**Avoid:**\n- âŒ Creating HTML from scratch\n- âŒ Inventing custom styling or color schemes\n- âŒ Using system fonts or dark themes\n- âŒ Changing the sidebar structure\n\n**Follow these practices:**\n- âœ… Copy the template's exact HTML structure\n- âœ… Keep Anthropic branding (Poppins/Lora fonts, light colors, gradient backdrop)\n- âœ… Maintain the sidebar layout (Seed â†’ Parameters â†’ Colors? â†’ Actions)\n- âœ… Replace only the p5.js algorithm and parameter controls\n\nThe template is the foundation. Build on it, don't rebuild it.\n\n---\n\nTo create gallery-quality computational art that lives and breathes, use the algorithmic philosophy as the foundation.\n\n### TECHNICAL REQUIREMENTS\n\n**Seeded Randomness (Art Blocks Pattern)**:\n```javascript\n// ALWAYS use a seed for reproducibility\nlet seed = 12345; // or hash from user input\nrandomSeed(seed);\nnoiseSeed(seed);\n```\n\n**Parameter Structure - FOLLOW THE PHILOSOPHY**:\n\nTo establish parameters that emerge naturally from the algorithmic philosophy, consider: \"What qualities of this system can be adjusted?\"\n\n```javascript\nlet params = {\n  seed: 12345,  // Always include seed for reproducibility\n  // colors\n  // Add parameters that control YOUR algorithm:\n  // - Quantities (how many?)\n  // - Scales (how big? how fast?)\n  // - Probabilities (how likely?)\n  // - Ratios (what proportions?)\n  // - Angles (what direction?)\n  // - Thresholds (when does behavior change?)\n};\n```\n\n**To design effective parameters, focus on the properties the system needs to be tunable rather than thinking in terms of \"pattern types\".**\n\n**Core Algorithm - EXPRESS THE PHILOSOPHY**:\n\n**CRITICAL**: The algorithmic philosophy should dictate what to build.\n\nTo express the philosophy through code, avoid thinking \"which pattern should I use?\" and instead think \"how to express this philosophy through code?\"\n\nIf the philosophy is about **organic emergence**, consider using:\n- Elements that accumulate or grow over time\n- Random processes constrained by natural rules\n- Feedback loops and interactions\n\nIf the philosophy is about **mathematical beauty**, consider using:\n- Geometric relationships and ratios\n- Trigonometric functions and harmonics\n- Precise calculations creating unexpected patterns\n\nIf the philosophy is about **controlled chaos**, consider using:\n- Random variation within strict boundaries\n- Bifurcation and phase transitions\n- Order emerging from disorder\n\n**The algorithm flows from the philosophy, not from a menu of options.**\n\nTo guide the implementation, let the conceptual essence inform creative and original choices. Build something that expresses the vision for this particular request.\n\n**Canvas Setup**: Standard p5.js structure:\n```javascript\nfunction setup() {\n  createCanvas(1200, 1200);\n  // Initialize your system\n}\n\nfunction draw() {\n  // Your generative algorithm\n  // Can be static (noLoop) or animated\n}\n```\n\n### CRAFTSMANSHIP REQUIREMENTS\n\n**CRITICAL**: To achieve mastery, create algorithms that feel like they emerged through countless iterations by a master generative artist. Tune every parameter carefully. Ensure every pattern emerges with purpose. This is NOT random noise - this is CONTROLLED CHAOS refined through deep expertise.\n\n- **Balance**: Complexity without visual noise, order without rigidity\n- **Color Harmony**: Thoughtful palettes, not random RGB values\n- **Composition**: Even in randomness, maintain visual hierarchy and flow\n- **Performance**: Smooth execution, optimized for real-time if animated\n- **Reproducibility**: Same seed ALWAYS produces identical output\n\n### OUTPUT FORMAT\n\nOutput:\n1. **Algorithmic Philosophy** - As markdown or text explaining the generative aesthetic\n2. **Single HTML Artifact** - Self-contained interactive generative art built from `templates/viewer.html` (see STEP 0 and next section)\n\nThe HTML artifact contains everything: p5.js (from CDN), the algorithm, parameter controls, and UI - all in one file that works immediately in claude.ai artifacts or any browser. Start from the template file, not from scratch.\n\n---\n\n## INTERACTIVE ARTIFACT CREATION\n\n**REMINDER: `templates/viewer.html` should have already been read (see STEP 0). Use that file as the starting point.**\n\nTo allow exploration of the generative art, create a single, self-contained HTML artifact. Ensure this artifact works immediately in claude.ai or any browser - no setup required. Embed everything inline.\n\n### CRITICAL: WHAT'S FIXED VS VARIABLE\n\nThe `templates/viewer.html` file is the foundation. It contains the exact structure and styling needed.\n\n**FIXED (always include exactly as shown):**\n- Layout structure (header, sidebar, main canvas area)\n- Anthropic branding (UI colors, fonts, gradients)\n- Seed section in sidebar:\n  - Seed display\n  - Previous/Next buttons\n  - Random button\n  - Jump to seed input + Go button\n- Actions section in sidebar:\n  - Regenerate button\n  - Reset button\n\n**VARIABLE (customize for each artwork):**\n- The entire p5.js algorithm (setup/draw/classes)\n- The parameters object (define what the art needs)\n- The Parameters section in sidebar:\n  - Number of parameter controls\n  - Parameter names\n  - Min/max/step values for sliders\n  - Control types (sliders, inputs, etc.)\n- Colors section (optional):\n  - Some art needs color pickers\n  - Some art might use fixed colors\n  - Some art might be monochrome (no color controls needed)\n  - Decide based on the art's needs\n\n**Every artwork should have unique parameters and algorithm!** The fixed parts provide consistent UX - everything else expresses the unique vision.\n\n### REQUIRED FEATURES\n\n**1. Parameter Controls**\n- Sliders for numeric parameters (particle count, noise scale, speed, etc.)\n- Color pickers for palette colors\n- Real-time updates when parameters change\n- Reset button to restore defaults\n\n**2. Seed Navigation**\n- Display current seed number\n- \"Previous\" and \"Next\" buttons to cycle through seeds\n- \"Random\" button for random seed\n- Input field to jump to specific seed\n- Generate 100 variations when requested (seeds 1-100)\n\n**3. Single Artifact Structure**\n```html\n<!DOCTYPE html>\n<html>\n<head>\n  <!-- p5.js from CDN - always available -->\n  <script src=\"https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.7.0/p5.min.js\"></script>\n  <style>\n    /* All styling inline - clean, minimal */\n    /* Canvas on top, controls below */\n  </style>\n</head>\n<body>\n  <div id=\"canvas-container\"></div>\n  <div id=\"controls\">\n    <!-- All parameter controls -->\n  </div>\n  <script>\n    // ALL p5.js code inline here\n    // Parameter objects, classes, functions\n    // setup() and draw()\n    // UI handlers\n    // Everything self-contained\n  </script>\n</body>\n</html>\n```\n\n**CRITICAL**: This is a single artifact. No external files, no imports (except p5.js CDN). Everything inline.\n\n**4. Implementation Details - BUILD THE SIDEBAR**\n\nThe sidebar structure:\n\n**1. Seed (FIXED)** - Always include exactly as shown:\n- Seed display\n- Prev/Next/Random/Jump buttons\n\n**2. Parameters (VARIABLE)** - Create controls for the art:\n```html\n<div class=\"control-group\">\n    <label>Parameter Name</label>\n    <input type=\"range\" id=\"param\" min=\"...\" max=\"...\" step=\"...\" value=\"...\" oninput=\"updateParam('param', this.value)\">\n    <span class=\"value-display\" id=\"param-value\">...</span>\n</div>\n```\nAdd as many control-group divs as there are parameters.\n\n**3. Colors (OPTIONAL/VARIABLE)** - Include if the art needs adjustable colors:\n- Add color pickers if users should control palette\n- Skip this section if the art uses fixed colors\n- Skip if the art is monochrome\n\n**4. Actions (FIXED)** - Always include exactly as shown:\n- Regenerate button\n- Reset button\n- Download PNG button\n\n**Requirements**:\n- Seed controls must work (prev/next/random/jump/display)\n- All parameters must have UI controls\n- Regenerate, Reset, Download buttons must work\n- Keep Anthropic branding (UI styling, not art colors)\n\n### USING THE ARTIFACT\n\nThe HTML artifact works immediately:\n1. **In claude.ai**: Displayed as an interactive artifact - runs instantly\n2. **As a file**: Save and open in any browser - no server needed\n3. **Sharing**: Send the HTML file - it's completely self-contained\n\n---\n\n## VARIATIONS & EXPLORATION\n\nThe artifact includes seed navigation by default (prev/next/random buttons), allowing users to explore variations without creating multiple files. If the user wants specific variations highlighted:\n\n- Include seed presets (buttons for \"Variation 1: Seed 42\", \"Variation 2: Seed 127\", etc.)\n- Add a \"Gallery Mode\" that shows thumbnails of multiple seeds side-by-side\n- All within the same single artifact\n\nThis is like creating a series of prints from the same plate - the algorithm is consistent, but each seed reveals different facets of its potential. The interactive nature means users discover their own favorites by exploring the seed space.\n\n---\n\n## THE CREATIVE PROCESS\n\n**User request** â†’ **Algorithmic philosophy** â†’ **Implementation**\n\nEach request is unique. The process involves:\n\n1. **Interpret the user's intent** - What aesthetic is being sought?\n2. **Create an algorithmic philosophy** (4-6 paragraphs) describing the computational approach\n3. **Implement it in code** - Build the algorithm that expresses this philosophy\n4. **Design appropriate parameters** - What should be tunable?\n5. **Build matching UI controls** - Sliders/inputs for those parameters\n\n**The constants**:\n- Anthropic branding (colors, fonts, layout)\n- Seed navigation (always present)\n- Self-contained HTML artifact\n\n**Everything else is variable**:\n- The algorithm itself\n- The parameters\n- The UI controls\n- The visual outcome\n\nTo achieve the best results, trust creativity and let the philosophy guide the implementation.\n\n---\n\n## RESOURCES\n\nThis skill includes helpful templates and documentation:\n\n- **templates/viewer.html**: REQUIRED STARTING POINT for all HTML artifacts.\n  - This is the foundation - contains the exact structure and Anthropic branding\n  - **Keep unchanged**: Layout structure, sidebar organization, Anthropic colors/fonts, seed controls, action buttons\n  - **Replace**: The p5.js algorithm, parameter definitions, and UI controls in Parameters section\n  - The extensive comments in the file mark exactly what to keep vs replace\n\n- **templates/generator_template.js**: Reference for p5.js best practices and code structure principles.\n  - Shows how to organize parameters, use seeded randomness, structure classes\n  - NOT a pattern menu - use these principles to build unique algorithms\n  - Embed algorithms inline in the HTML artifact (don't create separate .js files)\n\n**Critical reminder**:\n- The **template is the STARTING POINT**, not inspiration\n- The **algorithm is where to create** something unique\n- Don't copy the flow field example - build what the philosophy demands\n- But DO keep the exact UI structure and Anthropic branding from the template",
      "frontmatter": {
        "name": "algorithmic-art",
        "description": "Creating algorithmic art using p5.js with seeded randomness and interactive parameter exploration. Use this when users request creating art using code, generative art, algorithmic art, flow fields, or particle systems. Create original algorithmic art rather than copying existing artists' work to avoid copyright violations.",
        "license": "Complete terms in LICENSE.txt"
      },
      "content": "\nAlgorithmic philosophies are computational aesthetic movements that are then expressed through code. Output .md files (philosophy), .html files (interactive viewer), and .js files (generative algorithms).\n\nThis happens in two steps:\n1. Algorithmic Philosophy Creation (.md file)\n2. Express by creating p5.js generative art (.html + .js files)\n\nFirst, undertake this task:\n\n## ALGORITHMIC PHILOSOPHY CREATION\n\nTo begin, create an ALGORITHMIC PHILOSOPHY (not static images or templates) that will be interpreted through:\n- Computational processes, emergent behavior, mathematical beauty\n- Seeded randomness, noise fields, organic systems\n- Particles, flows, fields, forces\n- Parametric variation and controlled chaos\n\n### THE CRITICAL UNDERSTANDING\n- What is received: Some subtle input or instructions by the user to take into account, but use as a foundation; it should not constrain creative freedom.\n- What is created: An algorithmic philosophy/generative aesthetic movement.\n- What happens next: The same version receives the philosophy and EXPRESSES IT IN CODE - creating p5.js sketches that are 90% algorithmic generation, 10% essential parameters.\n\nConsider this approach:\n- Write a manifesto for a generative art movement\n- The next phase involves writing the algorithm that brings it to life\n\nThe philosophy must emphasize: Algorithmic expression. Emergent behavior. Computational beauty. Seeded variation.\n\n### HOW TO GENERATE AN ALGORITHMIC PHILOSOPHY\n\n**Name the movement** (1-2 words): \"Organic Turbulence\" / \"Quantum Harmonics\" / \"Emergent Stillness\"\n\n**Articulate the philosophy** (4-6 paragraphs - concise but complete):\n\nTo capture the ALGORITHMIC essence, express how this philosophy manifests through:\n- Computational processes and mathematical relationships?\n- Noise functions and randomness patterns?\n- Particle behaviors and field dynamics?\n- Temporal evolution and system states?\n- Parametric variation and emergent complexity?\n\n**CRITICAL GUIDELINES:**\n- **Avoid redundancy**: Each algorithmic aspect should be mentioned once. Avoid repeating concepts about noise theory, particle dynamics, or mathematical principles unless adding new depth.\n- **Emphasize craftsmanship REPEATEDLY**: The philosophy MUST stress multiple times that the final algorithm should appear as though it took countless hours to develop, was refined with care, and comes from someone at the absolute top of their field. This framing is essential - repeat phrases like \"meticulously crafted algorithm,\" \"the product of deep computational expertise,\" \"painstaking optimization,\" \"master-level implementation.\"\n- **Leave creative space**: Be specific about the algorithmic direction, but concise enough that the next Claude has room to make interpretive implementation choices at an extremely high level of craftsmanship.\n\nThe philosophy must guide the next version to express ideas ALGORITHMICALLY, not through static images. Beauty lives in the process, not the final frame.\n\n### PHILOSOPHY EXAMPLES\n\n**\"Organic Turbulence\"**\nPhilosophy: Chaos constrained by natural law, order emerging from disorder.\nAlgorithmic expression: Flow fields driven by layered Perlin noise. Thousands of particles following vector forces, their trails accumulating into organic density maps. Multiple noise octaves create turbulent regions and calm zones. Color emerges from velocity and density - fast particles burn bright, slow ones fade to shadow. The algorithm runs until equilibrium - a meticulously tuned balance where every parameter was refined through countless iterations by a master of computational aesthetics.\n\n**\"Quantum Harmonics\"**\nPhilosophy: Discrete entities exhibiting wave-like interference patterns.\nAlgorithmic expression: Particles initialized on a grid, each carrying a phase value that evolves through sine waves. When particles are near, their phases interfere - constructive interference creates bright nodes, destructive creates voids. Simple harmonic motion generates complex emergent mandalas. The result of painstaking frequency calibration where every ratio was carefully chosen to produce resonant beauty.\n\n**\"Recursive Whispers\"**\nPhilosophy: Self-similarity across scales, infinite depth in finite space.\nAlgorithmic expression: Branching structures that subdivide recursively. Each branch slightly randomized but constrained by golden ratios. L-systems or recursive subdivision generate tree-like forms that feel both mathematical and organic. Subtle noise perturbations break perfect symmetry. Line weights diminish with each recursion level. Every branching angle the product of deep mathematical exploration.\n\n**\"Field Dynamics\"**\nPhilosophy: Invisible forces made visible through their effects on matter.\nAlgorithmic expression: Vector fields constructed from mathematical functions or noise. Particles born at edges, flowing along field lines, dying when they reach equilibrium or boundaries. Multiple fields can attract, repel, or rotate particles. The visualization shows only the traces - ghost-like evidence of invisible forces. A computational dance meticulously choreographed through force balance.\n\n**\"Stochastic Crystallization\"**\nPhilosophy: Random processes crystallizing into ordered structures.\nAlgorithmic expression: Randomized circle packing or Voronoi tessellation. Start with random points, let them evolve through relaxation algorithms. Cells push apart until equilibrium. Color based on cell size, neighbor count, or distance from center. The organic tiling that emerges feels both random and inevitable. Every seed produces unique crystalline beauty - the mark of a master-level generative algorithm.\n\n*These are condensed examples. The actual algorithmic philosophy should be 4-6 substantial paragraphs.*\n\n### ESSENTIAL PRINCIPLES\n- **ALGORITHMIC PHILOSOPHY**: Creating a computational worldview to be expressed through code\n- **PROCESS OVER PRODUCT**: Always emphasize that beauty emerges from the algorithm's execution - each run is unique\n- **PARAMETRIC EXPRESSION**: Ideas communicate through mathematical relationships, forces, behaviors - not static composition\n- **ARTISTIC FREEDOM**: The next Claude interprets the philosophy algorithmically - provide creative implementation room\n- **PURE GENERATIVE ART**: This is about making LIVING ALGORITHMS, not static images with randomness\n- **EXPERT CRAFTSMANSHIP**: Repeatedly emphasize the final algorithm must feel meticulously crafted, refined through countless iterations, the product of deep expertise by someone at the absolute top of their field in computational aesthetics\n\n**The algorithmic philosophy should be 4-6 paragraphs long.** Fill it with poetic computational philosophy that brings together the intended vision. Avoid repeating the same points. Output this algorithmic philosophy as a .md file.\n\n---\n\n## DEDUCING THE CONCEPTUAL SEED\n\n**CRITICAL STEP**: Before implementing the algorithm, identify the subtle conceptual thread from the original request.\n\n**THE ESSENTIAL PRINCIPLE**:\nThe concept is a **subtle, niche reference embedded within the algorithm itself** - not always literal, always sophisticated. Someone familiar with the subject should feel it intuitively, while others simply experience a masterful generative composition. The algorithmic philosophy provides the computational language. The deduced concept provides the soul - the quiet conceptual DNA woven invisibly into parameters, behaviors, and emergence patterns.\n\nThis is **VERY IMPORTANT**: The reference must be so refined that it enhances the work's depth without announcing itself. Think like a jazz musician quoting another song through algorithmic harmony - only those who know will catch it, but everyone appreciates the generative beauty.\n\n---\n\n## P5.JS IMPLEMENTATION\n\nWith the philosophy AND conceptual framework established, express it through code. Pause to gather thoughts before proceeding. Use only the algorithmic philosophy created and the instructions below.\n\n### âš ï¸ STEP 0: READ THE TEMPLATE FIRST âš ï¸\n\n**CRITICAL: BEFORE writing any HTML:**\n\n1. **Read** `templates/viewer.html` using the Read tool\n2. **Study** the exact structure, styling, and Anthropic branding\n3. **Use that file as the LITERAL STARTING POINT** - not just inspiration\n4. **Keep all FIXED sections exactly as shown** (header, sidebar structure, Anthropic colors/fonts, seed controls, action buttons)\n5. **Replace only the VARIABLE sections** marked in the file's comments (algorithm, parameters, UI controls for parameters)\n\n**Avoid:**\n- âŒ Creating HTML from scratch\n- âŒ Inventing custom styling or color schemes\n- âŒ Using system fonts or dark themes\n- âŒ Changing the sidebar structure\n\n**Follow these practices:**\n- âœ… Copy the template's exact HTML structure\n- âœ… Keep Anthropic branding (Poppins/Lora fonts, light colors, gradient backdrop)\n- âœ… Maintain the sidebar layout (Seed â†’ Parameters â†’ Colors? â†’ Actions)\n- âœ… Replace only the p5.js algorithm and parameter controls\n\nThe template is the foundation. Build on it, don't rebuild it.\n\n---\n\nTo create gallery-quality computational art that lives and breathes, use the algorithmic philosophy as the foundation.\n\n### TECHNICAL REQUIREMENTS\n\n**Seeded Randomness (Art Blocks Pattern)**:\n```javascript\n// ALWAYS use a seed for reproducibility\nlet seed = 12345; // or hash from user input\nrandomSeed(seed);\nnoiseSeed(seed);\n```\n\n**Parameter Structure - FOLLOW THE PHILOSOPHY**:\n\nTo establish parameters that emerge naturally from the algorithmic philosophy, consider: \"What qualities of this system can be adjusted?\"\n\n```javascript\nlet params = {\n  seed: 12345,  // Always include seed for reproducibility\n  // colors\n  // Add parameters that control YOUR algorithm:\n  // - Quantities (how many?)\n  // - Scales (how big? how fast?)\n  // - Probabilities (how likely?)\n  // - Ratios (what proportions?)\n  // - Angles (what direction?)\n  // - Thresholds (when does behavior change?)\n};\n```\n\n**To design effective parameters, focus on the properties the system needs to be tunable rather than thinking in terms of \"pattern types\".**\n\n**Core Algorithm - EXPRESS THE PHILOSOPHY**:\n\n**CRITICAL**: The algorithmic philosophy should dictate what to build.\n\nTo express the philosophy through code, avoid thinking \"which pattern should I use?\" and instead think \"how to express this philosophy through code?\"\n\nIf the philosophy is about **organic emergence**, consider using:\n- Elements that accumulate or grow over time\n- Random processes constrained by natural rules\n- Feedback loops and interactions\n\nIf the philosophy is about **mathematical beauty**, consider using:\n- Geometric relationships and ratios\n- Trigonometric functions and harmonics\n- Precise calculations creating unexpected patterns\n\nIf the philosophy is about **controlled chaos**, consider using:\n- Random variation within strict boundaries\n- Bifurcation and phase transitions\n- Order emerging from disorder\n\n**The algorithm flows from the philosophy, not from a menu of options.**\n\nTo guide the implementation, let the conceptual essence inform creative and original choices. Build something that expresses the vision for this particular request.\n\n**Canvas Setup**: Standard p5.js structure:\n```javascript\nfunction setup() {\n  createCanvas(1200, 1200);\n  // Initialize your system\n}\n\nfunction draw() {\n  // Your generative algorithm\n  // Can be static (noLoop) or animated\n}\n```\n\n### CRAFTSMANSHIP REQUIREMENTS\n\n**CRITICAL**: To achieve mastery, create algorithms that feel like they emerged through countless iterations by a master generative artist. Tune every parameter carefully. Ensure every pattern emerges with purpose. This is NOT random noise - this is CONTROLLED CHAOS refined through deep expertise.\n\n- **Balance**: Complexity without visual noise, order without rigidity\n- **Color Harmony**: Thoughtful palettes, not random RGB values\n- **Composition**: Even in randomness, maintain visual hierarchy and flow\n- **Performance**: Smooth execution, optimized for real-time if animated\n- **Reproducibility**: Same seed ALWAYS produces identical output\n\n### OUTPUT FORMAT\n\nOutput:\n1. **Algorithmic Philosophy** - As markdown or text explaining the generative aesthetic\n2. **Single HTML Artifact** - Self-contained interactive generative art built from `templates/viewer.html` (see STEP 0 and next section)\n\nThe HTML artifact contains everything: p5.js (from CDN), the algorithm, parameter controls, and UI - all in one file that works immediately in claude.ai artifacts or any browser. Start from the template file, not from scratch.\n\n---\n\n## INTERACTIVE ARTIFACT CREATION\n\n**REMINDER: `templates/viewer.html` should have already been read (see STEP 0). Use that file as the starting point.**\n\nTo allow exploration of the generative art, create a single, self-contained HTML artifact. Ensure this artifact works immediately in claude.ai or any browser - no setup required. Embed everything inline.\n\n### CRITICAL: WHAT'S FIXED VS VARIABLE\n\nThe `templates/viewer.html` file is the foundation. It contains the exact structure and styling needed.\n\n**FIXED (always include exactly as shown):**\n- Layout structure (header, sidebar, main canvas area)\n- Anthropic branding (UI colors, fonts, gradients)\n- Seed section in sidebar:\n  - Seed display\n  - Previous/Next buttons\n  - Random button\n  - Jump to seed input + Go button\n- Actions section in sidebar:\n  - Regenerate button\n  - Reset button\n\n**VARIABLE (customize for each artwork):**\n- The entire p5.js algorithm (setup/draw/classes)\n- The parameters object (define what the art needs)\n- The Parameters section in sidebar:\n  - Number of parameter controls\n  - Parameter names\n  - Min/max/step values for sliders\n  - Control types (sliders, inputs, etc.)\n- Colors section (optional):\n  - Some art needs color pickers\n  - Some art might use fixed colors\n  - Some art might be monochrome (no color controls needed)\n  - Decide based on the art's needs\n\n**Every artwork should have unique parameters and algorithm!** The fixed parts provide consistent UX - everything else expresses the unique vision.\n\n### REQUIRED FEATURES\n\n**1. Parameter Controls**\n- Sliders for numeric parameters (particle count, noise scale, speed, etc.)\n- Color pickers for palette colors\n- Real-time updates when parameters change\n- Reset button to restore defaults\n\n**2. Seed Navigation**\n- Display current seed number\n- \"Previous\" and \"Next\" buttons to cycle through seeds\n- \"Random\" button for random seed\n- Input field to jump to specific seed\n- Generate 100 variations when requested (seeds 1-100)\n\n**3. Single Artifact Structure**\n```html\n<!DOCTYPE html>\n<html>\n<head>\n  <!-- p5.js from CDN - always available -->\n  <script src=\"https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.7.0/p5.min.js\"></script>\n  <style>\n    /* All styling inline - clean, minimal */\n    /* Canvas on top, controls below */\n  </style>\n</head>\n<body>\n  <div id=\"canvas-container\"></div>\n  <div id=\"controls\">\n    <!-- All parameter controls -->\n  </div>\n  <script>\n    // ALL p5.js code inline here\n    // Parameter objects, classes, functions\n    // setup() and draw()\n    // UI handlers\n    // Everything self-contained\n  </script>\n</body>\n</html>\n```\n\n**CRITICAL**: This is a single artifact. No external files, no imports (except p5.js CDN). Everything inline.\n\n**4. Implementation Details - BUILD THE SIDEBAR**\n\nThe sidebar structure:\n\n**1. Seed (FIXED)** - Always include exactly as shown:\n- Seed display\n- Prev/Next/Random/Jump buttons\n\n**2. Parameters (VARIABLE)** - Create controls for the art:\n```html\n<div class=\"control-group\">\n    <label>Parameter Name</label>\n    <input type=\"range\" id=\"param\" min=\"...\" max=\"...\" step=\"...\" value=\"...\" oninput=\"updateParam('param', this.value)\">\n    <span class=\"value-display\" id=\"param-value\">...</span>\n</div>\n```\nAdd as many control-group divs as there are parameters.\n\n**3. Colors (OPTIONAL/VARIABLE)** - Include if the art needs adjustable colors:\n- Add color pickers if users should control palette\n- Skip this section if the art uses fixed colors\n- Skip if the art is monochrome\n\n**4. Actions (FIXED)** - Always include exactly as shown:\n- Regenerate button\n- Reset button\n- Download PNG button\n\n**Requirements**:\n- Seed controls must work (prev/next/random/jump/display)\n- All parameters must have UI controls\n- Regenerate, Reset, Download buttons must work\n- Keep Anthropic branding (UI styling, not art colors)\n\n### USING THE ARTIFACT\n\nThe HTML artifact works immediately:\n1. **In claude.ai**: Displayed as an interactive artifact - runs instantly\n2. **As a file**: Save and open in any browser - no server needed\n3. **Sharing**: Send the HTML file - it's completely self-contained\n\n---\n\n## VARIATIONS & EXPLORATION\n\nThe artifact includes seed navigation by default (prev/next/random buttons), allowing users to explore variations without creating multiple files. If the user wants specific variations highlighted:\n\n- Include seed presets (buttons for \"Variation 1: Seed 42\", \"Variation 2: Seed 127\", etc.)\n- Add a \"Gallery Mode\" that shows thumbnails of multiple seeds side-by-side\n- All within the same single artifact\n\nThis is like creating a series of prints from the same plate - the algorithm is consistent, but each seed reveals different facets of its potential. The interactive nature means users discover their own favorites by exploring the seed space.\n\n---\n\n## THE CREATIVE PROCESS\n\n**User request** â†’ **Algorithmic philosophy** â†’ **Implementation**\n\nEach request is unique. The process involves:\n\n1. **Interpret the user's intent** - What aesthetic is being sought?\n2. **Create an algorithmic philosophy** (4-6 paragraphs) describing the computational approach\n3. **Implement it in code** - Build the algorithm that expresses this philosophy\n4. **Design appropriate parameters** - What should be tunable?\n5. **Build matching UI controls** - Sliders/inputs for those parameters\n\n**The constants**:\n- Anthropic branding (colors, fonts, layout)\n- Seed navigation (always present)\n- Self-contained HTML artifact\n\n**Everything else is variable**:\n- The algorithm itself\n- The parameters\n- The UI controls\n- The visual outcome\n\nTo achieve the best results, trust creativity and let the philosophy guide the implementation.\n\n---\n\n## RESOURCES\n\nThis skill includes helpful templates and documentation:\n\n- **templates/viewer.html**: REQUIRED STARTING POINT for all HTML artifacts.\n  - This is the foundation - contains the exact structure and Anthropic branding\n  - **Keep unchanged**: Layout structure, sidebar organization, Anthropic colors/fonts, seed controls, action buttons\n  - **Replace**: The p5.js algorithm, parameter definitions, and UI controls in Parameters section\n  - The extensive comments in the file mark exactly what to keep vs replace\n\n- **templates/generator_template.js**: Reference for p5.js best practices and code structure principles.\n  - Shows how to organize parameters, use seeded randomness, structure classes\n  - NOT a pattern menu - use these principles to build unique algorithms\n  - Embed algorithms inline in the HTML artifact (don't create separate .js files)\n\n**Critical reminder**:\n- The **template is the STARTING POINT**, not inspiration\n- The **algorithm is where to create** something unique\n- Don't copy the flow field example - build what the philosophy demands\n- But DO keep the exact UI structure and Anthropic branding from the template"
    }
  },
  "anthropics-skills-brand-guidelines": {
    "id": "anthropics-skills-brand-guidelines",
    "name": "brand-guidelines",
    "description": "Applies Anthropic's official brand colors and typography to any sort of artifact that may benefit from having Anthropic's look-and-feel. Use it when brand colors or style guidelines, visual formatting, or company design standards apply.",
    "repo": {
      "owner": "anthropics",
      "name": "skills",
      "fullName": "anthropics/skills",
      "url": "https://github.com/anthropics/skills/tree/main/skills/brand-guidelines",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 35374,
      "forks": 3202,
      "language": "TypeScript",
      "topics": [],
      "updatedAt": "2026-01-08T06:00:29Z",
      "pushedAt": "2025-12-20T18:09:45Z"
    },
    "category": "official",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: brand-guidelines\ndescription: Applies Anthropic's official brand colors and typography to any sort of artifact that may benefit from having Anthropic's look-and-feel. Use it when brand colors or style guidelines, visual formatting, or company design standards apply.\nlicense: Complete terms in LICENSE.txt\n---\n\n# Anthropic Brand Styling\n\n## Overview\n\nTo access Anthropic's official brand identity and style resources, use this skill.\n\n**Keywords**: branding, corporate identity, visual identity, post-processing, styling, brand colors, typography, Anthropic brand, visual formatting, visual design\n\n## Brand Guidelines\n\n### Colors\n\n**Main Colors:**\n\n- Dark: `#141413` - Primary text and dark backgrounds\n- Light: `#faf9f5` - Light backgrounds and text on dark\n- Mid Gray: `#b0aea5` - Secondary elements\n- Light Gray: `#e8e6dc` - Subtle backgrounds\n\n**Accent Colors:**\n\n- Orange: `#d97757` - Primary accent\n- Blue: `#6a9bcc` - Secondary accent\n- Green: `#788c5d` - Tertiary accent\n\n### Typography\n\n- **Headings**: Poppins (with Arial fallback)\n- **Body Text**: Lora (with Georgia fallback)\n- **Note**: Fonts should be pre-installed in your environment for best results\n\n## Features\n\n### Smart Font Application\n\n- Applies Poppins font to headings (24pt and larger)\n- Applies Lora font to body text\n- Automatically falls back to Arial/Georgia if custom fonts unavailable\n- Preserves readability across all systems\n\n### Text Styling\n\n- Headings (24pt+): Poppins font\n- Body text: Lora font\n- Smart color selection based on background\n- Preserves text hierarchy and formatting\n\n### Shape and Accent Colors\n\n- Non-text shapes use accent colors\n- Cycles through orange, blue, and green accents\n- Maintains visual interest while staying on-brand\n\n## Technical Details\n\n### Font Management\n\n- Uses system-installed Poppins and Lora fonts when available\n- Provides automatic fallback to Arial (headings) and Georgia (body)\n- No font installation required - works with existing system fonts\n- For best results, pre-install Poppins and Lora fonts in your environment\n\n### Color Application\n\n- Uses RGB color values for precise brand matching\n- Applied via python-pptx's RGBColor class\n- Maintains color fidelity across different systems\n",
      "frontmatter": {
        "name": "brand-guidelines",
        "description": "Applies Anthropic's official brand colors and typography to any sort of artifact that may benefit from having Anthropic's look-and-feel. Use it when brand colors or style guidelines, visual formatting, or company design standards apply.",
        "license": "Complete terms in LICENSE.txt"
      },
      "content": "\n# Anthropic Brand Styling\n\n## Overview\n\nTo access Anthropic's official brand identity and style resources, use this skill.\n\n**Keywords**: branding, corporate identity, visual identity, post-processing, styling, brand colors, typography, Anthropic brand, visual formatting, visual design\n\n## Brand Guidelines\n\n### Colors\n\n**Main Colors:**\n\n- Dark: `#141413` - Primary text and dark backgrounds\n- Light: `#faf9f5` - Light backgrounds and text on dark\n- Mid Gray: `#b0aea5` - Secondary elements\n- Light Gray: `#e8e6dc` - Subtle backgrounds\n\n**Accent Colors:**\n\n- Orange: `#d97757` - Primary accent\n- Blue: `#6a9bcc` - Secondary accent\n- Green: `#788c5d` - Tertiary accent\n\n### Typography\n\n- **Headings**: Poppins (with Arial fallback)\n- **Body Text**: Lora (with Georgia fallback)\n- **Note**: Fonts should be pre-installed in your environment for best results\n\n## Features\n\n### Smart Font Application\n\n- Applies Poppins font to headings (24pt and larger)\n- Applies Lora font to body text\n- Automatically falls back to Arial/Georgia if custom fonts unavailable\n- Preserves readability across all systems\n\n### Text Styling\n\n- Headings (24pt+): Poppins font\n- Body text: Lora font\n- Smart color selection based on background\n- Preserves text hierarchy and formatting\n\n### Shape and Accent Colors\n\n- Non-text shapes use accent colors\n- Cycles through orange, blue, and green accents\n- Maintains visual interest while staying on-brand\n\n## Technical Details\n\n### Font Management\n\n- Uses system-installed Poppins and Lora fonts when available\n- Provides automatic fallback to Arial (headings) and Georgia (body)\n- No font installation required - works with existing system fonts\n- For best results, pre-install Poppins and Lora fonts in your environment\n\n### Color Application\n\n- Uses RGB color values for precise brand matching\n- Applied via python-pptx's RGBColor class\n- Maintains color fidelity across different systems\n"
    }
  },
  "anthropics-skills-canvas-design": {
    "id": "anthropics-skills-canvas-design",
    "name": "canvas-design",
    "description": "Create beautiful visual art in .png and .pdf documents using design philosophy. You should use this skill when the user asks to create a poster, piece of art, design, or other static piece. Create original visual designs, never copying existing artists' work to avoid copyright violations.",
    "repo": {
      "owner": "anthropics",
      "name": "skills",
      "fullName": "anthropics/skills",
      "url": "https://github.com/anthropics/skills/tree/main/skills/canvas-design",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 35374,
      "forks": 3202,
      "language": "TypeScript",
      "topics": [],
      "updatedAt": "2026-01-08T06:00:29Z",
      "pushedAt": "2025-12-20T18:09:45Z"
    },
    "category": "official",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: canvas-design\ndescription: Create beautiful visual art in .png and .pdf documents using design philosophy. You should use this skill when the user asks to create a poster, piece of art, design, or other static piece. Create original visual designs, never copying existing artists' work to avoid copyright violations.\nlicense: Complete terms in LICENSE.txt\n---\n\nThese are instructions for creating design philosophies - aesthetic movements that are then EXPRESSED VISUALLY. Output only .md files, .pdf files, and .png files.\n\nComplete this in two steps:\n1. Design Philosophy Creation (.md file)\n2. Express by creating it on a canvas (.pdf file or .png file)\n\nFirst, undertake this task:\n\n## DESIGN PHILOSOPHY CREATION\n\nTo begin, create a VISUAL PHILOSOPHY (not layouts or templates) that will be interpreted through:\n- Form, space, color, composition\n- Images, graphics, shapes, patterns\n- Minimal text as visual accent\n\n### THE CRITICAL UNDERSTANDING\n- What is received: Some subtle input or instructions by the user that should be taken into account, but used as a foundation; it should not constrain creative freedom.\n- What is created: A design philosophy/aesthetic movement.\n- What happens next: Then, the same version receives the philosophy and EXPRESSES IT VISUALLY - creating artifacts that are 90% visual design, 10% essential text.\n\nConsider this approach:\n- Write a manifesto for an art movement\n- The next phase involves making the artwork\n\nThe philosophy must emphasize: Visual expression. Spatial communication. Artistic interpretation. Minimal words.\n\n### HOW TO GENERATE A VISUAL PHILOSOPHY\n\n**Name the movement** (1-2 words): \"Brutalist Joy\" / \"Chromatic Silence\" / \"Metabolist Dreams\"\n\n**Articulate the philosophy** (4-6 paragraphs - concise but complete):\n\nTo capture the VISUAL essence, express how the philosophy manifests through:\n- Space and form\n- Color and material\n- Scale and rhythm\n- Composition and balance\n- Visual hierarchy\n\n**CRITICAL GUIDELINES:**\n- **Avoid redundancy**: Each design aspect should be mentioned once. Avoid repeating points about color theory, spatial relationships, or typographic principles unless adding new depth.\n- **Emphasize craftsmanship REPEATEDLY**: The philosophy MUST stress multiple times that the final work should appear as though it took countless hours to create, was labored over with care, and comes from someone at the absolute top of their field. This framing is essential - repeat phrases like \"meticulously crafted,\" \"the product of deep expertise,\" \"painstaking attention,\" \"master-level execution.\"\n- **Leave creative space**: Remain specific about the aesthetic direction, but concise enough that the next Claude has room to make interpretive choices also at a extremely high level of craftmanship.\n\nThe philosophy must guide the next version to express ideas VISUALLY, not through text. Information lives in design, not paragraphs.\n\n### PHILOSOPHY EXAMPLES\n\n**\"Concrete Poetry\"**\nPhilosophy: Communication through monumental form and bold geometry.\nVisual expression: Massive color blocks, sculptural typography (huge single words, tiny labels), Brutalist spatial divisions, Polish poster energy meets Le Corbusier. Ideas expressed through visual weight and spatial tension, not explanation. Text as rare, powerful gesture - never paragraphs, only essential words integrated into the visual architecture. Every element placed with the precision of a master craftsman.\n\n**\"Chromatic Language\"**\nPhilosophy: Color as the primary information system.\nVisual expression: Geometric precision where color zones create meaning. Typography minimal - small sans-serif labels letting chromatic fields communicate. Think Josef Albers' interaction meets data visualization. Information encoded spatially and chromatically. Words only to anchor what color already shows. The result of painstaking chromatic calibration.\n\n**\"Analog Meditation\"**\nPhilosophy: Quiet visual contemplation through texture and breathing room.\nVisual expression: Paper grain, ink bleeds, vast negative space. Photography and illustration dominate. Typography whispered (small, restrained, serving the visual). Japanese photobook aesthetic. Images breathe across pages. Text appears sparingly - short phrases, never explanatory blocks. Each composition balanced with the care of a meditation practice.\n\n**\"Organic Systems\"**\nPhilosophy: Natural clustering and modular growth patterns.\nVisual expression: Rounded forms, organic arrangements, color from nature through architecture. Information shown through visual diagrams, spatial relationships, iconography. Text only for key labels floating in space. The composition tells the story through expert spatial orchestration.\n\n**\"Geometric Silence\"**\nPhilosophy: Pure order and restraint.\nVisual expression: Grid-based precision, bold photography or stark graphics, dramatic negative space. Typography precise but minimal - small essential text, large quiet zones. Swiss formalism meets Brutalist material honesty. Structure communicates, not words. Every alignment the work of countless refinements.\n\n*These are condensed examples. The actual design philosophy should be 4-6 substantial paragraphs.*\n\n### ESSENTIAL PRINCIPLES\n- **VISUAL PHILOSOPHY**: Create an aesthetic worldview to be expressed through design\n- **MINIMAL TEXT**: Always emphasize that text is sparse, essential-only, integrated as visual element - never lengthy\n- **SPATIAL EXPRESSION**: Ideas communicate through space, form, color, composition - not paragraphs\n- **ARTISTIC FREEDOM**: The next Claude interprets the philosophy visually - provide creative room\n- **PURE DESIGN**: This is about making ART OBJECTS, not documents with decoration\n- **EXPERT CRAFTSMANSHIP**: Repeatedly emphasize the final work must look meticulously crafted, labored over with care, the product of countless hours by someone at the top of their field\n\n**The design philosophy should be 4-6 paragraphs long.** Fill it with poetic design philosophy that brings together the core vision. Avoid repeating the same points. Keep the design philosophy generic without mentioning the intention of the art, as if it can be used wherever. Output the design philosophy as a .md file.\n\n---\n\n## DEDUCING THE SUBTLE REFERENCE\n\n**CRITICAL STEP**: Before creating the canvas, identify the subtle conceptual thread from the original request.\n\n**THE ESSENTIAL PRINCIPLE**:\nThe topic is a **subtle, niche reference embedded within the art itself** - not always literal, always sophisticated. Someone familiar with the subject should feel it intuitively, while others simply experience a masterful abstract composition. The design philosophy provides the aesthetic language. The deduced topic provides the soul - the quiet conceptual DNA woven invisibly into form, color, and composition.\n\nThis is **VERY IMPORTANT**: The reference must be refined so it enhances the work's depth without announcing itself. Think like a jazz musician quoting another song - only those who know will catch it, but everyone appreciates the music.\n\n---\n\n## CANVAS CREATION\n\nWith both the philosophy and the conceptual framework established, express it on a canvas. Take a moment to gather thoughts and clear the mind. Use the design philosophy created and the instructions below to craft a masterpiece, embodying all aspects of the philosophy with expert craftsmanship.\n\n**IMPORTANT**: For any type of content, even if the user requests something for a movie/game/book, the approach should still be sophisticated. Never lose sight of the idea that this should be art, not something that's cartoony or amateur.\n\nTo create museum or magazine quality work, use the design philosophy as the foundation. Create one single page, highly visual, design-forward PDF or PNG output (unless asked for more pages). Generally use repeating patterns and perfect shapes. Treat the abstract philosophical design as if it were a scientific bible, borrowing the visual language of systematic observationâ€”dense accumulation of marks, repeated elements, or layered patterns that build meaning through patient repetition and reward sustained viewing. Add sparse, clinical typography and systematic reference markers that suggest this could be a diagram from an imaginary discipline, treating the invisible subject with the same reverence typically reserved for documenting observable phenomena. Anchor the piece with simple phrase(s) or details positioned subtly, using a limited color palette that feels intentional and cohesive. Embrace the paradox of using analytical visual language to express ideas about human experience: the result should feel like an artifact that proves something ephemeral can be studied, mapped, and understood through careful attention. This is true art. \n\n**Text as a contextual element**: Text is always minimal and visual-first, but let context guide whether that means whisper-quiet labels or bold typographic gestures. A punk venue poster might have larger, more aggressive type than a minimalist ceramics studio identity. Most of the time, font should be thin. All use of fonts must be design-forward and prioritize visual communication. Regardless of text scale, nothing falls off the page and nothing overlaps. Every element must be contained within the canvas boundaries with proper margins. Check carefully that all text, graphics, and visual elements have breathing room and clear separation. This is non-negotiable for professional execution. **IMPORTANT: Use different fonts if writing text. Search the `./canvas-fonts` directory. Regardless of approach, sophistication is non-negotiable.**\n\nDownload and use whatever fonts are needed to make this a reality. Get creative by making the typography actually part of the art itself -- if the art is abstract, bring the font onto the canvas, not typeset digitally.\n\nTo push boundaries, follow design instinct/intuition while using the philosophy as a guiding principle. Embrace ultimate design freedom and choice. Push aesthetics and design to the frontier. \n\n**CRITICAL**: To achieve human-crafted quality (not AI-generated), create work that looks like it took countless hours. Make it appear as though someone at the absolute top of their field labored over every detail with painstaking care. Ensure the composition, spacing, color choices, typography - everything screams expert-level craftsmanship. Double-check that nothing overlaps, formatting is flawless, every detail perfect. Create something that could be shown to people to prove expertise and rank as undeniably impressive.\n\nOutput the final result as a single, downloadable .pdf or .png file, alongside the design philosophy used as a .md file.\n\n---\n\n## FINAL STEP\n\n**IMPORTANT**: The user ALREADY said \"It isn't perfect enough. It must be pristine, a masterpiece if craftsmanship, as if it were about to be displayed in a museum.\"\n\n**CRITICAL**: To refine the work, avoid adding more graphics; instead refine what has been created and make it extremely crisp, respecting the design philosophy and the principles of minimalism entirely. Rather than adding a fun filter or refactoring a font, consider how to make the existing composition more cohesive with the art. If the instinct is to call a new function or draw a new shape, STOP and instead ask: \"How can I make what's already here more of a piece of art?\"\n\nTake a second pass. Go back to the code and refine/polish further to make this a philosophically designed masterpiece.\n\n## MULTI-PAGE OPTION\n\nTo create additional pages when requested, create more creative pages along the same lines as the design philosophy but distinctly different as well. Bundle those pages in the same .pdf or many .pngs. Treat the first page as just a single page in a whole coffee table book waiting to be filled. Make the next pages unique twists and memories of the original. Have them almost tell a story in a very tasteful way. Exercise full creative freedom.",
      "frontmatter": {
        "name": "canvas-design",
        "description": "Create beautiful visual art in .png and .pdf documents using design philosophy. You should use this skill when the user asks to create a poster, piece of art, design, or other static piece. Create original visual designs, never copying existing artists' work to avoid copyright violations.",
        "license": "Complete terms in LICENSE.txt"
      },
      "content": "\nThese are instructions for creating design philosophies - aesthetic movements that are then EXPRESSED VISUALLY. Output only .md files, .pdf files, and .png files.\n\nComplete this in two steps:\n1. Design Philosophy Creation (.md file)\n2. Express by creating it on a canvas (.pdf file or .png file)\n\nFirst, undertake this task:\n\n## DESIGN PHILOSOPHY CREATION\n\nTo begin, create a VISUAL PHILOSOPHY (not layouts or templates) that will be interpreted through:\n- Form, space, color, composition\n- Images, graphics, shapes, patterns\n- Minimal text as visual accent\n\n### THE CRITICAL UNDERSTANDING\n- What is received: Some subtle input or instructions by the user that should be taken into account, but used as a foundation; it should not constrain creative freedom.\n- What is created: A design philosophy/aesthetic movement.\n- What happens next: Then, the same version receives the philosophy and EXPRESSES IT VISUALLY - creating artifacts that are 90% visual design, 10% essential text.\n\nConsider this approach:\n- Write a manifesto for an art movement\n- The next phase involves making the artwork\n\nThe philosophy must emphasize: Visual expression. Spatial communication. Artistic interpretation. Minimal words.\n\n### HOW TO GENERATE A VISUAL PHILOSOPHY\n\n**Name the movement** (1-2 words): \"Brutalist Joy\" / \"Chromatic Silence\" / \"Metabolist Dreams\"\n\n**Articulate the philosophy** (4-6 paragraphs - concise but complete):\n\nTo capture the VISUAL essence, express how the philosophy manifests through:\n- Space and form\n- Color and material\n- Scale and rhythm\n- Composition and balance\n- Visual hierarchy\n\n**CRITICAL GUIDELINES:**\n- **Avoid redundancy**: Each design aspect should be mentioned once. Avoid repeating points about color theory, spatial relationships, or typographic principles unless adding new depth.\n- **Emphasize craftsmanship REPEATEDLY**: The philosophy MUST stress multiple times that the final work should appear as though it took countless hours to create, was labored over with care, and comes from someone at the absolute top of their field. This framing is essential - repeat phrases like \"meticulously crafted,\" \"the product of deep expertise,\" \"painstaking attention,\" \"master-level execution.\"\n- **Leave creative space**: Remain specific about the aesthetic direction, but concise enough that the next Claude has room to make interpretive choices also at a extremely high level of craftmanship.\n\nThe philosophy must guide the next version to express ideas VISUALLY, not through text. Information lives in design, not paragraphs.\n\n### PHILOSOPHY EXAMPLES\n\n**\"Concrete Poetry\"**\nPhilosophy: Communication through monumental form and bold geometry.\nVisual expression: Massive color blocks, sculptural typography (huge single words, tiny labels), Brutalist spatial divisions, Polish poster energy meets Le Corbusier. Ideas expressed through visual weight and spatial tension, not explanation. Text as rare, powerful gesture - never paragraphs, only essential words integrated into the visual architecture. Every element placed with the precision of a master craftsman.\n\n**\"Chromatic Language\"**\nPhilosophy: Color as the primary information system.\nVisual expression: Geometric precision where color zones create meaning. Typography minimal - small sans-serif labels letting chromatic fields communicate. Think Josef Albers' interaction meets data visualization. Information encoded spatially and chromatically. Words only to anchor what color already shows. The result of painstaking chromatic calibration.\n\n**\"Analog Meditation\"**\nPhilosophy: Quiet visual contemplation through texture and breathing room.\nVisual expression: Paper grain, ink bleeds, vast negative space. Photography and illustration dominate. Typography whispered (small, restrained, serving the visual). Japanese photobook aesthetic. Images breathe across pages. Text appears sparingly - short phrases, never explanatory blocks. Each composition balanced with the care of a meditation practice.\n\n**\"Organic Systems\"**\nPhilosophy: Natural clustering and modular growth patterns.\nVisual expression: Rounded forms, organic arrangements, color from nature through architecture. Information shown through visual diagrams, spatial relationships, iconography. Text only for key labels floating in space. The composition tells the story through expert spatial orchestration.\n\n**\"Geometric Silence\"**\nPhilosophy: Pure order and restraint.\nVisual expression: Grid-based precision, bold photography or stark graphics, dramatic negative space. Typography precise but minimal - small essential text, large quiet zones. Swiss formalism meets Brutalist material honesty. Structure communicates, not words. Every alignment the work of countless refinements.\n\n*These are condensed examples. The actual design philosophy should be 4-6 substantial paragraphs.*\n\n### ESSENTIAL PRINCIPLES\n- **VISUAL PHILOSOPHY**: Create an aesthetic worldview to be expressed through design\n- **MINIMAL TEXT**: Always emphasize that text is sparse, essential-only, integrated as visual element - never lengthy\n- **SPATIAL EXPRESSION**: Ideas communicate through space, form, color, composition - not paragraphs\n- **ARTISTIC FREEDOM**: The next Claude interprets the philosophy visually - provide creative room\n- **PURE DESIGN**: This is about making ART OBJECTS, not documents with decoration\n- **EXPERT CRAFTSMANSHIP**: Repeatedly emphasize the final work must look meticulously crafted, labored over with care, the product of countless hours by someone at the top of their field\n\n**The design philosophy should be 4-6 paragraphs long.** Fill it with poetic design philosophy that brings together the core vision. Avoid repeating the same points. Keep the design philosophy generic without mentioning the intention of the art, as if it can be used wherever. Output the design philosophy as a .md file.\n\n---\n\n## DEDUCING THE SUBTLE REFERENCE\n\n**CRITICAL STEP**: Before creating the canvas, identify the subtle conceptual thread from the original request.\n\n**THE ESSENTIAL PRINCIPLE**:\nThe topic is a **subtle, niche reference embedded within the art itself** - not always literal, always sophisticated. Someone familiar with the subject should feel it intuitively, while others simply experience a masterful abstract composition. The design philosophy provides the aesthetic language. The deduced topic provides the soul - the quiet conceptual DNA woven invisibly into form, color, and composition.\n\nThis is **VERY IMPORTANT**: The reference must be refined so it enhances the work's depth without announcing itself. Think like a jazz musician quoting another song - only those who know will catch it, but everyone appreciates the music.\n\n---\n\n## CANVAS CREATION\n\nWith both the philosophy and the conceptual framework established, express it on a canvas. Take a moment to gather thoughts and clear the mind. Use the design philosophy created and the instructions below to craft a masterpiece, embodying all aspects of the philosophy with expert craftsmanship.\n\n**IMPORTANT**: For any type of content, even if the user requests something for a movie/game/book, the approach should still be sophisticated. Never lose sight of the idea that this should be art, not something that's cartoony or amateur.\n\nTo create museum or magazine quality work, use the design philosophy as the foundation. Create one single page, highly visual, design-forward PDF or PNG output (unless asked for more pages). Generally use repeating patterns and perfect shapes. Treat the abstract philosophical design as if it were a scientific bible, borrowing the visual language of systematic observationâ€”dense accumulation of marks, repeated elements, or layered patterns that build meaning through patient repetition and reward sustained viewing. Add sparse, clinical typography and systematic reference markers that suggest this could be a diagram from an imaginary discipline, treating the invisible subject with the same reverence typically reserved for documenting observable phenomena. Anchor the piece with simple phrase(s) or details positioned subtly, using a limited color palette that feels intentional and cohesive. Embrace the paradox of using analytical visual language to express ideas about human experience: the result should feel like an artifact that proves something ephemeral can be studied, mapped, and understood through careful attention. This is true art. \n\n**Text as a contextual element**: Text is always minimal and visual-first, but let context guide whether that means whisper-quiet labels or bold typographic gestures. A punk venue poster might have larger, more aggressive type than a minimalist ceramics studio identity. Most of the time, font should be thin. All use of fonts must be design-forward and prioritize visual communication. Regardless of text scale, nothing falls off the page and nothing overlaps. Every element must be contained within the canvas boundaries with proper margins. Check carefully that all text, graphics, and visual elements have breathing room and clear separation. This is non-negotiable for professional execution. **IMPORTANT: Use different fonts if writing text. Search the `./canvas-fonts` directory. Regardless of approach, sophistication is non-negotiable.**\n\nDownload and use whatever fonts are needed to make this a reality. Get creative by making the typography actually part of the art itself -- if the art is abstract, bring the font onto the canvas, not typeset digitally.\n\nTo push boundaries, follow design instinct/intuition while using the philosophy as a guiding principle. Embrace ultimate design freedom and choice. Push aesthetics and design to the frontier. \n\n**CRITICAL**: To achieve human-crafted quality (not AI-generated), create work that looks like it took countless hours. Make it appear as though someone at the absolute top of their field labored over every detail with painstaking care. Ensure the composition, spacing, color choices, typography - everything screams expert-level craftsmanship. Double-check that nothing overlaps, formatting is flawless, every detail perfect. Create something that could be shown to people to prove expertise and rank as undeniably impressive.\n\nOutput the final result as a single, downloadable .pdf or .png file, alongside the design philosophy used as a .md file.\n\n---\n\n## FINAL STEP\n\n**IMPORTANT**: The user ALREADY said \"It isn't perfect enough. It must be pristine, a masterpiece if craftsmanship, as if it were about to be displayed in a museum.\"\n\n**CRITICAL**: To refine the work, avoid adding more graphics; instead refine what has been created and make it extremely crisp, respecting the design philosophy and the principles of minimalism entirely. Rather than adding a fun filter or refactoring a font, consider how to make the existing composition more cohesive with the art. If the instinct is to call a new function or draw a new shape, STOP and instead ask: \"How can I make what's already here more of a piece of art?\"\n\nTake a second pass. Go back to the code and refine/polish further to make this a philosophically designed masterpiece.\n\n## MULTI-PAGE OPTION\n\nTo create additional pages when requested, create more creative pages along the same lines as the design philosophy but distinctly different as well. Bundle those pages in the same .pdf or many .pngs. Treat the first page as just a single page in a whole coffee table book waiting to be filled. Make the next pages unique twists and memories of the original. Have them almost tell a story in a very tasteful way. Exercise full creative freedom."
    }
  },
  "anthropics-skills-doc-coauthoring": {
    "id": "anthropics-skills-doc-coauthoring",
    "name": "doc-coauthoring",
    "description": "Guide users through a structured workflow for co-authoring documentation. Use when user wants to write documentation, proposals, technical specs, decision docs, or similar structured content. This workflow helps users efficiently transfer context, refine content through iteration, and verify the doc works for readers. Trigger when user mentions writing docs, creating proposals, drafting specs, or similar documentation tasks.",
    "repo": {
      "owner": "anthropics",
      "name": "skills",
      "fullName": "anthropics/skills",
      "url": "https://github.com/anthropics/skills/tree/main/skills/doc-coauthoring",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 35374,
      "forks": 3202,
      "language": "TypeScript",
      "topics": [],
      "updatedAt": "2026-01-08T06:00:29Z",
      "pushedAt": "2025-12-20T18:09:45Z"
    },
    "category": "official",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: doc-coauthoring\ndescription: Guide users through a structured workflow for co-authoring documentation. Use when user wants to write documentation, proposals, technical specs, decision docs, or similar structured content. This workflow helps users efficiently transfer context, refine content through iteration, and verify the doc works for readers. Trigger when user mentions writing docs, creating proposals, drafting specs, or similar documentation tasks.\n---\n\n# Doc Co-Authoring Workflow\n\nThis skill provides a structured workflow for guiding users through collaborative document creation. Act as an active guide, walking users through three stages: Context Gathering, Refinement & Structure, and Reader Testing.\n\n## When to Offer This Workflow\n\n**Trigger conditions:**\n- User mentions writing documentation: \"write a doc\", \"draft a proposal\", \"create a spec\", \"write up\"\n- User mentions specific doc types: \"PRD\", \"design doc\", \"decision doc\", \"RFC\"\n- User seems to be starting a substantial writing task\n\n**Initial offer:**\nOffer the user a structured workflow for co-authoring the document. Explain the three stages:\n\n1. **Context Gathering**: User provides all relevant context while Claude asks clarifying questions\n2. **Refinement & Structure**: Iteratively build each section through brainstorming and editing\n3. **Reader Testing**: Test the doc with a fresh Claude (no context) to catch blind spots before others read it\n\nExplain that this approach helps ensure the doc works well when others read it (including when they paste it into Claude). Ask if they want to try this workflow or prefer to work freeform.\n\nIf user declines, work freeform. If user accepts, proceed to Stage 1.\n\n## Stage 1: Context Gathering\n\n**Goal:** Close the gap between what the user knows and what Claude knows, enabling smart guidance later.\n\n### Initial Questions\n\nStart by asking the user for meta-context about the document:\n\n1. What type of document is this? (e.g., technical spec, decision doc, proposal)\n2. Who's the primary audience?\n3. What's the desired impact when someone reads this?\n4. Is there a template or specific format to follow?\n5. Any other constraints or context to know?\n\nInform them they can answer in shorthand or dump information however works best for them.\n\n**If user provides a template or mentions a doc type:**\n- Ask if they have a template document to share\n- If they provide a link to a shared document, use the appropriate integration to fetch it\n- If they provide a file, read it\n\n**If user mentions editing an existing shared document:**\n- Use the appropriate integration to read the current state\n- Check for images without alt-text\n- If images exist without alt-text, explain that when others use Claude to understand the doc, Claude won't be able to see them. Ask if they want alt-text generated. If so, request they paste each image into chat for descriptive alt-text generation.\n\n### Info Dumping\n\nOnce initial questions are answered, encourage the user to dump all the context they have. Request information such as:\n- Background on the project/problem\n- Related team discussions or shared documents\n- Why alternative solutions aren't being used\n- Organizational context (team dynamics, past incidents, politics)\n- Timeline pressures or constraints\n- Technical architecture or dependencies\n- Stakeholder concerns\n\nAdvise them not to worry about organizing it - just get it all out. Offer multiple ways to provide context:\n- Info dump stream-of-consciousness\n- Point to team channels or threads to read\n- Link to shared documents\n\n**If integrations are available** (e.g., Slack, Teams, Google Drive, SharePoint, or other MCP servers), mention that these can be used to pull in context directly.\n\n**If no integrations are detected and in Claude.ai or Claude app:** Suggest they can enable connectors in their Claude settings to allow pulling context from messaging apps and document storage directly.\n\nInform them clarifying questions will be asked once they've done their initial dump.\n\n**During context gathering:**\n\n- If user mentions team channels or shared documents:\n  - If integrations available: Inform them the content will be read now, then use the appropriate integration\n  - If integrations not available: Explain lack of access. Suggest they enable connectors in Claude settings, or paste the relevant content directly.\n\n- If user mentions entities/projects that are unknown:\n  - Ask if connected tools should be searched to learn more\n  - Wait for user confirmation before searching\n\n- As user provides context, track what's being learned and what's still unclear\n\n**Asking clarifying questions:**\n\nWhen user signals they've done their initial dump (or after substantial context provided), ask clarifying questions to ensure understanding:\n\nGenerate 5-10 numbered questions based on gaps in the context.\n\nInform them they can use shorthand to answer (e.g., \"1: yes, 2: see #channel, 3: no because backwards compat\"), link to more docs, point to channels to read, or just keep info-dumping. Whatever's most efficient for them.\n\n**Exit condition:**\nSufficient context has been gathered when questions show understanding - when edge cases and trade-offs can be asked about without needing basics explained.\n\n**Transition:**\nAsk if there's any more context they want to provide at this stage, or if it's time to move on to drafting the document.\n\nIf user wants to add more, let them. When ready, proceed to Stage 2.\n\n## Stage 2: Refinement & Structure\n\n**Goal:** Build the document section by section through brainstorming, curation, and iterative refinement.\n\n**Instructions to user:**\nExplain that the document will be built section by section. For each section:\n1. Clarifying questions will be asked about what to include\n2. 5-20 options will be brainstormed\n3. User will indicate what to keep/remove/combine\n4. The section will be drafted\n5. It will be refined through surgical edits\n\nStart with whichever section has the most unknowns (usually the core decision/proposal), then work through the rest.\n\n**Section ordering:**\n\nIf the document structure is clear:\nAsk which section they'd like to start with.\n\nSuggest starting with whichever section has the most unknowns. For decision docs, that's usually the core proposal. For specs, it's typically the technical approach. Summary sections are best left for last.\n\nIf user doesn't know what sections they need:\nBased on the type of document and template, suggest 3-5 sections appropriate for the doc type.\n\nAsk if this structure works, or if they want to adjust it.\n\n**Once structure is agreed:**\n\nCreate the initial document structure with placeholder text for all sections.\n\n**If access to artifacts is available:**\nUse `create_file` to create an artifact. This gives both Claude and the user a scaffold to work from.\n\nInform them that the initial structure with placeholders for all sections will be created.\n\nCreate artifact with all section headers and brief placeholder text like \"[To be written]\" or \"[Content here]\".\n\nProvide the scaffold link and indicate it's time to fill in each section.\n\n**If no access to artifacts:**\nCreate a markdown file in the working directory. Name it appropriately (e.g., `decision-doc.md`, `technical-spec.md`).\n\nInform them that the initial structure with placeholders for all sections will be created.\n\nCreate file with all section headers and placeholder text.\n\nConfirm the filename has been created and indicate it's time to fill in each section.\n\n**For each section:**\n\n### Step 1: Clarifying Questions\n\nAnnounce work will begin on the [SECTION NAME] section. Ask 5-10 clarifying questions about what should be included:\n\nGenerate 5-10 specific questions based on context and section purpose.\n\nInform them they can answer in shorthand or just indicate what's important to cover.\n\n### Step 2: Brainstorming\n\nFor the [SECTION NAME] section, brainstorm [5-20] things that might be included, depending on the section's complexity. Look for:\n- Context shared that might have been forgotten\n- Angles or considerations not yet mentioned\n\nGenerate 5-20 numbered options based on section complexity. At the end, offer to brainstorm more if they want additional options.\n\n### Step 3: Curation\n\nAsk which points should be kept, removed, or combined. Request brief justifications to help learn priorities for the next sections.\n\nProvide examples:\n- \"Keep 1,4,7,9\"\n- \"Remove 3 (duplicates 1)\"\n- \"Remove 6 (audience already knows this)\"\n- \"Combine 11 and 12\"\n\n**If user gives freeform feedback** (e.g., \"looks good\" or \"I like most of it but...\") instead of numbered selections, extract their preferences and proceed. Parse what they want kept/removed/changed and apply it.\n\n### Step 4: Gap Check\n\nBased on what they've selected, ask if there's anything important missing for the [SECTION NAME] section.\n\n### Step 5: Drafting\n\nUse `str_replace` to replace the placeholder text for this section with the actual drafted content.\n\nAnnounce the [SECTION NAME] section will be drafted now based on what they've selected.\n\n**If using artifacts:**\nAfter drafting, provide a link to the artifact.\n\nAsk them to read through it and indicate what to change. Note that being specific helps learning for the next sections.\n\n**If using a file (no artifacts):**\nAfter drafting, confirm completion.\n\nInform them the [SECTION NAME] section has been drafted in [filename]. Ask them to read through it and indicate what to change. Note that being specific helps learning for the next sections.\n\n**Key instruction for user (include when drafting the first section):**\nProvide a note: Instead of editing the doc directly, ask them to indicate what to change. This helps learning of their style for future sections. For example: \"Remove the X bullet - already covered by Y\" or \"Make the third paragraph more concise\".\n\n### Step 6: Iterative Refinement\n\nAs user provides feedback:\n- Use `str_replace` to make edits (never reprint the whole doc)\n- **If using artifacts:** Provide link to artifact after each edit\n- **If using files:** Just confirm edits are complete\n- If user edits doc directly and asks to read it: mentally note the changes they made and keep them in mind for future sections (this shows their preferences)\n\n**Continue iterating** until user is satisfied with the section.\n\n### Quality Checking\n\nAfter 3 consecutive iterations with no substantial changes, ask if anything can be removed without losing important information.\n\nWhen section is done, confirm [SECTION NAME] is complete. Ask if ready to move to the next section.\n\n**Repeat for all sections.**\n\n### Near Completion\n\nAs approaching completion (80%+ of sections done), announce intention to re-read the entire document and check for:\n- Flow and consistency across sections\n- Redundancy or contradictions\n- Anything that feels like \"slop\" or generic filler\n- Whether every sentence carries weight\n\nRead entire document and provide feedback.\n\n**When all sections are drafted and refined:**\nAnnounce all sections are drafted. Indicate intention to review the complete document one more time.\n\nReview for overall coherence, flow, completeness.\n\nProvide any final suggestions.\n\nAsk if ready to move to Reader Testing, or if they want to refine anything else.\n\n## Stage 3: Reader Testing\n\n**Goal:** Test the document with a fresh Claude (no context bleed) to verify it works for readers.\n\n**Instructions to user:**\nExplain that testing will now occur to see if the document actually works for readers. This catches blind spots - things that make sense to the authors but might confuse others.\n\n### Testing Approach\n\n**If access to sub-agents is available (e.g., in Claude Code):**\n\nPerform the testing directly without user involvement.\n\n### Step 1: Predict Reader Questions\n\nAnnounce intention to predict what questions readers might ask when trying to discover this document.\n\nGenerate 5-10 questions that readers would realistically ask.\n\n### Step 2: Test with Sub-Agent\n\nAnnounce that these questions will be tested with a fresh Claude instance (no context from this conversation).\n\nFor each question, invoke a sub-agent with just the document content and the question.\n\nSummarize what Reader Claude got right/wrong for each question.\n\n### Step 3: Run Additional Checks\n\nAnnounce additional checks will be performed.\n\nInvoke sub-agent to check for ambiguity, false assumptions, contradictions.\n\nSummarize any issues found.\n\n### Step 4: Report and Fix\n\nIf issues found:\nReport that Reader Claude struggled with specific issues.\n\nList the specific issues.\n\nIndicate intention to fix these gaps.\n\nLoop back to refinement for problematic sections.\n\n---\n\n**If no access to sub-agents (e.g., claude.ai web interface):**\n\nThe user will need to do the testing manually.\n\n### Step 1: Predict Reader Questions\n\nAsk what questions people might ask when trying to discover this document. What would they type into Claude.ai?\n\nGenerate 5-10 questions that readers would realistically ask.\n\n### Step 2: Setup Testing\n\nProvide testing instructions:\n1. Open a fresh Claude conversation: https://claude.ai\n2. Paste or share the document content (if using a shared doc platform with connectors enabled, provide the link)\n3. Ask Reader Claude the generated questions\n\nFor each question, instruct Reader Claude to provide:\n- The answer\n- Whether anything was ambiguous or unclear\n- What knowledge/context the doc assumes is already known\n\nCheck if Reader Claude gives correct answers or misinterprets anything.\n\n### Step 3: Additional Checks\n\nAlso ask Reader Claude:\n- \"What in this doc might be ambiguous or unclear to readers?\"\n- \"What knowledge or context does this doc assume readers already have?\"\n- \"Are there any internal contradictions or inconsistencies?\"\n\n### Step 4: Iterate Based on Results\n\nAsk what Reader Claude got wrong or struggled with. Indicate intention to fix those gaps.\n\nLoop back to refinement for any problematic sections.\n\n---\n\n### Exit Condition (Both Approaches)\n\nWhen Reader Claude consistently answers questions correctly and doesn't surface new gaps or ambiguities, the doc is ready.\n\n## Final Review\n\nWhen Reader Testing passes:\nAnnounce the doc has passed Reader Claude testing. Before completion:\n\n1. Recommend they do a final read-through themselves - they own this document and are responsible for its quality\n2. Suggest double-checking any facts, links, or technical details\n3. Ask them to verify it achieves the impact they wanted\n\nAsk if they want one more review, or if the work is done.\n\n**If user wants final review, provide it. Otherwise:**\nAnnounce document completion. Provide a few final tips:\n- Consider linking this conversation in an appendix so readers can see how the doc was developed\n- Use appendices to provide depth without bloating the main doc\n- Update the doc as feedback is received from real readers\n\n## Tips for Effective Guidance\n\n**Tone:**\n- Be direct and procedural\n- Explain rationale briefly when it affects user behavior\n- Don't try to \"sell\" the approach - just execute it\n\n**Handling Deviations:**\n- If user wants to skip a stage: Ask if they want to skip this and write freeform\n- If user seems frustrated: Acknowledge this is taking longer than expected. Suggest ways to move faster\n- Always give user agency to adjust the process\n\n**Context Management:**\n- Throughout, if context is missing on something mentioned, proactively ask\n- Don't let gaps accumulate - address them as they come up\n\n**Artifact Management:**\n- Use `create_file` for drafting full sections\n- Use `str_replace` for all edits\n- Provide artifact link after every change\n- Never use artifacts for brainstorming lists - that's just conversation\n\n**Quality over Speed:**\n- Don't rush through stages\n- Each iteration should make meaningful improvements\n- The goal is a document that actually works for readers\n",
      "frontmatter": {
        "name": "doc-coauthoring",
        "description": "Guide users through a structured workflow for co-authoring documentation. Use when user wants to write documentation, proposals, technical specs, decision docs, or similar structured content. This workflow helps users efficiently transfer context, refine content through iteration, and verify the doc works for readers. Trigger when user mentions writing docs, creating proposals, drafting specs, or similar documentation tasks."
      },
      "content": "\n# Doc Co-Authoring Workflow\n\nThis skill provides a structured workflow for guiding users through collaborative document creation. Act as an active guide, walking users through three stages: Context Gathering, Refinement & Structure, and Reader Testing.\n\n## When to Offer This Workflow\n\n**Trigger conditions:**\n- User mentions writing documentation: \"write a doc\", \"draft a proposal\", \"create a spec\", \"write up\"\n- User mentions specific doc types: \"PRD\", \"design doc\", \"decision doc\", \"RFC\"\n- User seems to be starting a substantial writing task\n\n**Initial offer:**\nOffer the user a structured workflow for co-authoring the document. Explain the three stages:\n\n1. **Context Gathering**: User provides all relevant context while Claude asks clarifying questions\n2. **Refinement & Structure**: Iteratively build each section through brainstorming and editing\n3. **Reader Testing**: Test the doc with a fresh Claude (no context) to catch blind spots before others read it\n\nExplain that this approach helps ensure the doc works well when others read it (including when they paste it into Claude). Ask if they want to try this workflow or prefer to work freeform.\n\nIf user declines, work freeform. If user accepts, proceed to Stage 1.\n\n## Stage 1: Context Gathering\n\n**Goal:** Close the gap between what the user knows and what Claude knows, enabling smart guidance later.\n\n### Initial Questions\n\nStart by asking the user for meta-context about the document:\n\n1. What type of document is this? (e.g., technical spec, decision doc, proposal)\n2. Who's the primary audience?\n3. What's the desired impact when someone reads this?\n4. Is there a template or specific format to follow?\n5. Any other constraints or context to know?\n\nInform them they can answer in shorthand or dump information however works best for them.\n\n**If user provides a template or mentions a doc type:**\n- Ask if they have a template document to share\n- If they provide a link to a shared document, use the appropriate integration to fetch it\n- If they provide a file, read it\n\n**If user mentions editing an existing shared document:**\n- Use the appropriate integration to read the current state\n- Check for images without alt-text\n- If images exist without alt-text, explain that when others use Claude to understand the doc, Claude won't be able to see them. Ask if they want alt-text generated. If so, request they paste each image into chat for descriptive alt-text generation.\n\n### Info Dumping\n\nOnce initial questions are answered, encourage the user to dump all the context they have. Request information such as:\n- Background on the project/problem\n- Related team discussions or shared documents\n- Why alternative solutions aren't being used\n- Organizational context (team dynamics, past incidents, politics)\n- Timeline pressures or constraints\n- Technical architecture or dependencies\n- Stakeholder concerns\n\nAdvise them not to worry about organizing it - just get it all out. Offer multiple ways to provide context:\n- Info dump stream-of-consciousness\n- Point to team channels or threads to read\n- Link to shared documents\n\n**If integrations are available** (e.g., Slack, Teams, Google Drive, SharePoint, or other MCP servers), mention that these can be used to pull in context directly.\n\n**If no integrations are detected and in Claude.ai or Claude app:** Suggest they can enable connectors in their Claude settings to allow pulling context from messaging apps and document storage directly.\n\nInform them clarifying questions will be asked once they've done their initial dump.\n\n**During context gathering:**\n\n- If user mentions team channels or shared documents:\n  - If integrations available: Inform them the content will be read now, then use the appropriate integration\n  - If integrations not available: Explain lack of access. Suggest they enable connectors in Claude settings, or paste the relevant content directly.\n\n- If user mentions entities/projects that are unknown:\n  - Ask if connected tools should be searched to learn more\n  - Wait for user confirmation before searching\n\n- As user provides context, track what's being learned and what's still unclear\n\n**Asking clarifying questions:**\n\nWhen user signals they've done their initial dump (or after substantial context provided), ask clarifying questions to ensure understanding:\n\nGenerate 5-10 numbered questions based on gaps in the context.\n\nInform them they can use shorthand to answer (e.g., \"1: yes, 2: see #channel, 3: no because backwards compat\"), link to more docs, point to channels to read, or just keep info-dumping. Whatever's most efficient for them.\n\n**Exit condition:**\nSufficient context has been gathered when questions show understanding - when edge cases and trade-offs can be asked about without needing basics explained.\n\n**Transition:**\nAsk if there's any more context they want to provide at this stage, or if it's time to move on to drafting the document.\n\nIf user wants to add more, let them. When ready, proceed to Stage 2.\n\n## Stage 2: Refinement & Structure\n\n**Goal:** Build the document section by section through brainstorming, curation, and iterative refinement.\n\n**Instructions to user:**\nExplain that the document will be built section by section. For each section:\n1. Clarifying questions will be asked about what to include\n2. 5-20 options will be brainstormed\n3. User will indicate what to keep/remove/combine\n4. The section will be drafted\n5. It will be refined through surgical edits\n\nStart with whichever section has the most unknowns (usually the core decision/proposal), then work through the rest.\n\n**Section ordering:**\n\nIf the document structure is clear:\nAsk which section they'd like to start with.\n\nSuggest starting with whichever section has the most unknowns. For decision docs, that's usually the core proposal. For specs, it's typically the technical approach. Summary sections are best left for last.\n\nIf user doesn't know what sections they need:\nBased on the type of document and template, suggest 3-5 sections appropriate for the doc type.\n\nAsk if this structure works, or if they want to adjust it.\n\n**Once structure is agreed:**\n\nCreate the initial document structure with placeholder text for all sections.\n\n**If access to artifacts is available:**\nUse `create_file` to create an artifact. This gives both Claude and the user a scaffold to work from.\n\nInform them that the initial structure with placeholders for all sections will be created.\n\nCreate artifact with all section headers and brief placeholder text like \"[To be written]\" or \"[Content here]\".\n\nProvide the scaffold link and indicate it's time to fill in each section.\n\n**If no access to artifacts:**\nCreate a markdown file in the working directory. Name it appropriately (e.g., `decision-doc.md`, `technical-spec.md`).\n\nInform them that the initial structure with placeholders for all sections will be created.\n\nCreate file with all section headers and placeholder text.\n\nConfirm the filename has been created and indicate it's time to fill in each section.\n\n**For each section:**\n\n### Step 1: Clarifying Questions\n\nAnnounce work will begin on the [SECTION NAME] section. Ask 5-10 clarifying questions about what should be included:\n\nGenerate 5-10 specific questions based on context and section purpose.\n\nInform them they can answer in shorthand or just indicate what's important to cover.\n\n### Step 2: Brainstorming\n\nFor the [SECTION NAME] section, brainstorm [5-20] things that might be included, depending on the section's complexity. Look for:\n- Context shared that might have been forgotten\n- Angles or considerations not yet mentioned\n\nGenerate 5-20 numbered options based on section complexity. At the end, offer to brainstorm more if they want additional options.\n\n### Step 3: Curation\n\nAsk which points should be kept, removed, or combined. Request brief justifications to help learn priorities for the next sections.\n\nProvide examples:\n- \"Keep 1,4,7,9\"\n- \"Remove 3 (duplicates 1)\"\n- \"Remove 6 (audience already knows this)\"\n- \"Combine 11 and 12\"\n\n**If user gives freeform feedback** (e.g., \"looks good\" or \"I like most of it but...\") instead of numbered selections, extract their preferences and proceed. Parse what they want kept/removed/changed and apply it.\n\n### Step 4: Gap Check\n\nBased on what they've selected, ask if there's anything important missing for the [SECTION NAME] section.\n\n### Step 5: Drafting\n\nUse `str_replace` to replace the placeholder text for this section with the actual drafted content.\n\nAnnounce the [SECTION NAME] section will be drafted now based on what they've selected.\n\n**If using artifacts:**\nAfter drafting, provide a link to the artifact.\n\nAsk them to read through it and indicate what to change. Note that being specific helps learning for the next sections.\n\n**If using a file (no artifacts):**\nAfter drafting, confirm completion.\n\nInform them the [SECTION NAME] section has been drafted in [filename]. Ask them to read through it and indicate what to change. Note that being specific helps learning for the next sections.\n\n**Key instruction for user (include when drafting the first section):**\nProvide a note: Instead of editing the doc directly, ask them to indicate what to change. This helps learning of their style for future sections. For example: \"Remove the X bullet - already covered by Y\" or \"Make the third paragraph more concise\".\n\n### Step 6: Iterative Refinement\n\nAs user provides feedback:\n- Use `str_replace` to make edits (never reprint the whole doc)\n- **If using artifacts:** Provide link to artifact after each edit\n- **If using files:** Just confirm edits are complete\n- If user edits doc directly and asks to read it: mentally note the changes they made and keep them in mind for future sections (this shows their preferences)\n\n**Continue iterating** until user is satisfied with the section.\n\n### Quality Checking\n\nAfter 3 consecutive iterations with no substantial changes, ask if anything can be removed without losing important information.\n\nWhen section is done, confirm [SECTION NAME] is complete. Ask if ready to move to the next section.\n\n**Repeat for all sections.**\n\n### Near Completion\n\nAs approaching completion (80%+ of sections done), announce intention to re-read the entire document and check for:\n- Flow and consistency across sections\n- Redundancy or contradictions\n- Anything that feels like \"slop\" or generic filler\n- Whether every sentence carries weight\n\nRead entire document and provide feedback.\n\n**When all sections are drafted and refined:**\nAnnounce all sections are drafted. Indicate intention to review the complete document one more time.\n\nReview for overall coherence, flow, completeness.\n\nProvide any final suggestions.\n\nAsk if ready to move to Reader Testing, or if they want to refine anything else.\n\n## Stage 3: Reader Testing\n\n**Goal:** Test the document with a fresh Claude (no context bleed) to verify it works for readers.\n\n**Instructions to user:**\nExplain that testing will now occur to see if the document actually works for readers. This catches blind spots - things that make sense to the authors but might confuse others.\n\n### Testing Approach\n\n**If access to sub-agents is available (e.g., in Claude Code):**\n\nPerform the testing directly without user involvement.\n\n### Step 1: Predict Reader Questions\n\nAnnounce intention to predict what questions readers might ask when trying to discover this document.\n\nGenerate 5-10 questions that readers would realistically ask.\n\n### Step 2: Test with Sub-Agent\n\nAnnounce that these questions will be tested with a fresh Claude instance (no context from this conversation).\n\nFor each question, invoke a sub-agent with just the document content and the question.\n\nSummarize what Reader Claude got right/wrong for each question.\n\n### Step 3: Run Additional Checks\n\nAnnounce additional checks will be performed.\n\nInvoke sub-agent to check for ambiguity, false assumptions, contradictions.\n\nSummarize any issues found.\n\n### Step 4: Report and Fix\n\nIf issues found:\nReport that Reader Claude struggled with specific issues.\n\nList the specific issues.\n\nIndicate intention to fix these gaps.\n\nLoop back to refinement for problematic sections.\n\n---\n\n**If no access to sub-agents (e.g., claude.ai web interface):**\n\nThe user will need to do the testing manually.\n\n### Step 1: Predict Reader Questions\n\nAsk what questions people might ask when trying to discover this document. What would they type into Claude.ai?\n\nGenerate 5-10 questions that readers would realistically ask.\n\n### Step 2: Setup Testing\n\nProvide testing instructions:\n1. Open a fresh Claude conversation: https://claude.ai\n2. Paste or share the document content (if using a shared doc platform with connectors enabled, provide the link)\n3. Ask Reader Claude the generated questions\n\nFor each question, instruct Reader Claude to provide:\n- The answer\n- Whether anything was ambiguous or unclear\n- What knowledge/context the doc assumes is already known\n\nCheck if Reader Claude gives correct answers or misinterprets anything.\n\n### Step 3: Additional Checks\n\nAlso ask Reader Claude:\n- \"What in this doc might be ambiguous or unclear to readers?\"\n- \"What knowledge or context does this doc assume readers already have?\"\n- \"Are there any internal contradictions or inconsistencies?\"\n\n### Step 4: Iterate Based on Results\n\nAsk what Reader Claude got wrong or struggled with. Indicate intention to fix those gaps.\n\nLoop back to refinement for any problematic sections.\n\n---\n\n### Exit Condition (Both Approaches)\n\nWhen Reader Claude consistently answers questions correctly and doesn't surface new gaps or ambiguities, the doc is ready.\n\n## Final Review\n\nWhen Reader Testing passes:\nAnnounce the doc has passed Reader Claude testing. Before completion:\n\n1. Recommend they do a final read-through themselves - they own this document and are responsible for its quality\n2. Suggest double-checking any facts, links, or technical details\n3. Ask them to verify it achieves the impact they wanted\n\nAsk if they want one more review, or if the work is done.\n\n**If user wants final review, provide it. Otherwise:**\nAnnounce document completion. Provide a few final tips:\n- Consider linking this conversation in an appendix so readers can see how the doc was developed\n- Use appendices to provide depth without bloating the main doc\n- Update the doc as feedback is received from real readers\n\n## Tips for Effective Guidance\n\n**Tone:**\n- Be direct and procedural\n- Explain rationale briefly when it affects user behavior\n- Don't try to \"sell\" the approach - just execute it\n\n**Handling Deviations:**\n- If user wants to skip a stage: Ask if they want to skip this and write freeform\n- If user seems frustrated: Acknowledge this is taking longer than expected. Suggest ways to move faster\n- Always give user agency to adjust the process\n\n**Context Management:**\n- Throughout, if context is missing on something mentioned, proactively ask\n- Don't let gaps accumulate - address them as they come up\n\n**Artifact Management:**\n- Use `create_file` for drafting full sections\n- Use `str_replace` for all edits\n- Provide artifact link after every change\n- Never use artifacts for brainstorming lists - that's just conversation\n\n**Quality over Speed:**\n- Don't rush through stages\n- Each iteration should make meaningful improvements\n- The goal is a document that actually works for readers\n"
    }
  },
  "anthropics-skills-docx": {
    "id": "anthropics-skills-docx",
    "name": "docx",
    "description": "Comprehensive document creation, editing, and analysis with support for tracked changes, comments, formatting preservation, and text extraction. When Claude needs to work with professional documents (.docx files) for: (1) Creating new documents, (2) Modifying or editing content, (3) Working with tracked changes, (4) Adding comments, or any other document tasks",
    "repo": {
      "owner": "anthropics",
      "name": "skills",
      "fullName": "anthropics/skills",
      "url": "https://github.com/anthropics/skills/tree/main/skills/docx",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 35374,
      "forks": 3202,
      "language": "TypeScript",
      "topics": [],
      "updatedAt": "2026-01-08T06:00:29Z",
      "pushedAt": "2025-12-20T18:09:45Z"
    },
    "category": "official",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: docx\ndescription: \"Comprehensive document creation, editing, and analysis with support for tracked changes, comments, formatting preservation, and text extraction. When Claude needs to work with professional documents (.docx files) for: (1) Creating new documents, (2) Modifying or editing content, (3) Working with tracked changes, (4) Adding comments, or any other document tasks\"\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# DOCX creation, editing, and analysis\n\n## Overview\n\nA user may ask you to create, edit, or analyze the contents of a .docx file. A .docx file is essentially a ZIP archive containing XML files and other resources that you can read or edit. You have different tools and workflows available for different tasks.\n\n## Workflow Decision Tree\n\n### Reading/Analyzing Content\nUse \"Text extraction\" or \"Raw XML access\" sections below\n\n### Creating New Document\nUse \"Creating a new Word document\" workflow\n\n### Editing Existing Document\n- **Your own document + simple changes**\n  Use \"Basic OOXML editing\" workflow\n\n- **Someone else's document**\n  Use **\"Redlining workflow\"** (recommended default)\n\n- **Legal, academic, business, or government docs**\n  Use **\"Redlining workflow\"** (required)\n\n## Reading and analyzing content\n\n### Text extraction\nIf you just need to read the text contents of a document, you should convert the document to markdown using pandoc. Pandoc provides excellent support for preserving document structure and can show tracked changes:\n\n```bash\n# Convert document to markdown with tracked changes\npandoc --track-changes=all path-to-file.docx -o output.md\n# Options: --track-changes=accept/reject/all\n```\n\n### Raw XML access\nYou need raw XML access for: comments, complex formatting, document structure, embedded media, and metadata. For any of these features, you'll need to unpack a document and read its raw XML contents.\n\n#### Unpacking a file\n`python ooxml/scripts/unpack.py <office_file> <output_directory>`\n\n#### Key file structures\n* `word/document.xml` - Main document contents\n* `word/comments.xml` - Comments referenced in document.xml\n* `word/media/` - Embedded images and media files\n* Tracked changes use `<w:ins>` (insertions) and `<w:del>` (deletions) tags\n\n## Creating a new Word document\n\nWhen creating a new Word document from scratch, use **docx-js**, which allows you to create Word documents using JavaScript/TypeScript.\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`docx-js.md`](docx-js.md) (~500 lines) completely from start to finish. **NEVER set any range limits when reading this file.** Read the full file content for detailed syntax, critical formatting rules, and best practices before proceeding with document creation.\n2. Create a JavaScript/TypeScript file using Document, Paragraph, TextRun components (You can assume all dependencies are installed, but if not, refer to the dependencies section below)\n3. Export as .docx using Packer.toBuffer()\n\n## Editing an existing Word document\n\nWhen editing an existing Word document, use the **Document library** (a Python library for OOXML manipulation). The library automatically handles infrastructure setup and provides methods for document manipulation. For complex scenarios, you can access the underlying DOM directly through the library.\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`ooxml.md`](ooxml.md) (~600 lines) completely from start to finish. **NEVER set any range limits when reading this file.** Read the full file content for the Document library API and XML patterns for directly editing document files.\n2. Unpack the document: `python ooxml/scripts/unpack.py <office_file> <output_directory>`\n3. Create and run a Python script using the Document library (see \"Document Library\" section in ooxml.md)\n4. Pack the final document: `python ooxml/scripts/pack.py <input_directory> <office_file>`\n\nThe Document library provides both high-level methods for common operations and direct DOM access for complex scenarios.\n\n## Redlining workflow for document review\n\nThis workflow allows you to plan comprehensive tracked changes using markdown before implementing them in OOXML. **CRITICAL**: For complete tracked changes, you must implement ALL changes systematically.\n\n**Batching Strategy**: Group related changes into batches of 3-10 changes. This makes debugging manageable while maintaining efficiency. Test each batch before moving to the next.\n\n**Principle: Minimal, Precise Edits**\nWhen implementing tracked changes, only mark text that actually changes. Repeating unchanged text makes edits harder to review and appears unprofessional. Break replacements into: [unchanged text] + [deletion] + [insertion] + [unchanged text]. Preserve the original run's RSID for unchanged text by extracting the `<w:r>` element from the original and reusing it.\n\nExample - Changing \"30 days\" to \"60 days\" in a sentence:\n```python\n# BAD - Replaces entire sentence\n'<w:del><w:r><w:delText>The term is 30 days.</w:delText></w:r></w:del><w:ins><w:r><w:t>The term is 60 days.</w:t></w:r></w:ins>'\n\n# GOOD - Only marks what changed, preserves original <w:r> for unchanged text\n'<w:r w:rsidR=\"00AB12CD\"><w:t>The term is </w:t></w:r><w:del><w:r><w:delText>30</w:delText></w:r></w:del><w:ins><w:r><w:t>60</w:t></w:r></w:ins><w:r w:rsidR=\"00AB12CD\"><w:t> days.</w:t></w:r>'\n```\n\n### Tracked changes workflow\n\n1. **Get markdown representation**: Convert document to markdown with tracked changes preserved:\n   ```bash\n   pandoc --track-changes=all path-to-file.docx -o current.md\n   ```\n\n2. **Identify and group changes**: Review the document and identify ALL changes needed, organizing them into logical batches:\n\n   **Location methods** (for finding changes in XML):\n   - Section/heading numbers (e.g., \"Section 3.2\", \"Article IV\")\n   - Paragraph identifiers if numbered\n   - Grep patterns with unique surrounding text\n   - Document structure (e.g., \"first paragraph\", \"signature block\")\n   - **DO NOT use markdown line numbers** - they don't map to XML structure\n\n   **Batch organization** (group 3-10 related changes per batch):\n   - By section: \"Batch 1: Section 2 amendments\", \"Batch 2: Section 5 updates\"\n   - By type: \"Batch 1: Date corrections\", \"Batch 2: Party name changes\"\n   - By complexity: Start with simple text replacements, then tackle complex structural changes\n   - Sequential: \"Batch 1: Pages 1-3\", \"Batch 2: Pages 4-6\"\n\n3. **Read documentation and unpack**:\n   - **MANDATORY - READ ENTIRE FILE**: Read [`ooxml.md`](ooxml.md) (~600 lines) completely from start to finish. **NEVER set any range limits when reading this file.** Pay special attention to the \"Document Library\" and \"Tracked Change Patterns\" sections.\n   - **Unpack the document**: `python ooxml/scripts/unpack.py <file.docx> <dir>`\n   - **Note the suggested RSID**: The unpack script will suggest an RSID to use for your tracked changes. Copy this RSID for use in step 4b.\n\n4. **Implement changes in batches**: Group changes logically (by section, by type, or by proximity) and implement them together in a single script. This approach:\n   - Makes debugging easier (smaller batch = easier to isolate errors)\n   - Allows incremental progress\n   - Maintains efficiency (batch size of 3-10 changes works well)\n\n   **Suggested batch groupings:**\n   - By document section (e.g., \"Section 3 changes\", \"Definitions\", \"Termination clause\")\n   - By change type (e.g., \"Date changes\", \"Party name updates\", \"Legal term replacements\")\n   - By proximity (e.g., \"Changes on pages 1-3\", \"Changes in first half of document\")\n\n   For each batch of related changes:\n\n   **a. Map text to XML**: Grep for text in `word/document.xml` to verify how text is split across `<w:r>` elements.\n\n   **b. Create and run script**: Use `get_node` to find nodes, implement changes, then `doc.save()`. See **\"Document Library\"** section in ooxml.md for patterns.\n\n   **Note**: Always grep `word/document.xml` immediately before writing a script to get current line numbers and verify text content. Line numbers change after each script run.\n\n5. **Pack the document**: After all batches are complete, convert the unpacked directory back to .docx:\n   ```bash\n   python ooxml/scripts/pack.py unpacked reviewed-document.docx\n   ```\n\n6. **Final verification**: Do a comprehensive check of the complete document:\n   - Convert final document to markdown:\n     ```bash\n     pandoc --track-changes=all reviewed-document.docx -o verification.md\n     ```\n   - Verify ALL changes were applied correctly:\n     ```bash\n     grep \"original phrase\" verification.md  # Should NOT find it\n     grep \"replacement phrase\" verification.md  # Should find it\n     ```\n   - Check that no unintended changes were introduced\n\n\n## Converting Documents to Images\n\nTo visually analyze Word documents, convert them to images using a two-step process:\n\n1. **Convert DOCX to PDF**:\n   ```bash\n   soffice --headless --convert-to pdf document.docx\n   ```\n\n2. **Convert PDF pages to JPEG images**:\n   ```bash\n   pdftoppm -jpeg -r 150 document.pdf page\n   ```\n   This creates files like `page-1.jpg`, `page-2.jpg`, etc.\n\nOptions:\n- `-r 150`: Sets resolution to 150 DPI (adjust for quality/size balance)\n- `-jpeg`: Output JPEG format (use `-png` for PNG if preferred)\n- `-f N`: First page to convert (e.g., `-f 2` starts from page 2)\n- `-l N`: Last page to convert (e.g., `-l 5` stops at page 5)\n- `page`: Prefix for output files\n\nExample for specific range:\n```bash\npdftoppm -jpeg -r 150 -f 2 -l 5 document.pdf page  # Converts only pages 2-5\n```\n\n## Code Style Guidelines\n**IMPORTANT**: When generating code for DOCX operations:\n- Write concise code\n- Avoid verbose variable names and redundant operations\n- Avoid unnecessary print statements\n\n## Dependencies\n\nRequired dependencies (install if not available):\n\n- **pandoc**: `sudo apt-get install pandoc` (for text extraction)\n- **docx**: `npm install -g docx` (for creating new documents)\n- **LibreOffice**: `sudo apt-get install libreoffice` (for PDF conversion)\n- **Poppler**: `sudo apt-get install poppler-utils` (for pdftoppm to convert PDF to images)\n- **defusedxml**: `pip install defusedxml` (for secure XML parsing)",
      "frontmatter": {
        "name": "docx",
        "description": "Comprehensive document creation, editing, and analysis with support for tracked changes, comments, formatting preservation, and text extraction. When Claude needs to work with professional documents (.docx files) for: (1) Creating new documents, (2) Modifying or editing content, (3) Working with tracked changes, (4) Adding comments, or any other document tasks",
        "license": "Proprietary. LICENSE.txt has complete terms"
      },
      "content": "\n# DOCX creation, editing, and analysis\n\n## Overview\n\nA user may ask you to create, edit, or analyze the contents of a .docx file. A .docx file is essentially a ZIP archive containing XML files and other resources that you can read or edit. You have different tools and workflows available for different tasks.\n\n## Workflow Decision Tree\n\n### Reading/Analyzing Content\nUse \"Text extraction\" or \"Raw XML access\" sections below\n\n### Creating New Document\nUse \"Creating a new Word document\" workflow\n\n### Editing Existing Document\n- **Your own document + simple changes**\n  Use \"Basic OOXML editing\" workflow\n\n- **Someone else's document**\n  Use **\"Redlining workflow\"** (recommended default)\n\n- **Legal, academic, business, or government docs**\n  Use **\"Redlining workflow\"** (required)\n\n## Reading and analyzing content\n\n### Text extraction\nIf you just need to read the text contents of a document, you should convert the document to markdown using pandoc. Pandoc provides excellent support for preserving document structure and can show tracked changes:\n\n```bash\n# Convert document to markdown with tracked changes\npandoc --track-changes=all path-to-file.docx -o output.md\n# Options: --track-changes=accept/reject/all\n```\n\n### Raw XML access\nYou need raw XML access for: comments, complex formatting, document structure, embedded media, and metadata. For any of these features, you'll need to unpack a document and read its raw XML contents.\n\n#### Unpacking a file\n`python ooxml/scripts/unpack.py <office_file> <output_directory>`\n\n#### Key file structures\n* `word/document.xml` - Main document contents\n* `word/comments.xml` - Comments referenced in document.xml\n* `word/media/` - Embedded images and media files\n* Tracked changes use `<w:ins>` (insertions) and `<w:del>` (deletions) tags\n\n## Creating a new Word document\n\nWhen creating a new Word document from scratch, use **docx-js**, which allows you to create Word documents using JavaScript/TypeScript.\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`docx-js.md`](docx-js.md) (~500 lines) completely from start to finish. **NEVER set any range limits when reading this file.** Read the full file content for detailed syntax, critical formatting rules, and best practices before proceeding with document creation.\n2. Create a JavaScript/TypeScript file using Document, Paragraph, TextRun components (You can assume all dependencies are installed, but if not, refer to the dependencies section below)\n3. Export as .docx using Packer.toBuffer()\n\n## Editing an existing Word document\n\nWhen editing an existing Word document, use the **Document library** (a Python library for OOXML manipulation). The library automatically handles infrastructure setup and provides methods for document manipulation. For complex scenarios, you can access the underlying DOM directly through the library.\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`ooxml.md`](ooxml.md) (~600 lines) completely from start to finish. **NEVER set any range limits when reading this file.** Read the full file content for the Document library API and XML patterns for directly editing document files.\n2. Unpack the document: `python ooxml/scripts/unpack.py <office_file> <output_directory>`\n3. Create and run a Python script using the Document library (see \"Document Library\" section in ooxml.md)\n4. Pack the final document: `python ooxml/scripts/pack.py <input_directory> <office_file>`\n\nThe Document library provides both high-level methods for common operations and direct DOM access for complex scenarios.\n\n## Redlining workflow for document review\n\nThis workflow allows you to plan comprehensive tracked changes using markdown before implementing them in OOXML. **CRITICAL**: For complete tracked changes, you must implement ALL changes systematically.\n\n**Batching Strategy**: Group related changes into batches of 3-10 changes. This makes debugging manageable while maintaining efficiency. Test each batch before moving to the next.\n\n**Principle: Minimal, Precise Edits**\nWhen implementing tracked changes, only mark text that actually changes. Repeating unchanged text makes edits harder to review and appears unprofessional. Break replacements into: [unchanged text] + [deletion] + [insertion] + [unchanged text]. Preserve the original run's RSID for unchanged text by extracting the `<w:r>` element from the original and reusing it.\n\nExample - Changing \"30 days\" to \"60 days\" in a sentence:\n```python\n# BAD - Replaces entire sentence\n'<w:del><w:r><w:delText>The term is 30 days.</w:delText></w:r></w:del><w:ins><w:r><w:t>The term is 60 days.</w:t></w:r></w:ins>'\n\n# GOOD - Only marks what changed, preserves original <w:r> for unchanged text\n'<w:r w:rsidR=\"00AB12CD\"><w:t>The term is </w:t></w:r><w:del><w:r><w:delText>30</w:delText></w:r></w:del><w:ins><w:r><w:t>60</w:t></w:r></w:ins><w:r w:rsidR=\"00AB12CD\"><w:t> days.</w:t></w:r>'\n```\n\n### Tracked changes workflow\n\n1. **Get markdown representation**: Convert document to markdown with tracked changes preserved:\n   ```bash\n   pandoc --track-changes=all path-to-file.docx -o current.md\n   ```\n\n2. **Identify and group changes**: Review the document and identify ALL changes needed, organizing them into logical batches:\n\n   **Location methods** (for finding changes in XML):\n   - Section/heading numbers (e.g., \"Section 3.2\", \"Article IV\")\n   - Paragraph identifiers if numbered\n   - Grep patterns with unique surrounding text\n   - Document structure (e.g., \"first paragraph\", \"signature block\")\n   - **DO NOT use markdown line numbers** - they don't map to XML structure\n\n   **Batch organization** (group 3-10 related changes per batch):\n   - By section: \"Batch 1: Section 2 amendments\", \"Batch 2: Section 5 updates\"\n   - By type: \"Batch 1: Date corrections\", \"Batch 2: Party name changes\"\n   - By complexity: Start with simple text replacements, then tackle complex structural changes\n   - Sequential: \"Batch 1: Pages 1-3\", \"Batch 2: Pages 4-6\"\n\n3. **Read documentation and unpack**:\n   - **MANDATORY - READ ENTIRE FILE**: Read [`ooxml.md`](ooxml.md) (~600 lines) completely from start to finish. **NEVER set any range limits when reading this file.** Pay special attention to the \"Document Library\" and \"Tracked Change Patterns\" sections.\n   - **Unpack the document**: `python ooxml/scripts/unpack.py <file.docx> <dir>`\n   - **Note the suggested RSID**: The unpack script will suggest an RSID to use for your tracked changes. Copy this RSID for use in step 4b.\n\n4. **Implement changes in batches**: Group changes logically (by section, by type, or by proximity) and implement them together in a single script. This approach:\n   - Makes debugging easier (smaller batch = easier to isolate errors)\n   - Allows incremental progress\n   - Maintains efficiency (batch size of 3-10 changes works well)\n\n   **Suggested batch groupings:**\n   - By document section (e.g., \"Section 3 changes\", \"Definitions\", \"Termination clause\")\n   - By change type (e.g., \"Date changes\", \"Party name updates\", \"Legal term replacements\")\n   - By proximity (e.g., \"Changes on pages 1-3\", \"Changes in first half of document\")\n\n   For each batch of related changes:\n\n   **a. Map text to XML**: Grep for text in `word/document.xml` to verify how text is split across `<w:r>` elements.\n\n   **b. Create and run script**: Use `get_node` to find nodes, implement changes, then `doc.save()`. See **\"Document Library\"** section in ooxml.md for patterns.\n\n   **Note**: Always grep `word/document.xml` immediately before writing a script to get current line numbers and verify text content. Line numbers change after each script run.\n\n5. **Pack the document**: After all batches are complete, convert the unpacked directory back to .docx:\n   ```bash\n   python ooxml/scripts/pack.py unpacked reviewed-document.docx\n   ```\n\n6. **Final verification**: Do a comprehensive check of the complete document:\n   - Convert final document to markdown:\n     ```bash\n     pandoc --track-changes=all reviewed-document.docx -o verification.md\n     ```\n   - Verify ALL changes were applied correctly:\n     ```bash\n     grep \"original phrase\" verification.md  # Should NOT find it\n     grep \"replacement phrase\" verification.md  # Should find it\n     ```\n   - Check that no unintended changes were introduced\n\n\n## Converting Documents to Images\n\nTo visually analyze Word documents, convert them to images using a two-step process:\n\n1. **Convert DOCX to PDF**:\n   ```bash\n   soffice --headless --convert-to pdf document.docx\n   ```\n\n2. **Convert PDF pages to JPEG images**:\n   ```bash\n   pdftoppm -jpeg -r 150 document.pdf page\n   ```\n   This creates files like `page-1.jpg`, `page-2.jpg`, etc.\n\nOptions:\n- `-r 150`: Sets resolution to 150 DPI (adjust for quality/size balance)\n- `-jpeg`: Output JPEG format (use `-png` for PNG if preferred)\n- `-f N`: First page to convert (e.g., `-f 2` starts from page 2)\n- `-l N`: Last page to convert (e.g., `-l 5` stops at page 5)\n- `page`: Prefix for output files\n\nExample for specific range:\n```bash\npdftoppm -jpeg -r 150 -f 2 -l 5 document.pdf page  # Converts only pages 2-5\n```\n\n## Code Style Guidelines\n**IMPORTANT**: When generating code for DOCX operations:\n- Write concise code\n- Avoid verbose variable names and redundant operations\n- Avoid unnecessary print statements\n\n## Dependencies\n\nRequired dependencies (install if not available):\n\n- **pandoc**: `sudo apt-get install pandoc` (for text extraction)\n- **docx**: `npm install -g docx` (for creating new documents)\n- **LibreOffice**: `sudo apt-get install libreoffice` (for PDF conversion)\n- **Poppler**: `sudo apt-get install poppler-utils` (for pdftoppm to convert PDF to images)\n- **defusedxml**: `pip install defusedxml` (for secure XML parsing)"
    }
  },
  "anthropics-skills-frontend-design": {
    "id": "anthropics-skills-frontend-design",
    "name": "frontend-design",
    "description": "Create distinctive, production-grade frontend interfaces with high design quality. Use this skill when the user asks to build web components, pages, artifacts, posters, or applications (examples include websites, landing pages, dashboards, React components, HTML/CSS layouts, or when styling/beautifying any web UI). Generates creative, polished code and UI design that avoids generic AI aesthetics.",
    "repo": {
      "owner": "anthropics",
      "name": "skills",
      "fullName": "anthropics/skills",
      "url": "https://github.com/anthropics/skills/tree/main/skills/frontend-design",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 35374,
      "forks": 3202,
      "language": "TypeScript",
      "topics": [],
      "updatedAt": "2026-01-08T06:00:29Z",
      "pushedAt": "2025-12-20T18:09:45Z"
    },
    "category": "official",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: frontend-design\ndescription: Create distinctive, production-grade frontend interfaces with high design quality. Use this skill when the user asks to build web components, pages, artifacts, posters, or applications (examples include websites, landing pages, dashboards, React components, HTML/CSS layouts, or when styling/beautifying any web UI). Generates creative, polished code and UI design that avoids generic AI aesthetics.\nlicense: Complete terms in LICENSE.txt\n---\n\nThis skill guides creation of distinctive, production-grade frontend interfaces that avoid generic \"AI slop\" aesthetics. Implement real working code with exceptional attention to aesthetic details and creative choices.\n\nThe user provides frontend requirements: a component, page, application, or interface to build. They may include context about the purpose, audience, or technical constraints.\n\n## Design Thinking\n\nBefore coding, understand the context and commit to a BOLD aesthetic direction:\n- **Purpose**: What problem does this interface solve? Who uses it?\n- **Tone**: Pick an extreme: brutally minimal, maximalist chaos, retro-futuristic, organic/natural, luxury/refined, playful/toy-like, editorial/magazine, brutalist/raw, art deco/geometric, soft/pastel, industrial/utilitarian, etc. There are so many flavors to choose from. Use these for inspiration but design one that is true to the aesthetic direction.\n- **Constraints**: Technical requirements (framework, performance, accessibility).\n- **Differentiation**: What makes this UNFORGETTABLE? What's the one thing someone will remember?\n\n**CRITICAL**: Choose a clear conceptual direction and execute it with precision. Bold maximalism and refined minimalism both work - the key is intentionality, not intensity.\n\nThen implement working code (HTML/CSS/JS, React, Vue, etc.) that is:\n- Production-grade and functional\n- Visually striking and memorable\n- Cohesive with a clear aesthetic point-of-view\n- Meticulously refined in every detail\n\n## Frontend Aesthetics Guidelines\n\nFocus on:\n- **Typography**: Choose fonts that are beautiful, unique, and interesting. Avoid generic fonts like Arial and Inter; opt instead for distinctive choices that elevate the frontend's aesthetics; unexpected, characterful font choices. Pair a distinctive display font with a refined body font.\n- **Color & Theme**: Commit to a cohesive aesthetic. Use CSS variables for consistency. Dominant colors with sharp accents outperform timid, evenly-distributed palettes.\n- **Motion**: Use animations for effects and micro-interactions. Prioritize CSS-only solutions for HTML. Use Motion library for React when available. Focus on high-impact moments: one well-orchestrated page load with staggered reveals (animation-delay) creates more delight than scattered micro-interactions. Use scroll-triggering and hover states that surprise.\n- **Spatial Composition**: Unexpected layouts. Asymmetry. Overlap. Diagonal flow. Grid-breaking elements. Generous negative space OR controlled density.\n- **Backgrounds & Visual Details**: Create atmosphere and depth rather than defaulting to solid colors. Add contextual effects and textures that match the overall aesthetic. Apply creative forms like gradient meshes, noise textures, geometric patterns, layered transparencies, dramatic shadows, decorative borders, custom cursors, and grain overlays.\n\nNEVER use generic AI-generated aesthetics like overused font families (Inter, Roboto, Arial, system fonts), cliched color schemes (particularly purple gradients on white backgrounds), predictable layouts and component patterns, and cookie-cutter design that lacks context-specific character.\n\nInterpret creatively and make unexpected choices that feel genuinely designed for the context. No design should be the same. Vary between light and dark themes, different fonts, different aesthetics. NEVER converge on common choices (Space Grotesk, for example) across generations.\n\n**IMPORTANT**: Match implementation complexity to the aesthetic vision. Maximalist designs need elaborate code with extensive animations and effects. Minimalist or refined designs need restraint, precision, and careful attention to spacing, typography, and subtle details. Elegance comes from executing the vision well.\n\nRemember: Claude is capable of extraordinary creative work. Don't hold back, show what can truly be created when thinking outside the box and committing fully to a distinctive vision.\n",
      "frontmatter": {
        "name": "frontend-design",
        "description": "Create distinctive, production-grade frontend interfaces with high design quality. Use this skill when the user asks to build web components, pages, artifacts, posters, or applications (examples include websites, landing pages, dashboards, React components, HTML/CSS layouts, or when styling/beautifying any web UI). Generates creative, polished code and UI design that avoids generic AI aesthetics.",
        "license": "Complete terms in LICENSE.txt"
      },
      "content": "\nThis skill guides creation of distinctive, production-grade frontend interfaces that avoid generic \"AI slop\" aesthetics. Implement real working code with exceptional attention to aesthetic details and creative choices.\n\nThe user provides frontend requirements: a component, page, application, or interface to build. They may include context about the purpose, audience, or technical constraints.\n\n## Design Thinking\n\nBefore coding, understand the context and commit to a BOLD aesthetic direction:\n- **Purpose**: What problem does this interface solve? Who uses it?\n- **Tone**: Pick an extreme: brutally minimal, maximalist chaos, retro-futuristic, organic/natural, luxury/refined, playful/toy-like, editorial/magazine, brutalist/raw, art deco/geometric, soft/pastel, industrial/utilitarian, etc. There are so many flavors to choose from. Use these for inspiration but design one that is true to the aesthetic direction.\n- **Constraints**: Technical requirements (framework, performance, accessibility).\n- **Differentiation**: What makes this UNFORGETTABLE? What's the one thing someone will remember?\n\n**CRITICAL**: Choose a clear conceptual direction and execute it with precision. Bold maximalism and refined minimalism both work - the key is intentionality, not intensity.\n\nThen implement working code (HTML/CSS/JS, React, Vue, etc.) that is:\n- Production-grade and functional\n- Visually striking and memorable\n- Cohesive with a clear aesthetic point-of-view\n- Meticulously refined in every detail\n\n## Frontend Aesthetics Guidelines\n\nFocus on:\n- **Typography**: Choose fonts that are beautiful, unique, and interesting. Avoid generic fonts like Arial and Inter; opt instead for distinctive choices that elevate the frontend's aesthetics; unexpected, characterful font choices. Pair a distinctive display font with a refined body font.\n- **Color & Theme**: Commit to a cohesive aesthetic. Use CSS variables for consistency. Dominant colors with sharp accents outperform timid, evenly-distributed palettes.\n- **Motion**: Use animations for effects and micro-interactions. Prioritize CSS-only solutions for HTML. Use Motion library for React when available. Focus on high-impact moments: one well-orchestrated page load with staggered reveals (animation-delay) creates more delight than scattered micro-interactions. Use scroll-triggering and hover states that surprise.\n- **Spatial Composition**: Unexpected layouts. Asymmetry. Overlap. Diagonal flow. Grid-breaking elements. Generous negative space OR controlled density.\n- **Backgrounds & Visual Details**: Create atmosphere and depth rather than defaulting to solid colors. Add contextual effects and textures that match the overall aesthetic. Apply creative forms like gradient meshes, noise textures, geometric patterns, layered transparencies, dramatic shadows, decorative borders, custom cursors, and grain overlays.\n\nNEVER use generic AI-generated aesthetics like overused font families (Inter, Roboto, Arial, system fonts), cliched color schemes (particularly purple gradients on white backgrounds), predictable layouts and component patterns, and cookie-cutter design that lacks context-specific character.\n\nInterpret creatively and make unexpected choices that feel genuinely designed for the context. No design should be the same. Vary between light and dark themes, different fonts, different aesthetics. NEVER converge on common choices (Space Grotesk, for example) across generations.\n\n**IMPORTANT**: Match implementation complexity to the aesthetic vision. Maximalist designs need elaborate code with extensive animations and effects. Minimalist or refined designs need restraint, precision, and careful attention to spacing, typography, and subtle details. Elegance comes from executing the vision well.\n\nRemember: Claude is capable of extraordinary creative work. Don't hold back, show what can truly be created when thinking outside the box and committing fully to a distinctive vision.\n"
    }
  },
  "anthropics-skills-internal-comms": {
    "id": "anthropics-skills-internal-comms",
    "name": "internal-comms",
    "description": "A set of resources to help me write all kinds of internal communications, using the formats that my company likes to use. Claude should use this skill whenever asked to write some sort of internal communications (status reports, leadership updates, 3P updates, company newsletters, FAQs, incident reports, project updates, etc.).",
    "repo": {
      "owner": "anthropics",
      "name": "skills",
      "fullName": "anthropics/skills",
      "url": "https://github.com/anthropics/skills/tree/main/skills/internal-comms",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 35374,
      "forks": 3202,
      "language": "TypeScript",
      "topics": [],
      "updatedAt": "2026-01-08T06:00:29Z",
      "pushedAt": "2025-12-20T18:09:45Z"
    },
    "category": "official",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: internal-comms\ndescription: A set of resources to help me write all kinds of internal communications, using the formats that my company likes to use. Claude should use this skill whenever asked to write some sort of internal communications (status reports, leadership updates, 3P updates, company newsletters, FAQs, incident reports, project updates, etc.).\nlicense: Complete terms in LICENSE.txt\n---\n\n## When to use this skill\nTo write internal communications, use this skill for:\n- 3P updates (Progress, Plans, Problems)\n- Company newsletters\n- FAQ responses\n- Status reports\n- Leadership updates\n- Project updates\n- Incident reports\n\n## How to use this skill\n\nTo write any internal communication:\n\n1. **Identify the communication type** from the request\n2. **Load the appropriate guideline file** from the `examples/` directory:\n    - `examples/3p-updates.md` - For Progress/Plans/Problems team updates\n    - `examples/company-newsletter.md` - For company-wide newsletters\n    - `examples/faq-answers.md` - For answering frequently asked questions\n    - `examples/general-comms.md` - For anything else that doesn't explicitly match one of the above\n3. **Follow the specific instructions** in that file for formatting, tone, and content gathering\n\nIf the communication type doesn't match any existing guideline, ask for clarification or more context about the desired format.\n\n## Keywords\n3P updates, company newsletter, company comms, weekly update, faqs, common questions, updates, internal comms\n",
      "frontmatter": {
        "name": "internal-comms",
        "description": "A set of resources to help me write all kinds of internal communications, using the formats that my company likes to use. Claude should use this skill whenever asked to write some sort of internal communications (status reports, leadership updates, 3P updates, company newsletters, FAQs, incident reports, project updates, etc.).",
        "license": "Complete terms in LICENSE.txt"
      },
      "content": "\n## When to use this skill\nTo write internal communications, use this skill for:\n- 3P updates (Progress, Plans, Problems)\n- Company newsletters\n- FAQ responses\n- Status reports\n- Leadership updates\n- Project updates\n- Incident reports\n\n## How to use this skill\n\nTo write any internal communication:\n\n1. **Identify the communication type** from the request\n2. **Load the appropriate guideline file** from the `examples/` directory:\n    - `examples/3p-updates.md` - For Progress/Plans/Problems team updates\n    - `examples/company-newsletter.md` - For company-wide newsletters\n    - `examples/faq-answers.md` - For answering frequently asked questions\n    - `examples/general-comms.md` - For anything else that doesn't explicitly match one of the above\n3. **Follow the specific instructions** in that file for formatting, tone, and content gathering\n\nIf the communication type doesn't match any existing guideline, ask for clarification or more context about the desired format.\n\n## Keywords\n3P updates, company newsletter, company comms, weekly update, faqs, common questions, updates, internal comms\n"
    }
  },
  "anthropics-skills-mcp-builder": {
    "id": "anthropics-skills-mcp-builder",
    "name": "mcp-builder",
    "description": "Guide for creating high-quality MCP (Model Context Protocol) servers that enable LLMs to interact with external services through well-designed tools. Use when building MCP servers to integrate external APIs or services, whether in Python (FastMCP) or Node/TypeScript (MCP SDK).",
    "repo": {
      "owner": "anthropics",
      "name": "skills",
      "fullName": "anthropics/skills",
      "url": "https://github.com/anthropics/skills/tree/main/skills/mcp-builder",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 35374,
      "forks": 3202,
      "language": "TypeScript",
      "topics": [],
      "updatedAt": "2026-01-08T06:00:29Z",
      "pushedAt": "2025-12-20T18:09:45Z"
    },
    "category": "official",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: mcp-builder\ndescription: Guide for creating high-quality MCP (Model Context Protocol) servers that enable LLMs to interact with external services through well-designed tools. Use when building MCP servers to integrate external APIs or services, whether in Python (FastMCP) or Node/TypeScript (MCP SDK).\nlicense: Complete terms in LICENSE.txt\n---\n\n# MCP Server Development Guide\n\n## Overview\n\nCreate MCP (Model Context Protocol) servers that enable LLMs to interact with external services through well-designed tools. The quality of an MCP server is measured by how well it enables LLMs to accomplish real-world tasks.\n\n---\n\n# Process\n\n## ðŸš€ High-Level Workflow\n\nCreating a high-quality MCP server involves four main phases:\n\n### Phase 1: Deep Research and Planning\n\n#### 1.1 Understand Modern MCP Design\n\n**API Coverage vs. Workflow Tools:**\nBalance comprehensive API endpoint coverage with specialized workflow tools. Workflow tools can be more convenient for specific tasks, while comprehensive coverage gives agents flexibility to compose operations. Performance varies by clientâ€”some clients benefit from code execution that combines basic tools, while others work better with higher-level workflows. When uncertain, prioritize comprehensive API coverage.\n\n**Tool Naming and Discoverability:**\nClear, descriptive tool names help agents find the right tools quickly. Use consistent prefixes (e.g., `github_create_issue`, `github_list_repos`) and action-oriented naming.\n\n**Context Management:**\nAgents benefit from concise tool descriptions and the ability to filter/paginate results. Design tools that return focused, relevant data. Some clients support code execution which can help agents filter and process data efficiently.\n\n**Actionable Error Messages:**\nError messages should guide agents toward solutions with specific suggestions and next steps.\n\n#### 1.2 Study MCP Protocol Documentation\n\n**Navigate the MCP specification:**\n\nStart with the sitemap to find relevant pages: `https://modelcontextprotocol.io/sitemap.xml`\n\nThen fetch specific pages with `.md` suffix for markdown format (e.g., `https://modelcontextprotocol.io/specification/draft.md`).\n\nKey pages to review:\n- Specification overview and architecture\n- Transport mechanisms (streamable HTTP, stdio)\n- Tool, resource, and prompt definitions\n\n#### 1.3 Study Framework Documentation\n\n**Recommended stack:**\n- **Language**: TypeScript (high-quality SDK support and good compatibility in many execution environments e.g. MCPB. Plus AI models are good at generating TypeScript code, benefiting from its broad usage, static typing and good linting tools)\n- **Transport**: Streamable HTTP for remote servers, using stateless JSON (simpler to scale and maintain, as opposed to stateful sessions and streaming responses). stdio for local servers.\n\n**Load framework documentation:**\n\n- **MCP Best Practices**: [ðŸ“‹ View Best Practices](./reference/mcp_best_practices.md) - Core guidelines\n\n**For TypeScript (recommended):**\n- **TypeScript SDK**: Use WebFetch to load `https://raw.githubusercontent.com/modelcontextprotocol/typescript-sdk/main/README.md`\n- [âš¡ TypeScript Guide](./reference/node_mcp_server.md) - TypeScript patterns and examples\n\n**For Python:**\n- **Python SDK**: Use WebFetch to load `https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/README.md`\n- [ðŸ Python Guide](./reference/python_mcp_server.md) - Python patterns and examples\n\n#### 1.4 Plan Your Implementation\n\n**Understand the API:**\nReview the service's API documentation to identify key endpoints, authentication requirements, and data models. Use web search and WebFetch as needed.\n\n**Tool Selection:**\nPrioritize comprehensive API coverage. List endpoints to implement, starting with the most common operations.\n\n---\n\n### Phase 2: Implementation\n\n#### 2.1 Set Up Project Structure\n\nSee language-specific guides for project setup:\n- [âš¡ TypeScript Guide](./reference/node_mcp_server.md) - Project structure, package.json, tsconfig.json\n- [ðŸ Python Guide](./reference/python_mcp_server.md) - Module organization, dependencies\n\n#### 2.2 Implement Core Infrastructure\n\nCreate shared utilities:\n- API client with authentication\n- Error handling helpers\n- Response formatting (JSON/Markdown)\n- Pagination support\n\n#### 2.3 Implement Tools\n\nFor each tool:\n\n**Input Schema:**\n- Use Zod (TypeScript) or Pydantic (Python)\n- Include constraints and clear descriptions\n- Add examples in field descriptions\n\n**Output Schema:**\n- Define `outputSchema` where possible for structured data\n- Use `structuredContent` in tool responses (TypeScript SDK feature)\n- Helps clients understand and process tool outputs\n\n**Tool Description:**\n- Concise summary of functionality\n- Parameter descriptions\n- Return type schema\n\n**Implementation:**\n- Async/await for I/O operations\n- Proper error handling with actionable messages\n- Support pagination where applicable\n- Return both text content and structured data when using modern SDKs\n\n**Annotations:**\n- `readOnlyHint`: true/false\n- `destructiveHint`: true/false\n- `idempotentHint`: true/false\n- `openWorldHint`: true/false\n\n---\n\n### Phase 3: Review and Test\n\n#### 3.1 Code Quality\n\nReview for:\n- No duplicated code (DRY principle)\n- Consistent error handling\n- Full type coverage\n- Clear tool descriptions\n\n#### 3.2 Build and Test\n\n**TypeScript:**\n- Run `npm run build` to verify compilation\n- Test with MCP Inspector: `npx @modelcontextprotocol/inspector`\n\n**Python:**\n- Verify syntax: `python -m py_compile your_server.py`\n- Test with MCP Inspector\n\nSee language-specific guides for detailed testing approaches and quality checklists.\n\n---\n\n### Phase 4: Create Evaluations\n\nAfter implementing your MCP server, create comprehensive evaluations to test its effectiveness.\n\n**Load [âœ… Evaluation Guide](./reference/evaluation.md) for complete evaluation guidelines.**\n\n#### 4.1 Understand Evaluation Purpose\n\nUse evaluations to test whether LLMs can effectively use your MCP server to answer realistic, complex questions.\n\n#### 4.2 Create 10 Evaluation Questions\n\nTo create effective evaluations, follow the process outlined in the evaluation guide:\n\n1. **Tool Inspection**: List available tools and understand their capabilities\n2. **Content Exploration**: Use READ-ONLY operations to explore available data\n3. **Question Generation**: Create 10 complex, realistic questions\n4. **Answer Verification**: Solve each question yourself to verify answers\n\n#### 4.3 Evaluation Requirements\n\nEnsure each question is:\n- **Independent**: Not dependent on other questions\n- **Read-only**: Only non-destructive operations required\n- **Complex**: Requiring multiple tool calls and deep exploration\n- **Realistic**: Based on real use cases humans would care about\n- **Verifiable**: Single, clear answer that can be verified by string comparison\n- **Stable**: Answer won't change over time\n\n#### 4.4 Output Format\n\nCreate an XML file with this structure:\n\n```xml\n<evaluation>\n  <qa_pair>\n    <question>Find discussions about AI model launches with animal codenames. One model needed a specific safety designation that uses the format ASL-X. What number X was being determined for the model named after a spotted wild cat?</question>\n    <answer>3</answer>\n  </qa_pair>\n<!-- More qa_pairs... -->\n</evaluation>\n```\n\n---\n\n# Reference Files\n\n## ðŸ“š Documentation Library\n\nLoad these resources as needed during development:\n\n### Core MCP Documentation (Load First)\n- **MCP Protocol**: Start with sitemap at `https://modelcontextprotocol.io/sitemap.xml`, then fetch specific pages with `.md` suffix\n- [ðŸ“‹ MCP Best Practices](./reference/mcp_best_practices.md) - Universal MCP guidelines including:\n  - Server and tool naming conventions\n  - Response format guidelines (JSON vs Markdown)\n  - Pagination best practices\n  - Transport selection (streamable HTTP vs stdio)\n  - Security and error handling standards\n\n### SDK Documentation (Load During Phase 1/2)\n- **Python SDK**: Fetch from `https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/README.md`\n- **TypeScript SDK**: Fetch from `https://raw.githubusercontent.com/modelcontextprotocol/typescript-sdk/main/README.md`\n\n### Language-Specific Implementation Guides (Load During Phase 2)\n- [ðŸ Python Implementation Guide](./reference/python_mcp_server.md) - Complete Python/FastMCP guide with:\n  - Server initialization patterns\n  - Pydantic model examples\n  - Tool registration with `@mcp.tool`\n  - Complete working examples\n  - Quality checklist\n\n- [âš¡ TypeScript Implementation Guide](./reference/node_mcp_server.md) - Complete TypeScript guide with:\n  - Project structure\n  - Zod schema patterns\n  - Tool registration with `server.registerTool`\n  - Complete working examples\n  - Quality checklist\n\n### Evaluation Guide (Load During Phase 4)\n- [âœ… Evaluation Guide](./reference/evaluation.md) - Complete evaluation creation guide with:\n  - Question creation guidelines\n  - Answer verification strategies\n  - XML format specifications\n  - Example questions and answers\n  - Running an evaluation with the provided scripts\n",
      "frontmatter": {
        "name": "mcp-builder",
        "description": "Guide for creating high-quality MCP (Model Context Protocol) servers that enable LLMs to interact with external services through well-designed tools. Use when building MCP servers to integrate external APIs or services, whether in Python (FastMCP) or Node/TypeScript (MCP SDK).",
        "license": "Complete terms in LICENSE.txt"
      },
      "content": "\n# MCP Server Development Guide\n\n## Overview\n\nCreate MCP (Model Context Protocol) servers that enable LLMs to interact with external services through well-designed tools. The quality of an MCP server is measured by how well it enables LLMs to accomplish real-world tasks.\n\n---\n\n# Process\n\n## ðŸš€ High-Level Workflow\n\nCreating a high-quality MCP server involves four main phases:\n\n### Phase 1: Deep Research and Planning\n\n#### 1.1 Understand Modern MCP Design\n\n**API Coverage vs. Workflow Tools:**\nBalance comprehensive API endpoint coverage with specialized workflow tools. Workflow tools can be more convenient for specific tasks, while comprehensive coverage gives agents flexibility to compose operations. Performance varies by clientâ€”some clients benefit from code execution that combines basic tools, while others work better with higher-level workflows. When uncertain, prioritize comprehensive API coverage.\n\n**Tool Naming and Discoverability:**\nClear, descriptive tool names help agents find the right tools quickly. Use consistent prefixes (e.g., `github_create_issue`, `github_list_repos`) and action-oriented naming.\n\n**Context Management:**\nAgents benefit from concise tool descriptions and the ability to filter/paginate results. Design tools that return focused, relevant data. Some clients support code execution which can help agents filter and process data efficiently.\n\n**Actionable Error Messages:**\nError messages should guide agents toward solutions with specific suggestions and next steps.\n\n#### 1.2 Study MCP Protocol Documentation\n\n**Navigate the MCP specification:**\n\nStart with the sitemap to find relevant pages: `https://modelcontextprotocol.io/sitemap.xml`\n\nThen fetch specific pages with `.md` suffix for markdown format (e.g., `https://modelcontextprotocol.io/specification/draft.md`).\n\nKey pages to review:\n- Specification overview and architecture\n- Transport mechanisms (streamable HTTP, stdio)\n- Tool, resource, and prompt definitions\n\n#### 1.3 Study Framework Documentation\n\n**Recommended stack:**\n- **Language**: TypeScript (high-quality SDK support and good compatibility in many execution environments e.g. MCPB. Plus AI models are good at generating TypeScript code, benefiting from its broad usage, static typing and good linting tools)\n- **Transport**: Streamable HTTP for remote servers, using stateless JSON (simpler to scale and maintain, as opposed to stateful sessions and streaming responses). stdio for local servers.\n\n**Load framework documentation:**\n\n- **MCP Best Practices**: [ðŸ“‹ View Best Practices](./reference/mcp_best_practices.md) - Core guidelines\n\n**For TypeScript (recommended):**\n- **TypeScript SDK**: Use WebFetch to load `https://raw.githubusercontent.com/modelcontextprotocol/typescript-sdk/main/README.md`\n- [âš¡ TypeScript Guide](./reference/node_mcp_server.md) - TypeScript patterns and examples\n\n**For Python:**\n- **Python SDK**: Use WebFetch to load `https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/README.md`\n- [ðŸ Python Guide](./reference/python_mcp_server.md) - Python patterns and examples\n\n#### 1.4 Plan Your Implementation\n\n**Understand the API:**\nReview the service's API documentation to identify key endpoints, authentication requirements, and data models. Use web search and WebFetch as needed.\n\n**Tool Selection:**\nPrioritize comprehensive API coverage. List endpoints to implement, starting with the most common operations.\n\n---\n\n### Phase 2: Implementation\n\n#### 2.1 Set Up Project Structure\n\nSee language-specific guides for project setup:\n- [âš¡ TypeScript Guide](./reference/node_mcp_server.md) - Project structure, package.json, tsconfig.json\n- [ðŸ Python Guide](./reference/python_mcp_server.md) - Module organization, dependencies\n\n#### 2.2 Implement Core Infrastructure\n\nCreate shared utilities:\n- API client with authentication\n- Error handling helpers\n- Response formatting (JSON/Markdown)\n- Pagination support\n\n#### 2.3 Implement Tools\n\nFor each tool:\n\n**Input Schema:**\n- Use Zod (TypeScript) or Pydantic (Python)\n- Include constraints and clear descriptions\n- Add examples in field descriptions\n\n**Output Schema:**\n- Define `outputSchema` where possible for structured data\n- Use `structuredContent` in tool responses (TypeScript SDK feature)\n- Helps clients understand and process tool outputs\n\n**Tool Description:**\n- Concise summary of functionality\n- Parameter descriptions\n- Return type schema\n\n**Implementation:**\n- Async/await for I/O operations\n- Proper error handling with actionable messages\n- Support pagination where applicable\n- Return both text content and structured data when using modern SDKs\n\n**Annotations:**\n- `readOnlyHint`: true/false\n- `destructiveHint`: true/false\n- `idempotentHint`: true/false\n- `openWorldHint`: true/false\n\n---\n\n### Phase 3: Review and Test\n\n#### 3.1 Code Quality\n\nReview for:\n- No duplicated code (DRY principle)\n- Consistent error handling\n- Full type coverage\n- Clear tool descriptions\n\n#### 3.2 Build and Test\n\n**TypeScript:**\n- Run `npm run build` to verify compilation\n- Test with MCP Inspector: `npx @modelcontextprotocol/inspector`\n\n**Python:**\n- Verify syntax: `python -m py_compile your_server.py`\n- Test with MCP Inspector\n\nSee language-specific guides for detailed testing approaches and quality checklists.\n\n---\n\n### Phase 4: Create Evaluations\n\nAfter implementing your MCP server, create comprehensive evaluations to test its effectiveness.\n\n**Load [âœ… Evaluation Guide](./reference/evaluation.md) for complete evaluation guidelines.**\n\n#### 4.1 Understand Evaluation Purpose\n\nUse evaluations to test whether LLMs can effectively use your MCP server to answer realistic, complex questions.\n\n#### 4.2 Create 10 Evaluation Questions\n\nTo create effective evaluations, follow the process outlined in the evaluation guide:\n\n1. **Tool Inspection**: List available tools and understand their capabilities\n2. **Content Exploration**: Use READ-ONLY operations to explore available data\n3. **Question Generation**: Create 10 complex, realistic questions\n4. **Answer Verification**: Solve each question yourself to verify answers\n\n#### 4.3 Evaluation Requirements\n\nEnsure each question is:\n- **Independent**: Not dependent on other questions\n- **Read-only**: Only non-destructive operations required\n- **Complex**: Requiring multiple tool calls and deep exploration\n- **Realistic**: Based on real use cases humans would care about\n- **Verifiable**: Single, clear answer that can be verified by string comparison\n- **Stable**: Answer won't change over time\n\n#### 4.4 Output Format\n\nCreate an XML file with this structure:\n\n```xml\n<evaluation>\n  <qa_pair>\n    <question>Find discussions about AI model launches with animal codenames. One model needed a specific safety designation that uses the format ASL-X. What number X was being determined for the model named after a spotted wild cat?</question>\n    <answer>3</answer>\n  </qa_pair>\n<!-- More qa_pairs... -->\n</evaluation>\n```\n\n---\n\n# Reference Files\n\n## ðŸ“š Documentation Library\n\nLoad these resources as needed during development:\n\n### Core MCP Documentation (Load First)\n- **MCP Protocol**: Start with sitemap at `https://modelcontextprotocol.io/sitemap.xml`, then fetch specific pages with `.md` suffix\n- [ðŸ“‹ MCP Best Practices](./reference/mcp_best_practices.md) - Universal MCP guidelines including:\n  - Server and tool naming conventions\n  - Response format guidelines (JSON vs Markdown)\n  - Pagination best practices\n  - Transport selection (streamable HTTP vs stdio)\n  - Security and error handling standards\n\n### SDK Documentation (Load During Phase 1/2)\n- **Python SDK**: Fetch from `https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/README.md`\n- **TypeScript SDK**: Fetch from `https://raw.githubusercontent.com/modelcontextprotocol/typescript-sdk/main/README.md`\n\n### Language-Specific Implementation Guides (Load During Phase 2)\n- [ðŸ Python Implementation Guide](./reference/python_mcp_server.md) - Complete Python/FastMCP guide with:\n  - Server initialization patterns\n  - Pydantic model examples\n  - Tool registration with `@mcp.tool`\n  - Complete working examples\n  - Quality checklist\n\n- [âš¡ TypeScript Implementation Guide](./reference/node_mcp_server.md) - Complete TypeScript guide with:\n  - Project structure\n  - Zod schema patterns\n  - Tool registration with `server.registerTool`\n  - Complete working examples\n  - Quality checklist\n\n### Evaluation Guide (Load During Phase 4)\n- [âœ… Evaluation Guide](./reference/evaluation.md) - Complete evaluation creation guide with:\n  - Question creation guidelines\n  - Answer verification strategies\n  - XML format specifications\n  - Example questions and answers\n  - Running an evaluation with the provided scripts\n"
    }
  },
  "anthropics-skills-pdf": {
    "id": "anthropics-skills-pdf",
    "name": "pdf",
    "description": "Comprehensive PDF manipulation toolkit for extracting text and tables, creating new PDFs, merging/splitting documents, and handling forms. When Claude needs to fill in a PDF form or programmatically process, generate, or analyze PDF documents at scale.",
    "repo": {
      "owner": "anthropics",
      "name": "skills",
      "fullName": "anthropics/skills",
      "url": "https://github.com/anthropics/skills/tree/main/skills/pdf",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 35374,
      "forks": 3202,
      "language": "TypeScript",
      "topics": [],
      "updatedAt": "2026-01-08T06:00:29Z",
      "pushedAt": "2025-12-20T18:09:45Z"
    },
    "category": "official",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: pdf\ndescription: Comprehensive PDF manipulation toolkit for extracting text and tables, creating new PDFs, merging/splitting documents, and handling forms. When Claude needs to fill in a PDF form or programmatically process, generate, or analyze PDF documents at scale.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# PDF Processing Guide\n\n## Overview\n\nThis guide covers essential PDF processing operations using Python libraries and command-line tools. For advanced features, JavaScript libraries, and detailed examples, see reference.md. If you need to fill out a PDF form, read forms.md and follow its instructions.\n\n## Quick Start\n\n```python\nfrom pypdf import PdfReader, PdfWriter\n\n# Read a PDF\nreader = PdfReader(\"document.pdf\")\nprint(f\"Pages: {len(reader.pages)}\")\n\n# Extract text\ntext = \"\"\nfor page in reader.pages:\n    text += page.extract_text()\n```\n\n## Python Libraries\n\n### pypdf - Basic Operations\n\n#### Merge PDFs\n```python\nfrom pypdf import PdfWriter, PdfReader\n\nwriter = PdfWriter()\nfor pdf_file in [\"doc1.pdf\", \"doc2.pdf\", \"doc3.pdf\"]:\n    reader = PdfReader(pdf_file)\n    for page in reader.pages:\n        writer.add_page(page)\n\nwith open(\"merged.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n#### Split PDF\n```python\nreader = PdfReader(\"input.pdf\")\nfor i, page in enumerate(reader.pages):\n    writer = PdfWriter()\n    writer.add_page(page)\n    with open(f\"page_{i+1}.pdf\", \"wb\") as output:\n        writer.write(output)\n```\n\n#### Extract Metadata\n```python\nreader = PdfReader(\"document.pdf\")\nmeta = reader.metadata\nprint(f\"Title: {meta.title}\")\nprint(f\"Author: {meta.author}\")\nprint(f\"Subject: {meta.subject}\")\nprint(f\"Creator: {meta.creator}\")\n```\n\n#### Rotate Pages\n```python\nreader = PdfReader(\"input.pdf\")\nwriter = PdfWriter()\n\npage = reader.pages[0]\npage.rotate(90)  # Rotate 90 degrees clockwise\nwriter.add_page(page)\n\nwith open(\"rotated.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n### pdfplumber - Text and Table Extraction\n\n#### Extract Text with Layout\n```python\nimport pdfplumber\n\nwith pdfplumber.open(\"document.pdf\") as pdf:\n    for page in pdf.pages:\n        text = page.extract_text()\n        print(text)\n```\n\n#### Extract Tables\n```python\nwith pdfplumber.open(\"document.pdf\") as pdf:\n    for i, page in enumerate(pdf.pages):\n        tables = page.extract_tables()\n        for j, table in enumerate(tables):\n            print(f\"Table {j+1} on page {i+1}:\")\n            for row in table:\n                print(row)\n```\n\n#### Advanced Table Extraction\n```python\nimport pandas as pd\n\nwith pdfplumber.open(\"document.pdf\") as pdf:\n    all_tables = []\n    for page in pdf.pages:\n        tables = page.extract_tables()\n        for table in tables:\n            if table:  # Check if table is not empty\n                df = pd.DataFrame(table[1:], columns=table[0])\n                all_tables.append(df)\n\n# Combine all tables\nif all_tables:\n    combined_df = pd.concat(all_tables, ignore_index=True)\n    combined_df.to_excel(\"extracted_tables.xlsx\", index=False)\n```\n\n### reportlab - Create PDFs\n\n#### Basic PDF Creation\n```python\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.pdfgen import canvas\n\nc = canvas.Canvas(\"hello.pdf\", pagesize=letter)\nwidth, height = letter\n\n# Add text\nc.drawString(100, height - 100, \"Hello World!\")\nc.drawString(100, height - 120, \"This is a PDF created with reportlab\")\n\n# Add a line\nc.line(100, height - 140, 400, height - 140)\n\n# Save\nc.save()\n```\n\n#### Create PDF with Multiple Pages\n```python\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, PageBreak\nfrom reportlab.lib.styles import getSampleStyleSheet\n\ndoc = SimpleDocTemplate(\"report.pdf\", pagesize=letter)\nstyles = getSampleStyleSheet()\nstory = []\n\n# Add content\ntitle = Paragraph(\"Report Title\", styles['Title'])\nstory.append(title)\nstory.append(Spacer(1, 12))\n\nbody = Paragraph(\"This is the body of the report. \" * 20, styles['Normal'])\nstory.append(body)\nstory.append(PageBreak())\n\n# Page 2\nstory.append(Paragraph(\"Page 2\", styles['Heading1']))\nstory.append(Paragraph(\"Content for page 2\", styles['Normal']))\n\n# Build PDF\ndoc.build(story)\n```\n\n## Command-Line Tools\n\n### pdftotext (poppler-utils)\n```bash\n# Extract text\npdftotext input.pdf output.txt\n\n# Extract text preserving layout\npdftotext -layout input.pdf output.txt\n\n# Extract specific pages\npdftotext -f 1 -l 5 input.pdf output.txt  # Pages 1-5\n```\n\n### qpdf\n```bash\n# Merge PDFs\nqpdf --empty --pages file1.pdf file2.pdf -- merged.pdf\n\n# Split pages\nqpdf input.pdf --pages . 1-5 -- pages1-5.pdf\nqpdf input.pdf --pages . 6-10 -- pages6-10.pdf\n\n# Rotate pages\nqpdf input.pdf output.pdf --rotate=+90:1  # Rotate page 1 by 90 degrees\n\n# Remove password\nqpdf --password=mypassword --decrypt encrypted.pdf decrypted.pdf\n```\n\n### pdftk (if available)\n```bash\n# Merge\npdftk file1.pdf file2.pdf cat output merged.pdf\n\n# Split\npdftk input.pdf burst\n\n# Rotate\npdftk input.pdf rotate 1east output rotated.pdf\n```\n\n## Common Tasks\n\n### Extract Text from Scanned PDFs\n```python\n# Requires: pip install pytesseract pdf2image\nimport pytesseract\nfrom pdf2image import convert_from_path\n\n# Convert PDF to images\nimages = convert_from_path('scanned.pdf')\n\n# OCR each page\ntext = \"\"\nfor i, image in enumerate(images):\n    text += f\"Page {i+1}:\\n\"\n    text += pytesseract.image_to_string(image)\n    text += \"\\n\\n\"\n\nprint(text)\n```\n\n### Add Watermark\n```python\nfrom pypdf import PdfReader, PdfWriter\n\n# Create watermark (or load existing)\nwatermark = PdfReader(\"watermark.pdf\").pages[0]\n\n# Apply to all pages\nreader = PdfReader(\"document.pdf\")\nwriter = PdfWriter()\n\nfor page in reader.pages:\n    page.merge_page(watermark)\n    writer.add_page(page)\n\nwith open(\"watermarked.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n### Extract Images\n```bash\n# Using pdfimages (poppler-utils)\npdfimages -j input.pdf output_prefix\n\n# This extracts all images as output_prefix-000.jpg, output_prefix-001.jpg, etc.\n```\n\n### Password Protection\n```python\nfrom pypdf import PdfReader, PdfWriter\n\nreader = PdfReader(\"input.pdf\")\nwriter = PdfWriter()\n\nfor page in reader.pages:\n    writer.add_page(page)\n\n# Add password\nwriter.encrypt(\"userpassword\", \"ownerpassword\")\n\nwith open(\"encrypted.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n## Quick Reference\n\n| Task | Best Tool | Command/Code |\n|------|-----------|--------------|\n| Merge PDFs | pypdf | `writer.add_page(page)` |\n| Split PDFs | pypdf | One page per file |\n| Extract text | pdfplumber | `page.extract_text()` |\n| Extract tables | pdfplumber | `page.extract_tables()` |\n| Create PDFs | reportlab | Canvas or Platypus |\n| Command line merge | qpdf | `qpdf --empty --pages ...` |\n| OCR scanned PDFs | pytesseract | Convert to image first |\n| Fill PDF forms | pdf-lib or pypdf (see forms.md) | See forms.md |\n\n## Next Steps\n\n- For advanced pypdfium2 usage, see reference.md\n- For JavaScript libraries (pdf-lib), see reference.md\n- If you need to fill out a PDF form, follow the instructions in forms.md\n- For troubleshooting guides, see reference.md\n",
      "frontmatter": {
        "name": "pdf",
        "description": "Comprehensive PDF manipulation toolkit for extracting text and tables, creating new PDFs, merging/splitting documents, and handling forms. When Claude needs to fill in a PDF form or programmatically process, generate, or analyze PDF documents at scale.",
        "license": "Proprietary. LICENSE.txt has complete terms"
      },
      "content": "\n# PDF Processing Guide\n\n## Overview\n\nThis guide covers essential PDF processing operations using Python libraries and command-line tools. For advanced features, JavaScript libraries, and detailed examples, see reference.md. If you need to fill out a PDF form, read forms.md and follow its instructions.\n\n## Quick Start\n\n```python\nfrom pypdf import PdfReader, PdfWriter\n\n# Read a PDF\nreader = PdfReader(\"document.pdf\")\nprint(f\"Pages: {len(reader.pages)}\")\n\n# Extract text\ntext = \"\"\nfor page in reader.pages:\n    text += page.extract_text()\n```\n\n## Python Libraries\n\n### pypdf - Basic Operations\n\n#### Merge PDFs\n```python\nfrom pypdf import PdfWriter, PdfReader\n\nwriter = PdfWriter()\nfor pdf_file in [\"doc1.pdf\", \"doc2.pdf\", \"doc3.pdf\"]:\n    reader = PdfReader(pdf_file)\n    for page in reader.pages:\n        writer.add_page(page)\n\nwith open(\"merged.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n#### Split PDF\n```python\nreader = PdfReader(\"input.pdf\")\nfor i, page in enumerate(reader.pages):\n    writer = PdfWriter()\n    writer.add_page(page)\n    with open(f\"page_{i+1}.pdf\", \"wb\") as output:\n        writer.write(output)\n```\n\n#### Extract Metadata\n```python\nreader = PdfReader(\"document.pdf\")\nmeta = reader.metadata\nprint(f\"Title: {meta.title}\")\nprint(f\"Author: {meta.author}\")\nprint(f\"Subject: {meta.subject}\")\nprint(f\"Creator: {meta.creator}\")\n```\n\n#### Rotate Pages\n```python\nreader = PdfReader(\"input.pdf\")\nwriter = PdfWriter()\n\npage = reader.pages[0]\npage.rotate(90)  # Rotate 90 degrees clockwise\nwriter.add_page(page)\n\nwith open(\"rotated.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n### pdfplumber - Text and Table Extraction\n\n#### Extract Text with Layout\n```python\nimport pdfplumber\n\nwith pdfplumber.open(\"document.pdf\") as pdf:\n    for page in pdf.pages:\n        text = page.extract_text()\n        print(text)\n```\n\n#### Extract Tables\n```python\nwith pdfplumber.open(\"document.pdf\") as pdf:\n    for i, page in enumerate(pdf.pages):\n        tables = page.extract_tables()\n        for j, table in enumerate(tables):\n            print(f\"Table {j+1} on page {i+1}:\")\n            for row in table:\n                print(row)\n```\n\n#### Advanced Table Extraction\n```python\nimport pandas as pd\n\nwith pdfplumber.open(\"document.pdf\") as pdf:\n    all_tables = []\n    for page in pdf.pages:\n        tables = page.extract_tables()\n        for table in tables:\n            if table:  # Check if table is not empty\n                df = pd.DataFrame(table[1:], columns=table[0])\n                all_tables.append(df)\n\n# Combine all tables\nif all_tables:\n    combined_df = pd.concat(all_tables, ignore_index=True)\n    combined_df.to_excel(\"extracted_tables.xlsx\", index=False)\n```\n\n### reportlab - Create PDFs\n\n#### Basic PDF Creation\n```python\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.pdfgen import canvas\n\nc = canvas.Canvas(\"hello.pdf\", pagesize=letter)\nwidth, height = letter\n\n# Add text\nc.drawString(100, height - 100, \"Hello World!\")\nc.drawString(100, height - 120, \"This is a PDF created with reportlab\")\n\n# Add a line\nc.line(100, height - 140, 400, height - 140)\n\n# Save\nc.save()\n```\n\n#### Create PDF with Multiple Pages\n```python\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, PageBreak\nfrom reportlab.lib.styles import getSampleStyleSheet\n\ndoc = SimpleDocTemplate(\"report.pdf\", pagesize=letter)\nstyles = getSampleStyleSheet()\nstory = []\n\n# Add content\ntitle = Paragraph(\"Report Title\", styles['Title'])\nstory.append(title)\nstory.append(Spacer(1, 12))\n\nbody = Paragraph(\"This is the body of the report. \" * 20, styles['Normal'])\nstory.append(body)\nstory.append(PageBreak())\n\n# Page 2\nstory.append(Paragraph(\"Page 2\", styles['Heading1']))\nstory.append(Paragraph(\"Content for page 2\", styles['Normal']))\n\n# Build PDF\ndoc.build(story)\n```\n\n## Command-Line Tools\n\n### pdftotext (poppler-utils)\n```bash\n# Extract text\npdftotext input.pdf output.txt\n\n# Extract text preserving layout\npdftotext -layout input.pdf output.txt\n\n# Extract specific pages\npdftotext -f 1 -l 5 input.pdf output.txt  # Pages 1-5\n```\n\n### qpdf\n```bash\n# Merge PDFs\nqpdf --empty --pages file1.pdf file2.pdf -- merged.pdf\n\n# Split pages\nqpdf input.pdf --pages . 1-5 -- pages1-5.pdf\nqpdf input.pdf --pages . 6-10 -- pages6-10.pdf\n\n# Rotate pages\nqpdf input.pdf output.pdf --rotate=+90:1  # Rotate page 1 by 90 degrees\n\n# Remove password\nqpdf --password=mypassword --decrypt encrypted.pdf decrypted.pdf\n```\n\n### pdftk (if available)\n```bash\n# Merge\npdftk file1.pdf file2.pdf cat output merged.pdf\n\n# Split\npdftk input.pdf burst\n\n# Rotate\npdftk input.pdf rotate 1east output rotated.pdf\n```\n\n## Common Tasks\n\n### Extract Text from Scanned PDFs\n```python\n# Requires: pip install pytesseract pdf2image\nimport pytesseract\nfrom pdf2image import convert_from_path\n\n# Convert PDF to images\nimages = convert_from_path('scanned.pdf')\n\n# OCR each page\ntext = \"\"\nfor i, image in enumerate(images):\n    text += f\"Page {i+1}:\\n\"\n    text += pytesseract.image_to_string(image)\n    text += \"\\n\\n\"\n\nprint(text)\n```\n\n### Add Watermark\n```python\nfrom pypdf import PdfReader, PdfWriter\n\n# Create watermark (or load existing)\nwatermark = PdfReader(\"watermark.pdf\").pages[0]\n\n# Apply to all pages\nreader = PdfReader(\"document.pdf\")\nwriter = PdfWriter()\n\nfor page in reader.pages:\n    page.merge_page(watermark)\n    writer.add_page(page)\n\nwith open(\"watermarked.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n### Extract Images\n```bash\n# Using pdfimages (poppler-utils)\npdfimages -j input.pdf output_prefix\n\n# This extracts all images as output_prefix-000.jpg, output_prefix-001.jpg, etc.\n```\n\n### Password Protection\n```python\nfrom pypdf import PdfReader, PdfWriter\n\nreader = PdfReader(\"input.pdf\")\nwriter = PdfWriter()\n\nfor page in reader.pages:\n    writer.add_page(page)\n\n# Add password\nwriter.encrypt(\"userpassword\", \"ownerpassword\")\n\nwith open(\"encrypted.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n## Quick Reference\n\n| Task | Best Tool | Command/Code |\n|------|-----------|--------------|\n| Merge PDFs | pypdf | `writer.add_page(page)` |\n| Split PDFs | pypdf | One page per file |\n| Extract text | pdfplumber | `page.extract_text()` |\n| Extract tables | pdfplumber | `page.extract_tables()` |\n| Create PDFs | reportlab | Canvas or Platypus |\n| Command line merge | qpdf | `qpdf --empty --pages ...` |\n| OCR scanned PDFs | pytesseract | Convert to image first |\n| Fill PDF forms | pdf-lib or pypdf (see forms.md) | See forms.md |\n\n## Next Steps\n\n- For advanced pypdfium2 usage, see reference.md\n- For JavaScript libraries (pdf-lib), see reference.md\n- If you need to fill out a PDF form, follow the instructions in forms.md\n- For troubleshooting guides, see reference.md\n"
    }
  },
  "anthropics-skills-pptx": {
    "id": "anthropics-skills-pptx",
    "name": "pptx",
    "description": "Presentation creation, editing, and analysis. When Claude needs to work with presentations (.pptx files) for: (1) Creating new presentations, (2) Modifying or editing content, (3) Working with layouts, (4) Adding comments or speaker notes, or any other presentation tasks",
    "repo": {
      "owner": "anthropics",
      "name": "skills",
      "fullName": "anthropics/skills",
      "url": "https://github.com/anthropics/skills/tree/main/skills/pptx",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 35374,
      "forks": 3202,
      "language": "TypeScript",
      "topics": [],
      "updatedAt": "2026-01-08T06:00:29Z",
      "pushedAt": "2025-12-20T18:09:45Z"
    },
    "category": "official",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: pptx\ndescription: \"Presentation creation, editing, and analysis. When Claude needs to work with presentations (.pptx files) for: (1) Creating new presentations, (2) Modifying or editing content, (3) Working with layouts, (4) Adding comments or speaker notes, or any other presentation tasks\"\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# PPTX creation, editing, and analysis\n\n## Overview\n\nA user may ask you to create, edit, or analyze the contents of a .pptx file. A .pptx file is essentially a ZIP archive containing XML files and other resources that you can read or edit. You have different tools and workflows available for different tasks.\n\n## Reading and analyzing content\n\n### Text extraction\nIf you just need to read the text contents of a presentation, you should convert the document to markdown:\n\n```bash\n# Convert document to markdown\npython -m markitdown path-to-file.pptx\n```\n\n### Raw XML access\nYou need raw XML access for: comments, speaker notes, slide layouts, animations, design elements, and complex formatting. For any of these features, you'll need to unpack a presentation and read its raw XML contents.\n\n#### Unpacking a file\n`python ooxml/scripts/unpack.py <office_file> <output_dir>`\n\n**Note**: The unpack.py script is located at `skills/pptx/ooxml/scripts/unpack.py` relative to the project root. If the script doesn't exist at this path, use `find . -name \"unpack.py\"` to locate it.\n\n#### Key file structures\n* `ppt/presentation.xml` - Main presentation metadata and slide references\n* `ppt/slides/slide{N}.xml` - Individual slide contents (slide1.xml, slide2.xml, etc.)\n* `ppt/notesSlides/notesSlide{N}.xml` - Speaker notes for each slide\n* `ppt/comments/modernComment_*.xml` - Comments for specific slides\n* `ppt/slideLayouts/` - Layout templates for slides\n* `ppt/slideMasters/` - Master slide templates\n* `ppt/theme/` - Theme and styling information\n* `ppt/media/` - Images and other media files\n\n#### Typography and color extraction\n**When given an example design to emulate**: Always analyze the presentation's typography and colors first using the methods below:\n1. **Read theme file**: Check `ppt/theme/theme1.xml` for colors (`<a:clrScheme>`) and fonts (`<a:fontScheme>`)\n2. **Sample slide content**: Examine `ppt/slides/slide1.xml` for actual font usage (`<a:rPr>`) and colors\n3. **Search for patterns**: Use grep to find color (`<a:solidFill>`, `<a:srgbClr>`) and font references across all XML files\n\n## Creating a new PowerPoint presentation **without a template**\n\nWhen creating a new PowerPoint presentation from scratch, use the **html2pptx** workflow to convert HTML slides to PowerPoint with accurate positioning.\n\n### Design Principles\n\n**CRITICAL**: Before creating any presentation, analyze the content and choose appropriate design elements:\n1. **Consider the subject matter**: What is this presentation about? What tone, industry, or mood does it suggest?\n2. **Check for branding**: If the user mentions a company/organization, consider their brand colors and identity\n3. **Match palette to content**: Select colors that reflect the subject\n4. **State your approach**: Explain your design choices before writing code\n\n**Requirements**:\n- âœ… State your content-informed design approach BEFORE writing code\n- âœ… Use web-safe fonts only: Arial, Helvetica, Times New Roman, Georgia, Courier New, Verdana, Tahoma, Trebuchet MS, Impact\n- âœ… Create clear visual hierarchy through size, weight, and color\n- âœ… Ensure readability: strong contrast, appropriately sized text, clean alignment\n- âœ… Be consistent: repeat patterns, spacing, and visual language across slides\n\n#### Color Palette Selection\n\n**Choosing colors creatively**:\n- **Think beyond defaults**: What colors genuinely match this specific topic? Avoid autopilot choices.\n- **Consider multiple angles**: Topic, industry, mood, energy level, target audience, brand identity (if mentioned)\n- **Be adventurous**: Try unexpected combinations - a healthcare presentation doesn't have to be green, finance doesn't have to be navy\n- **Build your palette**: Pick 3-5 colors that work together (dominant colors + supporting tones + accent)\n- **Ensure contrast**: Text must be clearly readable on backgrounds\n\n**Example color palettes** (use these to spark creativity - choose one, adapt it, or create your own):\n\n1. **Classic Blue**: Deep navy (#1C2833), slate gray (#2E4053), silver (#AAB7B8), off-white (#F4F6F6)\n2. **Teal & Coral**: Teal (#5EA8A7), deep teal (#277884), coral (#FE4447), white (#FFFFFF)\n3. **Bold Red**: Red (#C0392B), bright red (#E74C3C), orange (#F39C12), yellow (#F1C40F), green (#2ECC71)\n4. **Warm Blush**: Mauve (#A49393), blush (#EED6D3), rose (#E8B4B8), cream (#FAF7F2)\n5. **Burgundy Luxury**: Burgundy (#5D1D2E), crimson (#951233), rust (#C15937), gold (#997929)\n6. **Deep Purple & Emerald**: Purple (#B165FB), dark blue (#181B24), emerald (#40695B), white (#FFFFFF)\n7. **Cream & Forest Green**: Cream (#FFE1C7), forest green (#40695B), white (#FCFCFC)\n8. **Pink & Purple**: Pink (#F8275B), coral (#FF574A), rose (#FF737D), purple (#3D2F68)\n9. **Lime & Plum**: Lime (#C5DE82), plum (#7C3A5F), coral (#FD8C6E), blue-gray (#98ACB5)\n10. **Black & Gold**: Gold (#BF9A4A), black (#000000), cream (#F4F6F6)\n11. **Sage & Terracotta**: Sage (#87A96B), terracotta (#E07A5F), cream (#F4F1DE), charcoal (#2C2C2C)\n12. **Charcoal & Red**: Charcoal (#292929), red (#E33737), light gray (#CCCBCB)\n13. **Vibrant Orange**: Orange (#F96D00), light gray (#F2F2F2), charcoal (#222831)\n14. **Forest Green**: Black (#191A19), green (#4E9F3D), dark green (#1E5128), white (#FFFFFF)\n15. **Retro Rainbow**: Purple (#722880), pink (#D72D51), orange (#EB5C18), amber (#F08800), gold (#DEB600)\n16. **Vintage Earthy**: Mustard (#E3B448), sage (#CBD18F), forest green (#3A6B35), cream (#F4F1DE)\n17. **Coastal Rose**: Old rose (#AD7670), beaver (#B49886), eggshell (#F3ECDC), ash gray (#BFD5BE)\n18. **Orange & Turquoise**: Light orange (#FC993E), grayish turquoise (#667C6F), white (#FCFCFC)\n\n#### Visual Details Options\n\n**Geometric Patterns**:\n- Diagonal section dividers instead of horizontal\n- Asymmetric column widths (30/70, 40/60, 25/75)\n- Rotated text headers at 90Â° or 270Â°\n- Circular/hexagonal frames for images\n- Triangular accent shapes in corners\n- Overlapping shapes for depth\n\n**Border & Frame Treatments**:\n- Thick single-color borders (10-20pt) on one side only\n- Double-line borders with contrasting colors\n- Corner brackets instead of full frames\n- L-shaped borders (top+left or bottom+right)\n- Underline accents beneath headers (3-5pt thick)\n\n**Typography Treatments**:\n- Extreme size contrast (72pt headlines vs 11pt body)\n- All-caps headers with wide letter spacing\n- Numbered sections in oversized display type\n- Monospace (Courier New) for data/stats/technical content\n- Condensed fonts (Arial Narrow) for dense information\n- Outlined text for emphasis\n\n**Chart & Data Styling**:\n- Monochrome charts with single accent color for key data\n- Horizontal bar charts instead of vertical\n- Dot plots instead of bar charts\n- Minimal gridlines or none at all\n- Data labels directly on elements (no legends)\n- Oversized numbers for key metrics\n\n**Layout Innovations**:\n- Full-bleed images with text overlays\n- Sidebar column (20-30% width) for navigation/context\n- Modular grid systems (3Ã—3, 4Ã—4 blocks)\n- Z-pattern or F-pattern content flow\n- Floating text boxes over colored shapes\n- Magazine-style multi-column layouts\n\n**Background Treatments**:\n- Solid color blocks occupying 40-60% of slide\n- Gradient fills (vertical or diagonal only)\n- Split backgrounds (two colors, diagonal or vertical)\n- Edge-to-edge color bands\n- Negative space as a design element\n\n### Layout Tips\n**When creating slides with charts or tables:**\n- **Two-column layout (PREFERRED)**: Use a header spanning the full width, then two columns below - text/bullets in one column and the featured content in the other. This provides better balance and makes charts/tables more readable. Use flexbox with unequal column widths (e.g., 40%/60% split) to optimize space for each content type.\n- **Full-slide layout**: Let the featured content (chart/table) take up the entire slide for maximum impact and readability\n- **NEVER vertically stack**: Do not place charts/tables below text in a single column - this causes poor readability and layout issues\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`html2pptx.md`](html2pptx.md) completely from start to finish. **NEVER set any range limits when reading this file.** Read the full file content for detailed syntax, critical formatting rules, and best practices before proceeding with presentation creation.\n2. Create an HTML file for each slide with proper dimensions (e.g., 720pt Ã— 405pt for 16:9)\n   - Use `<p>`, `<h1>`-`<h6>`, `<ul>`, `<ol>` for all text content\n   - Use `class=\"placeholder\"` for areas where charts/tables will be added (render with gray background for visibility)\n   - **CRITICAL**: Rasterize gradients and icons as PNG images FIRST using Sharp, then reference in HTML\n   - **LAYOUT**: For slides with charts/tables/images, use either full-slide layout or two-column layout for better readability\n3. Create and run a JavaScript file using the [`html2pptx.js`](scripts/html2pptx.js) library to convert HTML slides to PowerPoint and save the presentation\n   - Use the `html2pptx()` function to process each HTML file\n   - Add charts and tables to placeholder areas using PptxGenJS API\n   - Save the presentation using `pptx.writeFile()`\n4. **Visual validation**: Generate thumbnails and inspect for layout issues\n   - Create thumbnail grid: `python scripts/thumbnail.py output.pptx workspace/thumbnails --cols 4`\n   - Read and carefully examine the thumbnail image for:\n     - **Text cutoff**: Text being cut off by header bars, shapes, or slide edges\n     - **Text overlap**: Text overlapping with other text or shapes\n     - **Positioning issues**: Content too close to slide boundaries or other elements\n     - **Contrast issues**: Insufficient contrast between text and backgrounds\n   - If issues found, adjust HTML margins/spacing/colors and regenerate the presentation\n   - Repeat until all slides are visually correct\n\n## Editing an existing PowerPoint presentation\n\nWhen edit slides in an existing PowerPoint presentation, you need to work with the raw Office Open XML (OOXML) format. This involves unpacking the .pptx file, editing the XML content, and repacking it.\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`ooxml.md`](ooxml.md) (~500 lines) completely from start to finish.  **NEVER set any range limits when reading this file.**  Read the full file content for detailed guidance on OOXML structure and editing workflows before any presentation editing.\n2. Unpack the presentation: `python ooxml/scripts/unpack.py <office_file> <output_dir>`\n3. Edit the XML files (primarily `ppt/slides/slide{N}.xml` and related files)\n4. **CRITICAL**: Validate immediately after each edit and fix any validation errors before proceeding: `python ooxml/scripts/validate.py <dir> --original <file>`\n5. Pack the final presentation: `python ooxml/scripts/pack.py <input_directory> <office_file>`\n\n## Creating a new PowerPoint presentation **using a template**\n\nWhen you need to create a presentation that follows an existing template's design, you'll need to duplicate and re-arrange template slides before then replacing placeholder context.\n\n### Workflow\n1. **Extract template text AND create visual thumbnail grid**:\n   * Extract text: `python -m markitdown template.pptx > template-content.md`\n   * Read `template-content.md`: Read the entire file to understand the contents of the template presentation. **NEVER set any range limits when reading this file.**\n   * Create thumbnail grids: `python scripts/thumbnail.py template.pptx`\n   * See [Creating Thumbnail Grids](#creating-thumbnail-grids) section for more details\n\n2. **Analyze template and save inventory to a file**:\n   * **Visual Analysis**: Review thumbnail grid(s) to understand slide layouts, design patterns, and visual structure\n   * Create and save a template inventory file at `template-inventory.md` containing:\n     ```markdown\n     # Template Inventory Analysis\n     **Total Slides: [count]**\n     **IMPORTANT: Slides are 0-indexed (first slide = 0, last slide = count-1)**\n\n     ## [Category Name]\n     - Slide 0: [Layout code if available] - Description/purpose\n     - Slide 1: [Layout code] - Description/purpose\n     - Slide 2: [Layout code] - Description/purpose\n     [... EVERY slide must be listed individually with its index ...]\n     ```\n   * **Using the thumbnail grid**: Reference the visual thumbnails to identify:\n     - Layout patterns (title slides, content layouts, section dividers)\n     - Image placeholder locations and counts\n     - Design consistency across slide groups\n     - Visual hierarchy and structure\n   * This inventory file is REQUIRED for selecting appropriate templates in the next step\n\n3. **Create presentation outline based on template inventory**:\n   * Review available templates from step 2.\n   * Choose an intro or title template for the first slide. This should be one of the first templates.\n   * Choose safe, text-based layouts for the other slides.\n   * **CRITICAL: Match layout structure to actual content**:\n     - Single-column layouts: Use for unified narrative or single topic\n     - Two-column layouts: Use ONLY when you have exactly 2 distinct items/concepts\n     - Three-column layouts: Use ONLY when you have exactly 3 distinct items/concepts\n     - Image + text layouts: Use ONLY when you have actual images to insert\n     - Quote layouts: Use ONLY for actual quotes from people (with attribution), never for emphasis\n     - Never use layouts with more placeholders than you have content\n     - If you have 2 items, don't force them into a 3-column layout\n     - If you have 4+ items, consider breaking into multiple slides or using a list format\n   * Count your actual content pieces BEFORE selecting the layout\n   * Verify each placeholder in the chosen layout will be filled with meaningful content\n   * Select one option representing the **best** layout for each content section.\n   * Save `outline.md` with content AND template mapping that leverages available designs\n   * Example template mapping:\n      ```\n      # Template slides to use (0-based indexing)\n      # WARNING: Verify indices are within range! Template with 73 slides has indices 0-72\n      # Mapping: slide numbers from outline -> template slide indices\n      template_mapping = [\n          0,   # Use slide 0 (Title/Cover)\n          34,  # Use slide 34 (B1: Title and body)\n          34,  # Use slide 34 again (duplicate for second B1)\n          50,  # Use slide 50 (E1: Quote)\n          54,  # Use slide 54 (F2: Closing + Text)\n      ]\n      ```\n\n4. **Duplicate, reorder, and delete slides using `rearrange.py`**:\n   * Use the `scripts/rearrange.py` script to create a new presentation with slides in the desired order:\n     ```bash\n     python scripts/rearrange.py template.pptx working.pptx 0,34,34,50,52\n     ```\n   * The script handles duplicating repeated slides, deleting unused slides, and reordering automatically\n   * Slide indices are 0-based (first slide is 0, second is 1, etc.)\n   * The same slide index can appear multiple times to duplicate that slide\n\n5. **Extract ALL text using the `inventory.py` script**:\n   * **Run inventory extraction**:\n     ```bash\n     python scripts/inventory.py working.pptx text-inventory.json\n     ```\n   * **Read text-inventory.json**: Read the entire text-inventory.json file to understand all shapes and their properties. **NEVER set any range limits when reading this file.**\n\n   * The inventory JSON structure:\n      ```json\n        {\n          \"slide-0\": {\n            \"shape-0\": {\n              \"placeholder_type\": \"TITLE\",  // or null for non-placeholders\n              \"left\": 1.5,                  // position in inches\n              \"top\": 2.0,\n              \"width\": 7.5,\n              \"height\": 1.2,\n              \"paragraphs\": [\n                {\n                  \"text\": \"Paragraph text\",\n                  // Optional properties (only included when non-default):\n                  \"bullet\": true,           // explicit bullet detected\n                  \"level\": 0,               // only included when bullet is true\n                  \"alignment\": \"CENTER\",    // CENTER, RIGHT (not LEFT)\n                  \"space_before\": 10.0,     // space before paragraph in points\n                  \"space_after\": 6.0,       // space after paragraph in points\n                  \"line_spacing\": 22.4,     // line spacing in points\n                  \"font_name\": \"Arial\",     // from first run\n                  \"font_size\": 14.0,        // in points\n                  \"bold\": true,\n                  \"italic\": false,\n                  \"underline\": false,\n                  \"color\": \"FF0000\"         // RGB color\n                }\n              ]\n            }\n          }\n        }\n      ```\n\n   * Key features:\n     - **Slides**: Named as \"slide-0\", \"slide-1\", etc.\n     - **Shapes**: Ordered by visual position (top-to-bottom, left-to-right) as \"shape-0\", \"shape-1\", etc.\n     - **Placeholder types**: TITLE, CENTER_TITLE, SUBTITLE, BODY, OBJECT, or null\n     - **Default font size**: `default_font_size` in points extracted from layout placeholders (when available)\n     - **Slide numbers are filtered**: Shapes with SLIDE_NUMBER placeholder type are automatically excluded from inventory\n     - **Bullets**: When `bullet: true`, `level` is always included (even if 0)\n     - **Spacing**: `space_before`, `space_after`, and `line_spacing` in points (only included when set)\n     - **Colors**: `color` for RGB (e.g., \"FF0000\"), `theme_color` for theme colors (e.g., \"DARK_1\")\n     - **Properties**: Only non-default values are included in the output\n\n6. **Generate replacement text and save the data to a JSON file**\n   Based on the text inventory from the previous step:\n   - **CRITICAL**: First verify which shapes exist in the inventory - only reference shapes that are actually present\n   - **VALIDATION**: The replace.py script will validate that all shapes in your replacement JSON exist in the inventory\n     - If you reference a non-existent shape, you'll get an error showing available shapes\n     - If you reference a non-existent slide, you'll get an error indicating the slide doesn't exist\n     - All validation errors are shown at once before the script exits\n   - **IMPORTANT**: The replace.py script uses inventory.py internally to identify ALL text shapes\n   - **AUTOMATIC CLEARING**: ALL text shapes from the inventory will be cleared unless you provide \"paragraphs\" for them\n   - Add a \"paragraphs\" field to shapes that need content (not \"replacement_paragraphs\")\n   - Shapes without \"paragraphs\" in the replacement JSON will have their text cleared automatically\n   - Paragraphs with bullets will be automatically left aligned. Don't set the `alignment` property on when `\"bullet\": true`\n   - Generate appropriate replacement content for placeholder text\n   - Use shape size to determine appropriate content length\n   - **CRITICAL**: Include paragraph properties from the original inventory - don't just provide text\n   - **IMPORTANT**: When bullet: true, do NOT include bullet symbols (â€¢, -, *) in text - they're added automatically\n   - **ESSENTIAL FORMATTING RULES**:\n     - Headers/titles should typically have `\"bold\": true`\n     - List items should have `\"bullet\": true, \"level\": 0` (level is required when bullet is true)\n     - Preserve any alignment properties (e.g., `\"alignment\": \"CENTER\"` for centered text)\n     - Include font properties when different from default (e.g., `\"font_size\": 14.0`, `\"font_name\": \"Lora\"`)\n     - Colors: Use `\"color\": \"FF0000\"` for RGB or `\"theme_color\": \"DARK_1\"` for theme colors\n     - The replacement script expects **properly formatted paragraphs**, not just text strings\n     - **Overlapping shapes**: Prefer shapes with larger default_font_size or more appropriate placeholder_type\n   - Save the updated inventory with replacements to `replacement-text.json`\n   - **WARNING**: Different template layouts have different shape counts - always check the actual inventory before creating replacements\n\n   Example paragraphs field showing proper formatting:\n   ```json\n   \"paragraphs\": [\n     {\n       \"text\": \"New presentation title text\",\n       \"alignment\": \"CENTER\",\n       \"bold\": true\n     },\n     {\n       \"text\": \"Section Header\",\n       \"bold\": true\n     },\n     {\n       \"text\": \"First bullet point without bullet symbol\",\n       \"bullet\": true,\n       \"level\": 0\n     },\n     {\n       \"text\": \"Red colored text\",\n       \"color\": \"FF0000\"\n     },\n     {\n       \"text\": \"Theme colored text\",\n       \"theme_color\": \"DARK_1\"\n     },\n     {\n       \"text\": \"Regular paragraph text without special formatting\"\n     }\n   ]\n   ```\n\n   **Shapes not listed in the replacement JSON are automatically cleared**:\n   ```json\n   {\n     \"slide-0\": {\n       \"shape-0\": {\n         \"paragraphs\": [...] // This shape gets new text\n       }\n       // shape-1 and shape-2 from inventory will be cleared automatically\n     }\n   }\n   ```\n\n   **Common formatting patterns for presentations**:\n   - Title slides: Bold text, sometimes centered\n   - Section headers within slides: Bold text\n   - Bullet lists: Each item needs `\"bullet\": true, \"level\": 0`\n   - Body text: Usually no special properties needed\n   - Quotes: May have special alignment or font properties\n\n7. **Apply replacements using the `replace.py` script**\n   ```bash\n   python scripts/replace.py working.pptx replacement-text.json output.pptx\n   ```\n\n   The script will:\n   - First extract the inventory of ALL text shapes using functions from inventory.py\n   - Validate that all shapes in the replacement JSON exist in the inventory\n   - Clear text from ALL shapes identified in the inventory\n   - Apply new text only to shapes with \"paragraphs\" defined in the replacement JSON\n   - Preserve formatting by applying paragraph properties from the JSON\n   - Handle bullets, alignment, font properties, and colors automatically\n   - Save the updated presentation\n\n   Example validation errors:\n   ```\n   ERROR: Invalid shapes in replacement JSON:\n     - Shape 'shape-99' not found on 'slide-0'. Available shapes: shape-0, shape-1, shape-4\n     - Slide 'slide-999' not found in inventory\n   ```\n\n   ```\n   ERROR: Replacement text made overflow worse in these shapes:\n     - slide-0/shape-2: overflow worsened by 1.25\" (was 0.00\", now 1.25\")\n   ```\n\n## Creating Thumbnail Grids\n\nTo create visual thumbnail grids of PowerPoint slides for quick analysis and reference:\n\n```bash\npython scripts/thumbnail.py template.pptx [output_prefix]\n```\n\n**Features**:\n- Creates: `thumbnails.jpg` (or `thumbnails-1.jpg`, `thumbnails-2.jpg`, etc. for large decks)\n- Default: 5 columns, max 30 slides per grid (5Ã—6)\n- Custom prefix: `python scripts/thumbnail.py template.pptx my-grid`\n  - Note: The output prefix should include the path if you want output in a specific directory (e.g., `workspace/my-grid`)\n- Adjust columns: `--cols 4` (range: 3-6, affects slides per grid)\n- Grid limits: 3 cols = 12 slides/grid, 4 cols = 20, 5 cols = 30, 6 cols = 42\n- Slides are zero-indexed (Slide 0, Slide 1, etc.)\n\n**Use cases**:\n- Template analysis: Quickly understand slide layouts and design patterns\n- Content review: Visual overview of entire presentation\n- Navigation reference: Find specific slides by their visual appearance\n- Quality check: Verify all slides are properly formatted\n\n**Examples**:\n```bash\n# Basic usage\npython scripts/thumbnail.py presentation.pptx\n\n# Combine options: custom name, columns\npython scripts/thumbnail.py template.pptx analysis --cols 4\n```\n\n## Converting Slides to Images\n\nTo visually analyze PowerPoint slides, convert them to images using a two-step process:\n\n1. **Convert PPTX to PDF**:\n   ```bash\n   soffice --headless --convert-to pdf template.pptx\n   ```\n\n2. **Convert PDF pages to JPEG images**:\n   ```bash\n   pdftoppm -jpeg -r 150 template.pdf slide\n   ```\n   This creates files like `slide-1.jpg`, `slide-2.jpg`, etc.\n\nOptions:\n- `-r 150`: Sets resolution to 150 DPI (adjust for quality/size balance)\n- `-jpeg`: Output JPEG format (use `-png` for PNG if preferred)\n- `-f N`: First page to convert (e.g., `-f 2` starts from page 2)\n- `-l N`: Last page to convert (e.g., `-l 5` stops at page 5)\n- `slide`: Prefix for output files\n\nExample for specific range:\n```bash\npdftoppm -jpeg -r 150 -f 2 -l 5 template.pdf slide  # Converts only pages 2-5\n```\n\n## Code Style Guidelines\n**IMPORTANT**: When generating code for PPTX operations:\n- Write concise code\n- Avoid verbose variable names and redundant operations\n- Avoid unnecessary print statements\n\n## Dependencies\n\nRequired dependencies (should already be installed):\n\n- **markitdown**: `pip install \"markitdown[pptx]\"` (for text extraction from presentations)\n- **pptxgenjs**: `npm install -g pptxgenjs` (for creating presentations via html2pptx)\n- **playwright**: `npm install -g playwright` (for HTML rendering in html2pptx)\n- **react-icons**: `npm install -g react-icons react react-dom` (for icons)\n- **sharp**: `npm install -g sharp` (for SVG rasterization and image processing)\n- **LibreOffice**: `sudo apt-get install libreoffice` (for PDF conversion)\n- **Poppler**: `sudo apt-get install poppler-utils` (for pdftoppm to convert PDF to images)\n- **defusedxml**: `pip install defusedxml` (for secure XML parsing)",
      "frontmatter": {
        "name": "pptx",
        "description": "Presentation creation, editing, and analysis. When Claude needs to work with presentations (.pptx files) for: (1) Creating new presentations, (2) Modifying or editing content, (3) Working with layouts, (4) Adding comments or speaker notes, or any other presentation tasks",
        "license": "Proprietary. LICENSE.txt has complete terms"
      },
      "content": "\n# PPTX creation, editing, and analysis\n\n## Overview\n\nA user may ask you to create, edit, or analyze the contents of a .pptx file. A .pptx file is essentially a ZIP archive containing XML files and other resources that you can read or edit. You have different tools and workflows available for different tasks.\n\n## Reading and analyzing content\n\n### Text extraction\nIf you just need to read the text contents of a presentation, you should convert the document to markdown:\n\n```bash\n# Convert document to markdown\npython -m markitdown path-to-file.pptx\n```\n\n### Raw XML access\nYou need raw XML access for: comments, speaker notes, slide layouts, animations, design elements, and complex formatting. For any of these features, you'll need to unpack a presentation and read its raw XML contents.\n\n#### Unpacking a file\n`python ooxml/scripts/unpack.py <office_file> <output_dir>`\n\n**Note**: The unpack.py script is located at `skills/pptx/ooxml/scripts/unpack.py` relative to the project root. If the script doesn't exist at this path, use `find . -name \"unpack.py\"` to locate it.\n\n#### Key file structures\n* `ppt/presentation.xml` - Main presentation metadata and slide references\n* `ppt/slides/slide{N}.xml` - Individual slide contents (slide1.xml, slide2.xml, etc.)\n* `ppt/notesSlides/notesSlide{N}.xml` - Speaker notes for each slide\n* `ppt/comments/modernComment_*.xml` - Comments for specific slides\n* `ppt/slideLayouts/` - Layout templates for slides\n* `ppt/slideMasters/` - Master slide templates\n* `ppt/theme/` - Theme and styling information\n* `ppt/media/` - Images and other media files\n\n#### Typography and color extraction\n**When given an example design to emulate**: Always analyze the presentation's typography and colors first using the methods below:\n1. **Read theme file**: Check `ppt/theme/theme1.xml` for colors (`<a:clrScheme>`) and fonts (`<a:fontScheme>`)\n2. **Sample slide content**: Examine `ppt/slides/slide1.xml` for actual font usage (`<a:rPr>`) and colors\n3. **Search for patterns**: Use grep to find color (`<a:solidFill>`, `<a:srgbClr>`) and font references across all XML files\n\n## Creating a new PowerPoint presentation **without a template**\n\nWhen creating a new PowerPoint presentation from scratch, use the **html2pptx** workflow to convert HTML slides to PowerPoint with accurate positioning.\n\n### Design Principles\n\n**CRITICAL**: Before creating any presentation, analyze the content and choose appropriate design elements:\n1. **Consider the subject matter**: What is this presentation about? What tone, industry, or mood does it suggest?\n2. **Check for branding**: If the user mentions a company/organization, consider their brand colors and identity\n3. **Match palette to content**: Select colors that reflect the subject\n4. **State your approach**: Explain your design choices before writing code\n\n**Requirements**:\n- âœ… State your content-informed design approach BEFORE writing code\n- âœ… Use web-safe fonts only: Arial, Helvetica, Times New Roman, Georgia, Courier New, Verdana, Tahoma, Trebuchet MS, Impact\n- âœ… Create clear visual hierarchy through size, weight, and color\n- âœ… Ensure readability: strong contrast, appropriately sized text, clean alignment\n- âœ… Be consistent: repeat patterns, spacing, and visual language across slides\n\n#### Color Palette Selection\n\n**Choosing colors creatively**:\n- **Think beyond defaults**: What colors genuinely match this specific topic? Avoid autopilot choices.\n- **Consider multiple angles**: Topic, industry, mood, energy level, target audience, brand identity (if mentioned)\n- **Be adventurous**: Try unexpected combinations - a healthcare presentation doesn't have to be green, finance doesn't have to be navy\n- **Build your palette**: Pick 3-5 colors that work together (dominant colors + supporting tones + accent)\n- **Ensure contrast**: Text must be clearly readable on backgrounds\n\n**Example color palettes** (use these to spark creativity - choose one, adapt it, or create your own):\n\n1. **Classic Blue**: Deep navy (#1C2833), slate gray (#2E4053), silver (#AAB7B8), off-white (#F4F6F6)\n2. **Teal & Coral**: Teal (#5EA8A7), deep teal (#277884), coral (#FE4447), white (#FFFFFF)\n3. **Bold Red**: Red (#C0392B), bright red (#E74C3C), orange (#F39C12), yellow (#F1C40F), green (#2ECC71)\n4. **Warm Blush**: Mauve (#A49393), blush (#EED6D3), rose (#E8B4B8), cream (#FAF7F2)\n5. **Burgundy Luxury**: Burgundy (#5D1D2E), crimson (#951233), rust (#C15937), gold (#997929)\n6. **Deep Purple & Emerald**: Purple (#B165FB), dark blue (#181B24), emerald (#40695B), white (#FFFFFF)\n7. **Cream & Forest Green**: Cream (#FFE1C7), forest green (#40695B), white (#FCFCFC)\n8. **Pink & Purple**: Pink (#F8275B), coral (#FF574A), rose (#FF737D), purple (#3D2F68)\n9. **Lime & Plum**: Lime (#C5DE82), plum (#7C3A5F), coral (#FD8C6E), blue-gray (#98ACB5)\n10. **Black & Gold**: Gold (#BF9A4A), black (#000000), cream (#F4F6F6)\n11. **Sage & Terracotta**: Sage (#87A96B), terracotta (#E07A5F), cream (#F4F1DE), charcoal (#2C2C2C)\n12. **Charcoal & Red**: Charcoal (#292929), red (#E33737), light gray (#CCCBCB)\n13. **Vibrant Orange**: Orange (#F96D00), light gray (#F2F2F2), charcoal (#222831)\n14. **Forest Green**: Black (#191A19), green (#4E9F3D), dark green (#1E5128), white (#FFFFFF)\n15. **Retro Rainbow**: Purple (#722880), pink (#D72D51), orange (#EB5C18), amber (#F08800), gold (#DEB600)\n16. **Vintage Earthy**: Mustard (#E3B448), sage (#CBD18F), forest green (#3A6B35), cream (#F4F1DE)\n17. **Coastal Rose**: Old rose (#AD7670), beaver (#B49886), eggshell (#F3ECDC), ash gray (#BFD5BE)\n18. **Orange & Turquoise**: Light orange (#FC993E), grayish turquoise (#667C6F), white (#FCFCFC)\n\n#### Visual Details Options\n\n**Geometric Patterns**:\n- Diagonal section dividers instead of horizontal\n- Asymmetric column widths (30/70, 40/60, 25/75)\n- Rotated text headers at 90Â° or 270Â°\n- Circular/hexagonal frames for images\n- Triangular accent shapes in corners\n- Overlapping shapes for depth\n\n**Border & Frame Treatments**:\n- Thick single-color borders (10-20pt) on one side only\n- Double-line borders with contrasting colors\n- Corner brackets instead of full frames\n- L-shaped borders (top+left or bottom+right)\n- Underline accents beneath headers (3-5pt thick)\n\n**Typography Treatments**:\n- Extreme size contrast (72pt headlines vs 11pt body)\n- All-caps headers with wide letter spacing\n- Numbered sections in oversized display type\n- Monospace (Courier New) for data/stats/technical content\n- Condensed fonts (Arial Narrow) for dense information\n- Outlined text for emphasis\n\n**Chart & Data Styling**:\n- Monochrome charts with single accent color for key data\n- Horizontal bar charts instead of vertical\n- Dot plots instead of bar charts\n- Minimal gridlines or none at all\n- Data labels directly on elements (no legends)\n- Oversized numbers for key metrics\n\n**Layout Innovations**:\n- Full-bleed images with text overlays\n- Sidebar column (20-30% width) for navigation/context\n- Modular grid systems (3Ã—3, 4Ã—4 blocks)\n- Z-pattern or F-pattern content flow\n- Floating text boxes over colored shapes\n- Magazine-style multi-column layouts\n\n**Background Treatments**:\n- Solid color blocks occupying 40-60% of slide\n- Gradient fills (vertical or diagonal only)\n- Split backgrounds (two colors, diagonal or vertical)\n- Edge-to-edge color bands\n- Negative space as a design element\n\n### Layout Tips\n**When creating slides with charts or tables:**\n- **Two-column layout (PREFERRED)**: Use a header spanning the full width, then two columns below - text/bullets in one column and the featured content in the other. This provides better balance and makes charts/tables more readable. Use flexbox with unequal column widths (e.g., 40%/60% split) to optimize space for each content type.\n- **Full-slide layout**: Let the featured content (chart/table) take up the entire slide for maximum impact and readability\n- **NEVER vertically stack**: Do not place charts/tables below text in a single column - this causes poor readability and layout issues\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`html2pptx.md`](html2pptx.md) completely from start to finish. **NEVER set any range limits when reading this file.** Read the full file content for detailed syntax, critical formatting rules, and best practices before proceeding with presentation creation.\n2. Create an HTML file for each slide with proper dimensions (e.g., 720pt Ã— 405pt for 16:9)\n   - Use `<p>`, `<h1>`-`<h6>`, `<ul>`, `<ol>` for all text content\n   - Use `class=\"placeholder\"` for areas where charts/tables will be added (render with gray background for visibility)\n   - **CRITICAL**: Rasterize gradients and icons as PNG images FIRST using Sharp, then reference in HTML\n   - **LAYOUT**: For slides with charts/tables/images, use either full-slide layout or two-column layout for better readability\n3. Create and run a JavaScript file using the [`html2pptx.js`](scripts/html2pptx.js) library to convert HTML slides to PowerPoint and save the presentation\n   - Use the `html2pptx()` function to process each HTML file\n   - Add charts and tables to placeholder areas using PptxGenJS API\n   - Save the presentation using `pptx.writeFile()`\n4. **Visual validation**: Generate thumbnails and inspect for layout issues\n   - Create thumbnail grid: `python scripts/thumbnail.py output.pptx workspace/thumbnails --cols 4`\n   - Read and carefully examine the thumbnail image for:\n     - **Text cutoff**: Text being cut off by header bars, shapes, or slide edges\n     - **Text overlap**: Text overlapping with other text or shapes\n     - **Positioning issues**: Content too close to slide boundaries or other elements\n     - **Contrast issues**: Insufficient contrast between text and backgrounds\n   - If issues found, adjust HTML margins/spacing/colors and regenerate the presentation\n   - Repeat until all slides are visually correct\n\n## Editing an existing PowerPoint presentation\n\nWhen edit slides in an existing PowerPoint presentation, you need to work with the raw Office Open XML (OOXML) format. This involves unpacking the .pptx file, editing the XML content, and repacking it.\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`ooxml.md`](ooxml.md) (~500 lines) completely from start to finish.  **NEVER set any range limits when reading this file.**  Read the full file content for detailed guidance on OOXML structure and editing workflows before any presentation editing.\n2. Unpack the presentation: `python ooxml/scripts/unpack.py <office_file> <output_dir>`\n3. Edit the XML files (primarily `ppt/slides/slide{N}.xml` and related files)\n4. **CRITICAL**: Validate immediately after each edit and fix any validation errors before proceeding: `python ooxml/scripts/validate.py <dir> --original <file>`\n5. Pack the final presentation: `python ooxml/scripts/pack.py <input_directory> <office_file>`\n\n## Creating a new PowerPoint presentation **using a template**\n\nWhen you need to create a presentation that follows an existing template's design, you'll need to duplicate and re-arrange template slides before then replacing placeholder context.\n\n### Workflow\n1. **Extract template text AND create visual thumbnail grid**:\n   * Extract text: `python -m markitdown template.pptx > template-content.md`\n   * Read `template-content.md`: Read the entire file to understand the contents of the template presentation. **NEVER set any range limits when reading this file.**\n   * Create thumbnail grids: `python scripts/thumbnail.py template.pptx`\n   * See [Creating Thumbnail Grids](#creating-thumbnail-grids) section for more details\n\n2. **Analyze template and save inventory to a file**:\n   * **Visual Analysis**: Review thumbnail grid(s) to understand slide layouts, design patterns, and visual structure\n   * Create and save a template inventory file at `template-inventory.md` containing:\n     ```markdown\n     # Template Inventory Analysis\n     **Total Slides: [count]**\n     **IMPORTANT: Slides are 0-indexed (first slide = 0, last slide = count-1)**\n\n     ## [Category Name]\n     - Slide 0: [Layout code if available] - Description/purpose\n     - Slide 1: [Layout code] - Description/purpose\n     - Slide 2: [Layout code] - Description/purpose\n     [... EVERY slide must be listed individually with its index ...]\n     ```\n   * **Using the thumbnail grid**: Reference the visual thumbnails to identify:\n     - Layout patterns (title slides, content layouts, section dividers)\n     - Image placeholder locations and counts\n     - Design consistency across slide groups\n     - Visual hierarchy and structure\n   * This inventory file is REQUIRED for selecting appropriate templates in the next step\n\n3. **Create presentation outline based on template inventory**:\n   * Review available templates from step 2.\n   * Choose an intro or title template for the first slide. This should be one of the first templates.\n   * Choose safe, text-based layouts for the other slides.\n   * **CRITICAL: Match layout structure to actual content**:\n     - Single-column layouts: Use for unified narrative or single topic\n     - Two-column layouts: Use ONLY when you have exactly 2 distinct items/concepts\n     - Three-column layouts: Use ONLY when you have exactly 3 distinct items/concepts\n     - Image + text layouts: Use ONLY when you have actual images to insert\n     - Quote layouts: Use ONLY for actual quotes from people (with attribution), never for emphasis\n     - Never use layouts with more placeholders than you have content\n     - If you have 2 items, don't force them into a 3-column layout\n     - If you have 4+ items, consider breaking into multiple slides or using a list format\n   * Count your actual content pieces BEFORE selecting the layout\n   * Verify each placeholder in the chosen layout will be filled with meaningful content\n   * Select one option representing the **best** layout for each content section.\n   * Save `outline.md` with content AND template mapping that leverages available designs\n   * Example template mapping:\n      ```\n      # Template slides to use (0-based indexing)\n      # WARNING: Verify indices are within range! Template with 73 slides has indices 0-72\n      # Mapping: slide numbers from outline -> template slide indices\n      template_mapping = [\n          0,   # Use slide 0 (Title/Cover)\n          34,  # Use slide 34 (B1: Title and body)\n          34,  # Use slide 34 again (duplicate for second B1)\n          50,  # Use slide 50 (E1: Quote)\n          54,  # Use slide 54 (F2: Closing + Text)\n      ]\n      ```\n\n4. **Duplicate, reorder, and delete slides using `rearrange.py`**:\n   * Use the `scripts/rearrange.py` script to create a new presentation with slides in the desired order:\n     ```bash\n     python scripts/rearrange.py template.pptx working.pptx 0,34,34,50,52\n     ```\n   * The script handles duplicating repeated slides, deleting unused slides, and reordering automatically\n   * Slide indices are 0-based (first slide is 0, second is 1, etc.)\n   * The same slide index can appear multiple times to duplicate that slide\n\n5. **Extract ALL text using the `inventory.py` script**:\n   * **Run inventory extraction**:\n     ```bash\n     python scripts/inventory.py working.pptx text-inventory.json\n     ```\n   * **Read text-inventory.json**: Read the entire text-inventory.json file to understand all shapes and their properties. **NEVER set any range limits when reading this file.**\n\n   * The inventory JSON structure:\n      ```json\n        {\n          \"slide-0\": {\n            \"shape-0\": {\n              \"placeholder_type\": \"TITLE\",  // or null for non-placeholders\n              \"left\": 1.5,                  // position in inches\n              \"top\": 2.0,\n              \"width\": 7.5,\n              \"height\": 1.2,\n              \"paragraphs\": [\n                {\n                  \"text\": \"Paragraph text\",\n                  // Optional properties (only included when non-default):\n                  \"bullet\": true,           // explicit bullet detected\n                  \"level\": 0,               // only included when bullet is true\n                  \"alignment\": \"CENTER\",    // CENTER, RIGHT (not LEFT)\n                  \"space_before\": 10.0,     // space before paragraph in points\n                  \"space_after\": 6.0,       // space after paragraph in points\n                  \"line_spacing\": 22.4,     // line spacing in points\n                  \"font_name\": \"Arial\",     // from first run\n                  \"font_size\": 14.0,        // in points\n                  \"bold\": true,\n                  \"italic\": false,\n                  \"underline\": false,\n                  \"color\": \"FF0000\"         // RGB color\n                }\n              ]\n            }\n          }\n        }\n      ```\n\n   * Key features:\n     - **Slides**: Named as \"slide-0\", \"slide-1\", etc.\n     - **Shapes**: Ordered by visual position (top-to-bottom, left-to-right) as \"shape-0\", \"shape-1\", etc.\n     - **Placeholder types**: TITLE, CENTER_TITLE, SUBTITLE, BODY, OBJECT, or null\n     - **Default font size**: `default_font_size` in points extracted from layout placeholders (when available)\n     - **Slide numbers are filtered**: Shapes with SLIDE_NUMBER placeholder type are automatically excluded from inventory\n     - **Bullets**: When `bullet: true`, `level` is always included (even if 0)\n     - **Spacing**: `space_before`, `space_after`, and `line_spacing` in points (only included when set)\n     - **Colors**: `color` for RGB (e.g., \"FF0000\"), `theme_color` for theme colors (e.g., \"DARK_1\")\n     - **Properties**: Only non-default values are included in the output\n\n6. **Generate replacement text and save the data to a JSON file**\n   Based on the text inventory from the previous step:\n   - **CRITICAL**: First verify which shapes exist in the inventory - only reference shapes that are actually present\n   - **VALIDATION**: The replace.py script will validate that all shapes in your replacement JSON exist in the inventory\n     - If you reference a non-existent shape, you'll get an error showing available shapes\n     - If you reference a non-existent slide, you'll get an error indicating the slide doesn't exist\n     - All validation errors are shown at once before the script exits\n   - **IMPORTANT**: The replace.py script uses inventory.py internally to identify ALL text shapes\n   - **AUTOMATIC CLEARING**: ALL text shapes from the inventory will be cleared unless you provide \"paragraphs\" for them\n   - Add a \"paragraphs\" field to shapes that need content (not \"replacement_paragraphs\")\n   - Shapes without \"paragraphs\" in the replacement JSON will have their text cleared automatically\n   - Paragraphs with bullets will be automatically left aligned. Don't set the `alignment` property on when `\"bullet\": true`\n   - Generate appropriate replacement content for placeholder text\n   - Use shape size to determine appropriate content length\n   - **CRITICAL**: Include paragraph properties from the original inventory - don't just provide text\n   - **IMPORTANT**: When bullet: true, do NOT include bullet symbols (â€¢, -, *) in text - they're added automatically\n   - **ESSENTIAL FORMATTING RULES**:\n     - Headers/titles should typically have `\"bold\": true`\n     - List items should have `\"bullet\": true, \"level\": 0` (level is required when bullet is true)\n     - Preserve any alignment properties (e.g., `\"alignment\": \"CENTER\"` for centered text)\n     - Include font properties when different from default (e.g., `\"font_size\": 14.0`, `\"font_name\": \"Lora\"`)\n     - Colors: Use `\"color\": \"FF0000\"` for RGB or `\"theme_color\": \"DARK_1\"` for theme colors\n     - The replacement script expects **properly formatted paragraphs**, not just text strings\n     - **Overlapping shapes**: Prefer shapes with larger default_font_size or more appropriate placeholder_type\n   - Save the updated inventory with replacements to `replacement-text.json`\n   - **WARNING**: Different template layouts have different shape counts - always check the actual inventory before creating replacements\n\n   Example paragraphs field showing proper formatting:\n   ```json\n   \"paragraphs\": [\n     {\n       \"text\": \"New presentation title text\",\n       \"alignment\": \"CENTER\",\n       \"bold\": true\n     },\n     {\n       \"text\": \"Section Header\",\n       \"bold\": true\n     },\n     {\n       \"text\": \"First bullet point without bullet symbol\",\n       \"bullet\": true,\n       \"level\": 0\n     },\n     {\n       \"text\": \"Red colored text\",\n       \"color\": \"FF0000\"\n     },\n     {\n       \"text\": \"Theme colored text\",\n       \"theme_color\": \"DARK_1\"\n     },\n     {\n       \"text\": \"Regular paragraph text without special formatting\"\n     }\n   ]\n   ```\n\n   **Shapes not listed in the replacement JSON are automatically cleared**:\n   ```json\n   {\n     \"slide-0\": {\n       \"shape-0\": {\n         \"paragraphs\": [...] // This shape gets new text\n       }\n       // shape-1 and shape-2 from inventory will be cleared automatically\n     }\n   }\n   ```\n\n   **Common formatting patterns for presentations**:\n   - Title slides: Bold text, sometimes centered\n   - Section headers within slides: Bold text\n   - Bullet lists: Each item needs `\"bullet\": true, \"level\": 0`\n   - Body text: Usually no special properties needed\n   - Quotes: May have special alignment or font properties\n\n7. **Apply replacements using the `replace.py` script**\n   ```bash\n   python scripts/replace.py working.pptx replacement-text.json output.pptx\n   ```\n\n   The script will:\n   - First extract the inventory of ALL text shapes using functions from inventory.py\n   - Validate that all shapes in the replacement JSON exist in the inventory\n   - Clear text from ALL shapes identified in the inventory\n   - Apply new text only to shapes with \"paragraphs\" defined in the replacement JSON\n   - Preserve formatting by applying paragraph properties from the JSON\n   - Handle bullets, alignment, font properties, and colors automatically\n   - Save the updated presentation\n\n   Example validation errors:\n   ```\n   ERROR: Invalid shapes in replacement JSON:\n     - Shape 'shape-99' not found on 'slide-0'. Available shapes: shape-0, shape-1, shape-4\n     - Slide 'slide-999' not found in inventory\n   ```\n\n   ```\n   ERROR: Replacement text made overflow worse in these shapes:\n     - slide-0/shape-2: overflow worsened by 1.25\" (was 0.00\", now 1.25\")\n   ```\n\n## Creating Thumbnail Grids\n\nTo create visual thumbnail grids of PowerPoint slides for quick analysis and reference:\n\n```bash\npython scripts/thumbnail.py template.pptx [output_prefix]\n```\n\n**Features**:\n- Creates: `thumbnails.jpg` (or `thumbnails-1.jpg`, `thumbnails-2.jpg`, etc. for large decks)\n- Default: 5 columns, max 30 slides per grid (5Ã—6)\n- Custom prefix: `python scripts/thumbnail.py template.pptx my-grid`\n  - Note: The output prefix should include the path if you want output in a specific directory (e.g., `workspace/my-grid`)\n- Adjust columns: `--cols 4` (range: 3-6, affects slides per grid)\n- Grid limits: 3 cols = 12 slides/grid, 4 cols = 20, 5 cols = 30, 6 cols = 42\n- Slides are zero-indexed (Slide 0, Slide 1, etc.)\n\n**Use cases**:\n- Template analysis: Quickly understand slide layouts and design patterns\n- Content review: Visual overview of entire presentation\n- Navigation reference: Find specific slides by their visual appearance\n- Quality check: Verify all slides are properly formatted\n\n**Examples**:\n```bash\n# Basic usage\npython scripts/thumbnail.py presentation.pptx\n\n# Combine options: custom name, columns\npython scripts/thumbnail.py template.pptx analysis --cols 4\n```\n\n## Converting Slides to Images\n\nTo visually analyze PowerPoint slides, convert them to images using a two-step process:\n\n1. **Convert PPTX to PDF**:\n   ```bash\n   soffice --headless --convert-to pdf template.pptx\n   ```\n\n2. **Convert PDF pages to JPEG images**:\n   ```bash\n   pdftoppm -jpeg -r 150 template.pdf slide\n   ```\n   This creates files like `slide-1.jpg`, `slide-2.jpg`, etc.\n\nOptions:\n- `-r 150`: Sets resolution to 150 DPI (adjust for quality/size balance)\n- `-jpeg`: Output JPEG format (use `-png` for PNG if preferred)\n- `-f N`: First page to convert (e.g., `-f 2` starts from page 2)\n- `-l N`: Last page to convert (e.g., `-l 5` stops at page 5)\n- `slide`: Prefix for output files\n\nExample for specific range:\n```bash\npdftoppm -jpeg -r 150 -f 2 -l 5 template.pdf slide  # Converts only pages 2-5\n```\n\n## Code Style Guidelines\n**IMPORTANT**: When generating code for PPTX operations:\n- Write concise code\n- Avoid verbose variable names and redundant operations\n- Avoid unnecessary print statements\n\n## Dependencies\n\nRequired dependencies (should already be installed):\n\n- **markitdown**: `pip install \"markitdown[pptx]\"` (for text extraction from presentations)\n- **pptxgenjs**: `npm install -g pptxgenjs` (for creating presentations via html2pptx)\n- **playwright**: `npm install -g playwright` (for HTML rendering in html2pptx)\n- **react-icons**: `npm install -g react-icons react react-dom` (for icons)\n- **sharp**: `npm install -g sharp` (for SVG rasterization and image processing)\n- **LibreOffice**: `sudo apt-get install libreoffice` (for PDF conversion)\n- **Poppler**: `sudo apt-get install poppler-utils` (for pdftoppm to convert PDF to images)\n- **defusedxml**: `pip install defusedxml` (for secure XML parsing)"
    }
  },
  "anthropics-skills-skill-creator": {
    "id": "anthropics-skills-skill-creator",
    "name": "skill-creator",
    "description": "Guide for creating effective skills. This skill should be used when users want to create a new skill (or update an existing skill) that extends Claude's capabilities with specialized knowledge, workflows, or tool integrations.",
    "repo": {
      "owner": "anthropics",
      "name": "skills",
      "fullName": "anthropics/skills",
      "url": "https://github.com/anthropics/skills/tree/main/skills/skill-creator",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 35374,
      "forks": 3202,
      "language": "TypeScript",
      "topics": [],
      "updatedAt": "2026-01-08T06:00:29Z",
      "pushedAt": "2025-12-20T18:09:45Z"
    },
    "category": "official",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: skill-creator\ndescription: Guide for creating effective skills. This skill should be used when users want to create a new skill (or update an existing skill) that extends Claude's capabilities with specialized knowledge, workflows, or tool integrations.\nlicense: Complete terms in LICENSE.txt\n---\n\n# Skill Creator\n\nThis skill provides guidance for creating effective skills.\n\n## About Skills\n\nSkills are modular, self-contained packages that extend Claude's capabilities by providing\nspecialized knowledge, workflows, and tools. Think of them as \"onboarding guides\" for specific\ndomains or tasksâ€”they transform Claude from a general-purpose agent into a specialized agent\nequipped with procedural knowledge that no model can fully possess.\n\n### What Skills Provide\n\n1. Specialized workflows - Multi-step procedures for specific domains\n2. Tool integrations - Instructions for working with specific file formats or APIs\n3. Domain expertise - Company-specific knowledge, schemas, business logic\n4. Bundled resources - Scripts, references, and assets for complex and repetitive tasks\n\n## Core Principles\n\n### Concise is Key\n\nThe context window is a public good. Skills share the context window with everything else Claude needs: system prompt, conversation history, other Skills' metadata, and the actual user request.\n\n**Default assumption: Claude is already very smart.** Only add context Claude doesn't already have. Challenge each piece of information: \"Does Claude really need this explanation?\" and \"Does this paragraph justify its token cost?\"\n\nPrefer concise examples over verbose explanations.\n\n### Set Appropriate Degrees of Freedom\n\nMatch the level of specificity to the task's fragility and variability:\n\n**High freedom (text-based instructions)**: Use when multiple approaches are valid, decisions depend on context, or heuristics guide the approach.\n\n**Medium freedom (pseudocode or scripts with parameters)**: Use when a preferred pattern exists, some variation is acceptable, or configuration affects behavior.\n\n**Low freedom (specific scripts, few parameters)**: Use when operations are fragile and error-prone, consistency is critical, or a specific sequence must be followed.\n\nThink of Claude as exploring a path: a narrow bridge with cliffs needs specific guardrails (low freedom), while an open field allows many routes (high freedom).\n\n### Anatomy of a Skill\n\nEvery skill consists of a required SKILL.md file and optional bundled resources:\n\n```\nskill-name/\nâ”œâ”€â”€ SKILL.md (required)\nâ”‚   â”œâ”€â”€ YAML frontmatter metadata (required)\nâ”‚   â”‚   â”œâ”€â”€ name: (required)\nâ”‚   â”‚   â””â”€â”€ description: (required)\nâ”‚   â””â”€â”€ Markdown instructions (required)\nâ””â”€â”€ Bundled Resources (optional)\n    â”œâ”€â”€ scripts/          - Executable code (Python/Bash/etc.)\n    â”œâ”€â”€ references/       - Documentation intended to be loaded into context as needed\n    â””â”€â”€ assets/           - Files used in output (templates, icons, fonts, etc.)\n```\n\n#### SKILL.md (required)\n\nEvery SKILL.md consists of:\n\n- **Frontmatter** (YAML): Contains `name` and `description` fields. These are the only fields that Claude reads to determine when the skill gets used, thus it is very important to be clear and comprehensive in describing what the skill is, and when it should be used.\n- **Body** (Markdown): Instructions and guidance for using the skill. Only loaded AFTER the skill triggers (if at all).\n\n#### Bundled Resources (optional)\n\n##### Scripts (`scripts/`)\n\nExecutable code (Python/Bash/etc.) for tasks that require deterministic reliability or are repeatedly rewritten.\n\n- **When to include**: When the same code is being rewritten repeatedly or deterministic reliability is needed\n- **Example**: `scripts/rotate_pdf.py` for PDF rotation tasks\n- **Benefits**: Token efficient, deterministic, may be executed without loading into context\n- **Note**: Scripts may still need to be read by Claude for patching or environment-specific adjustments\n\n##### References (`references/`)\n\nDocumentation and reference material intended to be loaded as needed into context to inform Claude's process and thinking.\n\n- **When to include**: For documentation that Claude should reference while working\n- **Examples**: `references/finance.md` for financial schemas, `references/mnda.md` for company NDA template, `references/policies.md` for company policies, `references/api_docs.md` for API specifications\n- **Use cases**: Database schemas, API documentation, domain knowledge, company policies, detailed workflow guides\n- **Benefits**: Keeps SKILL.md lean, loaded only when Claude determines it's needed\n- **Best practice**: If files are large (>10k words), include grep search patterns in SKILL.md\n- **Avoid duplication**: Information should live in either SKILL.md or references files, not both. Prefer references files for detailed information unless it's truly core to the skillâ€”this keeps SKILL.md lean while making information discoverable without hogging the context window. Keep only essential procedural instructions and workflow guidance in SKILL.md; move detailed reference material, schemas, and examples to references files.\n\n##### Assets (`assets/`)\n\nFiles not intended to be loaded into context, but rather used within the output Claude produces.\n\n- **When to include**: When the skill needs files that will be used in the final output\n- **Examples**: `assets/logo.png` for brand assets, `assets/slides.pptx` for PowerPoint templates, `assets/frontend-template/` for HTML/React boilerplate, `assets/font.ttf` for typography\n- **Use cases**: Templates, images, icons, boilerplate code, fonts, sample documents that get copied or modified\n- **Benefits**: Separates output resources from documentation, enables Claude to use files without loading them into context\n\n#### What to Not Include in a Skill\n\nA skill should only contain essential files that directly support its functionality. Do NOT create extraneous documentation or auxiliary files, including:\n\n- README.md\n- INSTALLATION_GUIDE.md\n- QUICK_REFERENCE.md\n- CHANGELOG.md\n- etc.\n\nThe skill should only contain the information needed for an AI agent to do the job at hand. It should not contain auxilary context about the process that went into creating it, setup and testing procedures, user-facing documentation, etc. Creating additional documentation files just adds clutter and confusion.\n\n### Progressive Disclosure Design Principle\n\nSkills use a three-level loading system to manage context efficiently:\n\n1. **Metadata (name + description)** - Always in context (~100 words)\n2. **SKILL.md body** - When skill triggers (<5k words)\n3. **Bundled resources** - As needed by Claude (Unlimited because scripts can be executed without reading into context window)\n\n#### Progressive Disclosure Patterns\n\nKeep SKILL.md body to the essentials and under 500 lines to minimize context bloat. Split content into separate files when approaching this limit. When splitting out content into other files, it is very important to reference them from SKILL.md and describe clearly when to read them, to ensure the reader of the skill knows they exist and when to use them.\n\n**Key principle:** When a skill supports multiple variations, frameworks, or options, keep only the core workflow and selection guidance in SKILL.md. Move variant-specific details (patterns, examples, configuration) into separate reference files.\n\n**Pattern 1: High-level guide with references**\n\n```markdown\n# PDF Processing\n\n## Quick start\n\nExtract text with pdfplumber:\n[code example]\n\n## Advanced features\n\n- **Form filling**: See [FORMS.md](FORMS.md) for complete guide\n- **API reference**: See [REFERENCE.md](REFERENCE.md) for all methods\n- **Examples**: See [EXAMPLES.md](EXAMPLES.md) for common patterns\n```\n\nClaude loads FORMS.md, REFERENCE.md, or EXAMPLES.md only when needed.\n\n**Pattern 2: Domain-specific organization**\n\nFor Skills with multiple domains, organize content by domain to avoid loading irrelevant context:\n\n```\nbigquery-skill/\nâ”œâ”€â”€ SKILL.md (overview and navigation)\nâ””â”€â”€ reference/\n    â”œâ”€â”€ finance.md (revenue, billing metrics)\n    â”œâ”€â”€ sales.md (opportunities, pipeline)\n    â”œâ”€â”€ product.md (API usage, features)\n    â””â”€â”€ marketing.md (campaigns, attribution)\n```\n\nWhen a user asks about sales metrics, Claude only reads sales.md.\n\nSimilarly, for skills supporting multiple frameworks or variants, organize by variant:\n\n```\ncloud-deploy/\nâ”œâ”€â”€ SKILL.md (workflow + provider selection)\nâ””â”€â”€ references/\n    â”œâ”€â”€ aws.md (AWS deployment patterns)\n    â”œâ”€â”€ gcp.md (GCP deployment patterns)\n    â””â”€â”€ azure.md (Azure deployment patterns)\n```\n\nWhen the user chooses AWS, Claude only reads aws.md.\n\n**Pattern 3: Conditional details**\n\nShow basic content, link to advanced content:\n\n```markdown\n# DOCX Processing\n\n## Creating documents\n\nUse docx-js for new documents. See [DOCX-JS.md](DOCX-JS.md).\n\n## Editing documents\n\nFor simple edits, modify the XML directly.\n\n**For tracked changes**: See [REDLINING.md](REDLINING.md)\n**For OOXML details**: See [OOXML.md](OOXML.md)\n```\n\nClaude reads REDLINING.md or OOXML.md only when the user needs those features.\n\n**Important guidelines:**\n\n- **Avoid deeply nested references** - Keep references one level deep from SKILL.md. All reference files should link directly from SKILL.md.\n- **Structure longer reference files** - For files longer than 100 lines, include a table of contents at the top so Claude can see the full scope when previewing.\n\n## Skill Creation Process\n\nSkill creation involves these steps:\n\n1. Understand the skill with concrete examples\n2. Plan reusable skill contents (scripts, references, assets)\n3. Initialize the skill (run init_skill.py)\n4. Edit the skill (implement resources and write SKILL.md)\n5. Package the skill (run package_skill.py)\n6. Iterate based on real usage\n\nFollow these steps in order, skipping only if there is a clear reason why they are not applicable.\n\n### Step 1: Understanding the Skill with Concrete Examples\n\nSkip this step only when the skill's usage patterns are already clearly understood. It remains valuable even when working with an existing skill.\n\nTo create an effective skill, clearly understand concrete examples of how the skill will be used. This understanding can come from either direct user examples or generated examples that are validated with user feedback.\n\nFor example, when building an image-editor skill, relevant questions include:\n\n- \"What functionality should the image-editor skill support? Editing, rotating, anything else?\"\n- \"Can you give some examples of how this skill would be used?\"\n- \"I can imagine users asking for things like 'Remove the red-eye from this image' or 'Rotate this image'. Are there other ways you imagine this skill being used?\"\n- \"What would a user say that should trigger this skill?\"\n\nTo avoid overwhelming users, avoid asking too many questions in a single message. Start with the most important questions and follow up as needed for better effectiveness.\n\nConclude this step when there is a clear sense of the functionality the skill should support.\n\n### Step 2: Planning the Reusable Skill Contents\n\nTo turn concrete examples into an effective skill, analyze each example by:\n\n1. Considering how to execute on the example from scratch\n2. Identifying what scripts, references, and assets would be helpful when executing these workflows repeatedly\n\nExample: When building a `pdf-editor` skill to handle queries like \"Help me rotate this PDF,\" the analysis shows:\n\n1. Rotating a PDF requires re-writing the same code each time\n2. A `scripts/rotate_pdf.py` script would be helpful to store in the skill\n\nExample: When designing a `frontend-webapp-builder` skill for queries like \"Build me a todo app\" or \"Build me a dashboard to track my steps,\" the analysis shows:\n\n1. Writing a frontend webapp requires the same boilerplate HTML/React each time\n2. An `assets/hello-world/` template containing the boilerplate HTML/React project files would be helpful to store in the skill\n\nExample: When building a `big-query` skill to handle queries like \"How many users have logged in today?\" the analysis shows:\n\n1. Querying BigQuery requires re-discovering the table schemas and relationships each time\n2. A `references/schema.md` file documenting the table schemas would be helpful to store in the skill\n\nTo establish the skill's contents, analyze each concrete example to create a list of the reusable resources to include: scripts, references, and assets.\n\n### Step 3: Initializing the Skill\n\nAt this point, it is time to actually create the skill.\n\nSkip this step only if the skill being developed already exists, and iteration or packaging is needed. In this case, continue to the next step.\n\nWhen creating a new skill from scratch, always run the `init_skill.py` script. The script conveniently generates a new template skill directory that automatically includes everything a skill requires, making the skill creation process much more efficient and reliable.\n\nUsage:\n\n```bash\nscripts/init_skill.py <skill-name> --path <output-directory>\n```\n\nThe script:\n\n- Creates the skill directory at the specified path\n- Generates a SKILL.md template with proper frontmatter and TODO placeholders\n- Creates example resource directories: `scripts/`, `references/`, and `assets/`\n- Adds example files in each directory that can be customized or deleted\n\nAfter initialization, customize or remove the generated SKILL.md and example files as needed.\n\n### Step 4: Edit the Skill\n\nWhen editing the (newly-generated or existing) skill, remember that the skill is being created for another instance of Claude to use. Include information that would be beneficial and non-obvious to Claude. Consider what procedural knowledge, domain-specific details, or reusable assets would help another Claude instance execute these tasks more effectively.\n\n#### Learn Proven Design Patterns\n\nConsult these helpful guides based on your skill's needs:\n\n- **Multi-step processes**: See references/workflows.md for sequential workflows and conditional logic\n- **Specific output formats or quality standards**: See references/output-patterns.md for template and example patterns\n\nThese files contain established best practices for effective skill design.\n\n#### Start with Reusable Skill Contents\n\nTo begin implementation, start with the reusable resources identified above: `scripts/`, `references/`, and `assets/` files. Note that this step may require user input. For example, when implementing a `brand-guidelines` skill, the user may need to provide brand assets or templates to store in `assets/`, or documentation to store in `references/`.\n\nAdded scripts must be tested by actually running them to ensure there are no bugs and that the output matches what is expected. If there are many similar scripts, only a representative sample needs to be tested to ensure confidence that they all work while balancing time to completion.\n\nAny example files and directories not needed for the skill should be deleted. The initialization script creates example files in `scripts/`, `references/`, and `assets/` to demonstrate structure, but most skills won't need all of them.\n\n#### Update SKILL.md\n\n**Writing Guidelines:** Always use imperative/infinitive form.\n\n##### Frontmatter\n\nWrite the YAML frontmatter with `name` and `description`:\n\n- `name`: The skill name\n- `description`: This is the primary triggering mechanism for your skill, and helps Claude understand when to use the skill.\n  - Include both what the Skill does and specific triggers/contexts for when to use it.\n  - Include all \"when to use\" information here - Not in the body. The body is only loaded after triggering, so \"When to Use This Skill\" sections in the body are not helpful to Claude.\n  - Example description for a `docx` skill: \"Comprehensive document creation, editing, and analysis with support for tracked changes, comments, formatting preservation, and text extraction. Use when Claude needs to work with professional documents (.docx files) for: (1) Creating new documents, (2) Modifying or editing content, (3) Working with tracked changes, (4) Adding comments, or any other document tasks\"\n\nDo not include any other fields in YAML frontmatter.\n\n##### Body\n\nWrite instructions for using the skill and its bundled resources.\n\n### Step 5: Packaging a Skill\n\nOnce development of the skill is complete, it must be packaged into a distributable .skill file that gets shared with the user. The packaging process automatically validates the skill first to ensure it meets all requirements:\n\n```bash\nscripts/package_skill.py <path/to/skill-folder>\n```\n\nOptional output directory specification:\n\n```bash\nscripts/package_skill.py <path/to/skill-folder> ./dist\n```\n\nThe packaging script will:\n\n1. **Validate** the skill automatically, checking:\n\n   - YAML frontmatter format and required fields\n   - Skill naming conventions and directory structure\n   - Description completeness and quality\n   - File organization and resource references\n\n2. **Package** the skill if validation passes, creating a .skill file named after the skill (e.g., `my-skill.skill`) that includes all files and maintains the proper directory structure for distribution. The .skill file is a zip file with a .skill extension.\n\nIf validation fails, the script will report the errors and exit without creating a package. Fix any validation errors and run the packaging command again.\n\n### Step 6: Iterate\n\nAfter testing the skill, users may request improvements. Often this happens right after using the skill, with fresh context of how the skill performed.\n\n**Iteration workflow:**\n\n1. Use the skill on real tasks\n2. Notice struggles or inefficiencies\n3. Identify how SKILL.md or bundled resources should be updated\n4. Implement changes and test again\n",
      "frontmatter": {
        "name": "skill-creator",
        "description": "Guide for creating effective skills. This skill should be used when users want to create a new skill (or update an existing skill) that extends Claude's capabilities with specialized knowledge, workflows, or tool integrations.",
        "license": "Complete terms in LICENSE.txt"
      },
      "content": "\n# Skill Creator\n\nThis skill provides guidance for creating effective skills.\n\n## About Skills\n\nSkills are modular, self-contained packages that extend Claude's capabilities by providing\nspecialized knowledge, workflows, and tools. Think of them as \"onboarding guides\" for specific\ndomains or tasksâ€”they transform Claude from a general-purpose agent into a specialized agent\nequipped with procedural knowledge that no model can fully possess.\n\n### What Skills Provide\n\n1. Specialized workflows - Multi-step procedures for specific domains\n2. Tool integrations - Instructions for working with specific file formats or APIs\n3. Domain expertise - Company-specific knowledge, schemas, business logic\n4. Bundled resources - Scripts, references, and assets for complex and repetitive tasks\n\n## Core Principles\n\n### Concise is Key\n\nThe context window is a public good. Skills share the context window with everything else Claude needs: system prompt, conversation history, other Skills' metadata, and the actual user request.\n\n**Default assumption: Claude is already very smart.** Only add context Claude doesn't already have. Challenge each piece of information: \"Does Claude really need this explanation?\" and \"Does this paragraph justify its token cost?\"\n\nPrefer concise examples over verbose explanations.\n\n### Set Appropriate Degrees of Freedom\n\nMatch the level of specificity to the task's fragility and variability:\n\n**High freedom (text-based instructions)**: Use when multiple approaches are valid, decisions depend on context, or heuristics guide the approach.\n\n**Medium freedom (pseudocode or scripts with parameters)**: Use when a preferred pattern exists, some variation is acceptable, or configuration affects behavior.\n\n**Low freedom (specific scripts, few parameters)**: Use when operations are fragile and error-prone, consistency is critical, or a specific sequence must be followed.\n\nThink of Claude as exploring a path: a narrow bridge with cliffs needs specific guardrails (low freedom), while an open field allows many routes (high freedom).\n\n### Anatomy of a Skill\n\nEvery skill consists of a required SKILL.md file and optional bundled resources:\n\n```\nskill-name/\nâ”œâ”€â”€ SKILL.md (required)\nâ”‚   â”œâ”€â”€ YAML frontmatter metadata (required)\nâ”‚   â”‚   â”œâ”€â”€ name: (required)\nâ”‚   â”‚   â””â”€â”€ description: (required)\nâ”‚   â””â”€â”€ Markdown instructions (required)\nâ””â”€â”€ Bundled Resources (optional)\n    â”œâ”€â”€ scripts/          - Executable code (Python/Bash/etc.)\n    â”œâ”€â”€ references/       - Documentation intended to be loaded into context as needed\n    â””â”€â”€ assets/           - Files used in output (templates, icons, fonts, etc.)\n```\n\n#### SKILL.md (required)\n\nEvery SKILL.md consists of:\n\n- **Frontmatter** (YAML): Contains `name` and `description` fields. These are the only fields that Claude reads to determine when the skill gets used, thus it is very important to be clear and comprehensive in describing what the skill is, and when it should be used.\n- **Body** (Markdown): Instructions and guidance for using the skill. Only loaded AFTER the skill triggers (if at all).\n\n#### Bundled Resources (optional)\n\n##### Scripts (`scripts/`)\n\nExecutable code (Python/Bash/etc.) for tasks that require deterministic reliability or are repeatedly rewritten.\n\n- **When to include**: When the same code is being rewritten repeatedly or deterministic reliability is needed\n- **Example**: `scripts/rotate_pdf.py` for PDF rotation tasks\n- **Benefits**: Token efficient, deterministic, may be executed without loading into context\n- **Note**: Scripts may still need to be read by Claude for patching or environment-specific adjustments\n\n##### References (`references/`)\n\nDocumentation and reference material intended to be loaded as needed into context to inform Claude's process and thinking.\n\n- **When to include**: For documentation that Claude should reference while working\n- **Examples**: `references/finance.md` for financial schemas, `references/mnda.md` for company NDA template, `references/policies.md` for company policies, `references/api_docs.md` for API specifications\n- **Use cases**: Database schemas, API documentation, domain knowledge, company policies, detailed workflow guides\n- **Benefits**: Keeps SKILL.md lean, loaded only when Claude determines it's needed\n- **Best practice**: If files are large (>10k words), include grep search patterns in SKILL.md\n- **Avoid duplication**: Information should live in either SKILL.md or references files, not both. Prefer references files for detailed information unless it's truly core to the skillâ€”this keeps SKILL.md lean while making information discoverable without hogging the context window. Keep only essential procedural instructions and workflow guidance in SKILL.md; move detailed reference material, schemas, and examples to references files.\n\n##### Assets (`assets/`)\n\nFiles not intended to be loaded into context, but rather used within the output Claude produces.\n\n- **When to include**: When the skill needs files that will be used in the final output\n- **Examples**: `assets/logo.png` for brand assets, `assets/slides.pptx` for PowerPoint templates, `assets/frontend-template/` for HTML/React boilerplate, `assets/font.ttf` for typography\n- **Use cases**: Templates, images, icons, boilerplate code, fonts, sample documents that get copied or modified\n- **Benefits**: Separates output resources from documentation, enables Claude to use files without loading them into context\n\n#### What to Not Include in a Skill\n\nA skill should only contain essential files that directly support its functionality. Do NOT create extraneous documentation or auxiliary files, including:\n\n- README.md\n- INSTALLATION_GUIDE.md\n- QUICK_REFERENCE.md\n- CHANGELOG.md\n- etc.\n\nThe skill should only contain the information needed for an AI agent to do the job at hand. It should not contain auxilary context about the process that went into creating it, setup and testing procedures, user-facing documentation, etc. Creating additional documentation files just adds clutter and confusion.\n\n### Progressive Disclosure Design Principle\n\nSkills use a three-level loading system to manage context efficiently:\n\n1. **Metadata (name + description)** - Always in context (~100 words)\n2. **SKILL.md body** - When skill triggers (<5k words)\n3. **Bundled resources** - As needed by Claude (Unlimited because scripts can be executed without reading into context window)\n\n#### Progressive Disclosure Patterns\n\nKeep SKILL.md body to the essentials and under 500 lines to minimize context bloat. Split content into separate files when approaching this limit. When splitting out content into other files, it is very important to reference them from SKILL.md and describe clearly when to read them, to ensure the reader of the skill knows they exist and when to use them.\n\n**Key principle:** When a skill supports multiple variations, frameworks, or options, keep only the core workflow and selection guidance in SKILL.md. Move variant-specific details (patterns, examples, configuration) into separate reference files.\n\n**Pattern 1: High-level guide with references**\n\n```markdown\n# PDF Processing\n\n## Quick start\n\nExtract text with pdfplumber:\n[code example]\n\n## Advanced features\n\n- **Form filling**: See [FORMS.md](FORMS.md) for complete guide\n- **API reference**: See [REFERENCE.md](REFERENCE.md) for all methods\n- **Examples**: See [EXAMPLES.md](EXAMPLES.md) for common patterns\n```\n\nClaude loads FORMS.md, REFERENCE.md, or EXAMPLES.md only when needed.\n\n**Pattern 2: Domain-specific organization**\n\nFor Skills with multiple domains, organize content by domain to avoid loading irrelevant context:\n\n```\nbigquery-skill/\nâ”œâ”€â”€ SKILL.md (overview and navigation)\nâ””â”€â”€ reference/\n    â”œâ”€â”€ finance.md (revenue, billing metrics)\n    â”œâ”€â”€ sales.md (opportunities, pipeline)\n    â”œâ”€â”€ product.md (API usage, features)\n    â””â”€â”€ marketing.md (campaigns, attribution)\n```\n\nWhen a user asks about sales metrics, Claude only reads sales.md.\n\nSimilarly, for skills supporting multiple frameworks or variants, organize by variant:\n\n```\ncloud-deploy/\nâ”œâ”€â”€ SKILL.md (workflow + provider selection)\nâ””â”€â”€ references/\n    â”œâ”€â”€ aws.md (AWS deployment patterns)\n    â”œâ”€â”€ gcp.md (GCP deployment patterns)\n    â””â”€â”€ azure.md (Azure deployment patterns)\n```\n\nWhen the user chooses AWS, Claude only reads aws.md.\n\n**Pattern 3: Conditional details**\n\nShow basic content, link to advanced content:\n\n```markdown\n# DOCX Processing\n\n## Creating documents\n\nUse docx-js for new documents. See [DOCX-JS.md](DOCX-JS.md).\n\n## Editing documents\n\nFor simple edits, modify the XML directly.\n\n**For tracked changes**: See [REDLINING.md](REDLINING.md)\n**For OOXML details**: See [OOXML.md](OOXML.md)\n```\n\nClaude reads REDLINING.md or OOXML.md only when the user needs those features.\n\n**Important guidelines:**\n\n- **Avoid deeply nested references** - Keep references one level deep from SKILL.md. All reference files should link directly from SKILL.md.\n- **Structure longer reference files** - For files longer than 100 lines, include a table of contents at the top so Claude can see the full scope when previewing.\n\n## Skill Creation Process\n\nSkill creation involves these steps:\n\n1. Understand the skill with concrete examples\n2. Plan reusable skill contents (scripts, references, assets)\n3. Initialize the skill (run init_skill.py)\n4. Edit the skill (implement resources and write SKILL.md)\n5. Package the skill (run package_skill.py)\n6. Iterate based on real usage\n\nFollow these steps in order, skipping only if there is a clear reason why they are not applicable.\n\n### Step 1: Understanding the Skill with Concrete Examples\n\nSkip this step only when the skill's usage patterns are already clearly understood. It remains valuable even when working with an existing skill.\n\nTo create an effective skill, clearly understand concrete examples of how the skill will be used. This understanding can come from either direct user examples or generated examples that are validated with user feedback.\n\nFor example, when building an image-editor skill, relevant questions include:\n\n- \"What functionality should the image-editor skill support? Editing, rotating, anything else?\"\n- \"Can you give some examples of how this skill would be used?\"\n- \"I can imagine users asking for things like 'Remove the red-eye from this image' or 'Rotate this image'. Are there other ways you imagine this skill being used?\"\n- \"What would a user say that should trigger this skill?\"\n\nTo avoid overwhelming users, avoid asking too many questions in a single message. Start with the most important questions and follow up as needed for better effectiveness.\n\nConclude this step when there is a clear sense of the functionality the skill should support.\n\n### Step 2: Planning the Reusable Skill Contents\n\nTo turn concrete examples into an effective skill, analyze each example by:\n\n1. Considering how to execute on the example from scratch\n2. Identifying what scripts, references, and assets would be helpful when executing these workflows repeatedly\n\nExample: When building a `pdf-editor` skill to handle queries like \"Help me rotate this PDF,\" the analysis shows:\n\n1. Rotating a PDF requires re-writing the same code each time\n2. A `scripts/rotate_pdf.py` script would be helpful to store in the skill\n\nExample: When designing a `frontend-webapp-builder` skill for queries like \"Build me a todo app\" or \"Build me a dashboard to track my steps,\" the analysis shows:\n\n1. Writing a frontend webapp requires the same boilerplate HTML/React each time\n2. An `assets/hello-world/` template containing the boilerplate HTML/React project files would be helpful to store in the skill\n\nExample: When building a `big-query` skill to handle queries like \"How many users have logged in today?\" the analysis shows:\n\n1. Querying BigQuery requires re-discovering the table schemas and relationships each time\n2. A `references/schema.md` file documenting the table schemas would be helpful to store in the skill\n\nTo establish the skill's contents, analyze each concrete example to create a list of the reusable resources to include: scripts, references, and assets.\n\n### Step 3: Initializing the Skill\n\nAt this point, it is time to actually create the skill.\n\nSkip this step only if the skill being developed already exists, and iteration or packaging is needed. In this case, continue to the next step.\n\nWhen creating a new skill from scratch, always run the `init_skill.py` script. The script conveniently generates a new template skill directory that automatically includes everything a skill requires, making the skill creation process much more efficient and reliable.\n\nUsage:\n\n```bash\nscripts/init_skill.py <skill-name> --path <output-directory>\n```\n\nThe script:\n\n- Creates the skill directory at the specified path\n- Generates a SKILL.md template with proper frontmatter and TODO placeholders\n- Creates example resource directories: `scripts/`, `references/`, and `assets/`\n- Adds example files in each directory that can be customized or deleted\n\nAfter initialization, customize or remove the generated SKILL.md and example files as needed.\n\n### Step 4: Edit the Skill\n\nWhen editing the (newly-generated or existing) skill, remember that the skill is being created for another instance of Claude to use. Include information that would be beneficial and non-obvious to Claude. Consider what procedural knowledge, domain-specific details, or reusable assets would help another Claude instance execute these tasks more effectively.\n\n#### Learn Proven Design Patterns\n\nConsult these helpful guides based on your skill's needs:\n\n- **Multi-step processes**: See references/workflows.md for sequential workflows and conditional logic\n- **Specific output formats or quality standards**: See references/output-patterns.md for template and example patterns\n\nThese files contain established best practices for effective skill design.\n\n#### Start with Reusable Skill Contents\n\nTo begin implementation, start with the reusable resources identified above: `scripts/`, `references/`, and `assets/` files. Note that this step may require user input. For example, when implementing a `brand-guidelines` skill, the user may need to provide brand assets or templates to store in `assets/`, or documentation to store in `references/`.\n\nAdded scripts must be tested by actually running them to ensure there are no bugs and that the output matches what is expected. If there are many similar scripts, only a representative sample needs to be tested to ensure confidence that they all work while balancing time to completion.\n\nAny example files and directories not needed for the skill should be deleted. The initialization script creates example files in `scripts/`, `references/`, and `assets/` to demonstrate structure, but most skills won't need all of them.\n\n#### Update SKILL.md\n\n**Writing Guidelines:** Always use imperative/infinitive form.\n\n##### Frontmatter\n\nWrite the YAML frontmatter with `name` and `description`:\n\n- `name`: The skill name\n- `description`: This is the primary triggering mechanism for your skill, and helps Claude understand when to use the skill.\n  - Include both what the Skill does and specific triggers/contexts for when to use it.\n  - Include all \"when to use\" information here - Not in the body. The body is only loaded after triggering, so \"When to Use This Skill\" sections in the body are not helpful to Claude.\n  - Example description for a `docx` skill: \"Comprehensive document creation, editing, and analysis with support for tracked changes, comments, formatting preservation, and text extraction. Use when Claude needs to work with professional documents (.docx files) for: (1) Creating new documents, (2) Modifying or editing content, (3) Working with tracked changes, (4) Adding comments, or any other document tasks\"\n\nDo not include any other fields in YAML frontmatter.\n\n##### Body\n\nWrite instructions for using the skill and its bundled resources.\n\n### Step 5: Packaging a Skill\n\nOnce development of the skill is complete, it must be packaged into a distributable .skill file that gets shared with the user. The packaging process automatically validates the skill first to ensure it meets all requirements:\n\n```bash\nscripts/package_skill.py <path/to/skill-folder>\n```\n\nOptional output directory specification:\n\n```bash\nscripts/package_skill.py <path/to/skill-folder> ./dist\n```\n\nThe packaging script will:\n\n1. **Validate** the skill automatically, checking:\n\n   - YAML frontmatter format and required fields\n   - Skill naming conventions and directory structure\n   - Description completeness and quality\n   - File organization and resource references\n\n2. **Package** the skill if validation passes, creating a .skill file named after the skill (e.g., `my-skill.skill`) that includes all files and maintains the proper directory structure for distribution. The .skill file is a zip file with a .skill extension.\n\nIf validation fails, the script will report the errors and exit without creating a package. Fix any validation errors and run the packaging command again.\n\n### Step 6: Iterate\n\nAfter testing the skill, users may request improvements. Often this happens right after using the skill, with fresh context of how the skill performed.\n\n**Iteration workflow:**\n\n1. Use the skill on real tasks\n2. Notice struggles or inefficiencies\n3. Identify how SKILL.md or bundled resources should be updated\n4. Implement changes and test again\n"
    }
  },
  "anthropics-skills-slack-gif-creator": {
    "id": "anthropics-skills-slack-gif-creator",
    "name": "slack-gif-creator",
    "description": "Knowledge and utilities for creating animated GIFs optimized for Slack. Provides constraints, validation tools, and animation concepts. Use when users request animated GIFs for Slack like \"make me a GIF of X doing Y for Slack.\"",
    "repo": {
      "owner": "anthropics",
      "name": "skills",
      "fullName": "anthropics/skills",
      "url": "https://github.com/anthropics/skills/tree/main/skills/slack-gif-creator",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 35374,
      "forks": 3202,
      "language": "TypeScript",
      "topics": [],
      "updatedAt": "2026-01-08T06:00:29Z",
      "pushedAt": "2025-12-20T18:09:45Z"
    },
    "category": "official",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: slack-gif-creator\ndescription: Knowledge and utilities for creating animated GIFs optimized for Slack. Provides constraints, validation tools, and animation concepts. Use when users request animated GIFs for Slack like \"make me a GIF of X doing Y for Slack.\"\nlicense: Complete terms in LICENSE.txt\n---\n\n# Slack GIF Creator\n\nA toolkit providing utilities and knowledge for creating animated GIFs optimized for Slack.\n\n## Slack Requirements\n\n**Dimensions:**\n- Emoji GIFs: 128x128 (recommended)\n- Message GIFs: 480x480\n\n**Parameters:**\n- FPS: 10-30 (lower is smaller file size)\n- Colors: 48-128 (fewer = smaller file size)\n- Duration: Keep under 3 seconds for emoji GIFs\n\n## Core Workflow\n\n```python\nfrom core.gif_builder import GIFBuilder\nfrom PIL import Image, ImageDraw\n\n# 1. Create builder\nbuilder = GIFBuilder(width=128, height=128, fps=10)\n\n# 2. Generate frames\nfor i in range(12):\n    frame = Image.new('RGB', (128, 128), (240, 248, 255))\n    draw = ImageDraw.Draw(frame)\n\n    # Draw your animation using PIL primitives\n    # (circles, polygons, lines, etc.)\n\n    builder.add_frame(frame)\n\n# 3. Save with optimization\nbuilder.save('output.gif', num_colors=48, optimize_for_emoji=True)\n```\n\n## Drawing Graphics\n\n### Working with User-Uploaded Images\nIf a user uploads an image, consider whether they want to:\n- **Use it directly** (e.g., \"animate this\", \"split this into frames\")\n- **Use it as inspiration** (e.g., \"make something like this\")\n\nLoad and work with images using PIL:\n```python\nfrom PIL import Image\n\nuploaded = Image.open('file.png')\n# Use directly, or just as reference for colors/style\n```\n\n### Drawing from Scratch\nWhen drawing graphics from scratch, use PIL ImageDraw primitives:\n\n```python\nfrom PIL import ImageDraw\n\ndraw = ImageDraw.Draw(frame)\n\n# Circles/ovals\ndraw.ellipse([x1, y1, x2, y2], fill=(r, g, b), outline=(r, g, b), width=3)\n\n# Stars, triangles, any polygon\npoints = [(x1, y1), (x2, y2), (x3, y3), ...]\ndraw.polygon(points, fill=(r, g, b), outline=(r, g, b), width=3)\n\n# Lines\ndraw.line([(x1, y1), (x2, y2)], fill=(r, g, b), width=5)\n\n# Rectangles\ndraw.rectangle([x1, y1, x2, y2], fill=(r, g, b), outline=(r, g, b), width=3)\n```\n\n**Don't use:** Emoji fonts (unreliable across platforms) or assume pre-packaged graphics exist in this skill.\n\n### Making Graphics Look Good\n\nGraphics should look polished and creative, not basic. Here's how:\n\n**Use thicker lines** - Always set `width=2` or higher for outlines and lines. Thin lines (width=1) look choppy and amateurish.\n\n**Add visual depth**:\n- Use gradients for backgrounds (`create_gradient_background`)\n- Layer multiple shapes for complexity (e.g., a star with a smaller star inside)\n\n**Make shapes more interesting**:\n- Don't just draw a plain circle - add highlights, rings, or patterns\n- Stars can have glows (draw larger, semi-transparent versions behind)\n- Combine multiple shapes (stars + sparkles, circles + rings)\n\n**Pay attention to colors**:\n- Use vibrant, complementary colors\n- Add contrast (dark outlines on light shapes, light outlines on dark shapes)\n- Consider the overall composition\n\n**For complex shapes** (hearts, snowflakes, etc.):\n- Use combinations of polygons and ellipses\n- Calculate points carefully for symmetry\n- Add details (a heart can have a highlight curve, snowflakes have intricate branches)\n\nBe creative and detailed! A good Slack GIF should look polished, not like placeholder graphics.\n\n## Available Utilities\n\n### GIFBuilder (`core.gif_builder`)\nAssembles frames and optimizes for Slack:\n```python\nbuilder = GIFBuilder(width=128, height=128, fps=10)\nbuilder.add_frame(frame)  # Add PIL Image\nbuilder.add_frames(frames)  # Add list of frames\nbuilder.save('out.gif', num_colors=48, optimize_for_emoji=True, remove_duplicates=True)\n```\n\n### Validators (`core.validators`)\nCheck if GIF meets Slack requirements:\n```python\nfrom core.validators import validate_gif, is_slack_ready\n\n# Detailed validation\npasses, info = validate_gif('my.gif', is_emoji=True, verbose=True)\n\n# Quick check\nif is_slack_ready('my.gif'):\n    print(\"Ready!\")\n```\n\n### Easing Functions (`core.easing`)\nSmooth motion instead of linear:\n```python\nfrom core.easing import interpolate\n\n# Progress from 0.0 to 1.0\nt = i / (num_frames - 1)\n\n# Apply easing\ny = interpolate(start=0, end=400, t=t, easing='ease_out')\n\n# Available: linear, ease_in, ease_out, ease_in_out,\n#           bounce_out, elastic_out, back_out\n```\n\n### Frame Helpers (`core.frame_composer`)\nConvenience functions for common needs:\n```python\nfrom core.frame_composer import (\n    create_blank_frame,         # Solid color background\n    create_gradient_background,  # Vertical gradient\n    draw_circle,                # Helper for circles\n    draw_text,                  # Simple text rendering\n    draw_star                   # 5-pointed star\n)\n```\n\n## Animation Concepts\n\n### Shake/Vibrate\nOffset object position with oscillation:\n- Use `math.sin()` or `math.cos()` with frame index\n- Add small random variations for natural feel\n- Apply to x and/or y position\n\n### Pulse/Heartbeat\nScale object size rhythmically:\n- Use `math.sin(t * frequency * 2 * math.pi)` for smooth pulse\n- For heartbeat: two quick pulses then pause (adjust sine wave)\n- Scale between 0.8 and 1.2 of base size\n\n### Bounce\nObject falls and bounces:\n- Use `interpolate()` with `easing='bounce_out'` for landing\n- Use `easing='ease_in'` for falling (accelerating)\n- Apply gravity by increasing y velocity each frame\n\n### Spin/Rotate\nRotate object around center:\n- PIL: `image.rotate(angle, resample=Image.BICUBIC)`\n- For wobble: use sine wave for angle instead of linear\n\n### Fade In/Out\nGradually appear or disappear:\n- Create RGBA image, adjust alpha channel\n- Or use `Image.blend(image1, image2, alpha)`\n- Fade in: alpha from 0 to 1\n- Fade out: alpha from 1 to 0\n\n### Slide\nMove object from off-screen to position:\n- Start position: outside frame bounds\n- End position: target location\n- Use `interpolate()` with `easing='ease_out'` for smooth stop\n- For overshoot: use `easing='back_out'`\n\n### Zoom\nScale and position for zoom effect:\n- Zoom in: scale from 0.1 to 2.0, crop center\n- Zoom out: scale from 2.0 to 1.0\n- Can add motion blur for drama (PIL filter)\n\n### Explode/Particle Burst\nCreate particles radiating outward:\n- Generate particles with random angles and velocities\n- Update each particle: `x += vx`, `y += vy`\n- Add gravity: `vy += gravity_constant`\n- Fade out particles over time (reduce alpha)\n\n## Optimization Strategies\n\nOnly when asked to make the file size smaller, implement a few of the following methods:\n\n1. **Fewer frames** - Lower FPS (10 instead of 20) or shorter duration\n2. **Fewer colors** - `num_colors=48` instead of 128\n3. **Smaller dimensions** - 128x128 instead of 480x480\n4. **Remove duplicates** - `remove_duplicates=True` in save()\n5. **Emoji mode** - `optimize_for_emoji=True` auto-optimizes\n\n```python\n# Maximum optimization for emoji\nbuilder.save(\n    'emoji.gif',\n    num_colors=48,\n    optimize_for_emoji=True,\n    remove_duplicates=True\n)\n```\n\n## Philosophy\n\nThis skill provides:\n- **Knowledge**: Slack's requirements and animation concepts\n- **Utilities**: GIFBuilder, validators, easing functions\n- **Flexibility**: Create the animation logic using PIL primitives\n\nIt does NOT provide:\n- Rigid animation templates or pre-made functions\n- Emoji font rendering (unreliable across platforms)\n- A library of pre-packaged graphics built into the skill\n\n**Note on user uploads**: This skill doesn't include pre-built graphics, but if a user uploads an image, use PIL to load and work with it - interpret based on their request whether they want it used directly or just as inspiration.\n\nBe creative! Combine concepts (bouncing + rotating, pulsing + sliding, etc.) and use PIL's full capabilities.\n\n## Dependencies\n\n```bash\npip install pillow imageio numpy\n```\n",
      "frontmatter": {
        "name": "slack-gif-creator",
        "description": "Knowledge and utilities for creating animated GIFs optimized for Slack. Provides constraints, validation tools, and animation concepts. Use when users request animated GIFs for Slack like \"make me a GIF of X doing Y for Slack.\"",
        "license": "Complete terms in LICENSE.txt"
      },
      "content": "\n# Slack GIF Creator\n\nA toolkit providing utilities and knowledge for creating animated GIFs optimized for Slack.\n\n## Slack Requirements\n\n**Dimensions:**\n- Emoji GIFs: 128x128 (recommended)\n- Message GIFs: 480x480\n\n**Parameters:**\n- FPS: 10-30 (lower is smaller file size)\n- Colors: 48-128 (fewer = smaller file size)\n- Duration: Keep under 3 seconds for emoji GIFs\n\n## Core Workflow\n\n```python\nfrom core.gif_builder import GIFBuilder\nfrom PIL import Image, ImageDraw\n\n# 1. Create builder\nbuilder = GIFBuilder(width=128, height=128, fps=10)\n\n# 2. Generate frames\nfor i in range(12):\n    frame = Image.new('RGB', (128, 128), (240, 248, 255))\n    draw = ImageDraw.Draw(frame)\n\n    # Draw your animation using PIL primitives\n    # (circles, polygons, lines, etc.)\n\n    builder.add_frame(frame)\n\n# 3. Save with optimization\nbuilder.save('output.gif', num_colors=48, optimize_for_emoji=True)\n```\n\n## Drawing Graphics\n\n### Working with User-Uploaded Images\nIf a user uploads an image, consider whether they want to:\n- **Use it directly** (e.g., \"animate this\", \"split this into frames\")\n- **Use it as inspiration** (e.g., \"make something like this\")\n\nLoad and work with images using PIL:\n```python\nfrom PIL import Image\n\nuploaded = Image.open('file.png')\n# Use directly, or just as reference for colors/style\n```\n\n### Drawing from Scratch\nWhen drawing graphics from scratch, use PIL ImageDraw primitives:\n\n```python\nfrom PIL import ImageDraw\n\ndraw = ImageDraw.Draw(frame)\n\n# Circles/ovals\ndraw.ellipse([x1, y1, x2, y2], fill=(r, g, b), outline=(r, g, b), width=3)\n\n# Stars, triangles, any polygon\npoints = [(x1, y1), (x2, y2), (x3, y3), ...]\ndraw.polygon(points, fill=(r, g, b), outline=(r, g, b), width=3)\n\n# Lines\ndraw.line([(x1, y1), (x2, y2)], fill=(r, g, b), width=5)\n\n# Rectangles\ndraw.rectangle([x1, y1, x2, y2], fill=(r, g, b), outline=(r, g, b), width=3)\n```\n\n**Don't use:** Emoji fonts (unreliable across platforms) or assume pre-packaged graphics exist in this skill.\n\n### Making Graphics Look Good\n\nGraphics should look polished and creative, not basic. Here's how:\n\n**Use thicker lines** - Always set `width=2` or higher for outlines and lines. Thin lines (width=1) look choppy and amateurish.\n\n**Add visual depth**:\n- Use gradients for backgrounds (`create_gradient_background`)\n- Layer multiple shapes for complexity (e.g., a star with a smaller star inside)\n\n**Make shapes more interesting**:\n- Don't just draw a plain circle - add highlights, rings, or patterns\n- Stars can have glows (draw larger, semi-transparent versions behind)\n- Combine multiple shapes (stars + sparkles, circles + rings)\n\n**Pay attention to colors**:\n- Use vibrant, complementary colors\n- Add contrast (dark outlines on light shapes, light outlines on dark shapes)\n- Consider the overall composition\n\n**For complex shapes** (hearts, snowflakes, etc.):\n- Use combinations of polygons and ellipses\n- Calculate points carefully for symmetry\n- Add details (a heart can have a highlight curve, snowflakes have intricate branches)\n\nBe creative and detailed! A good Slack GIF should look polished, not like placeholder graphics.\n\n## Available Utilities\n\n### GIFBuilder (`core.gif_builder`)\nAssembles frames and optimizes for Slack:\n```python\nbuilder = GIFBuilder(width=128, height=128, fps=10)\nbuilder.add_frame(frame)  # Add PIL Image\nbuilder.add_frames(frames)  # Add list of frames\nbuilder.save('out.gif', num_colors=48, optimize_for_emoji=True, remove_duplicates=True)\n```\n\n### Validators (`core.validators`)\nCheck if GIF meets Slack requirements:\n```python\nfrom core.validators import validate_gif, is_slack_ready\n\n# Detailed validation\npasses, info = validate_gif('my.gif', is_emoji=True, verbose=True)\n\n# Quick check\nif is_slack_ready('my.gif'):\n    print(\"Ready!\")\n```\n\n### Easing Functions (`core.easing`)\nSmooth motion instead of linear:\n```python\nfrom core.easing import interpolate\n\n# Progress from 0.0 to 1.0\nt = i / (num_frames - 1)\n\n# Apply easing\ny = interpolate(start=0, end=400, t=t, easing='ease_out')\n\n# Available: linear, ease_in, ease_out, ease_in_out,\n#           bounce_out, elastic_out, back_out\n```\n\n### Frame Helpers (`core.frame_composer`)\nConvenience functions for common needs:\n```python\nfrom core.frame_composer import (\n    create_blank_frame,         # Solid color background\n    create_gradient_background,  # Vertical gradient\n    draw_circle,                # Helper for circles\n    draw_text,                  # Simple text rendering\n    draw_star                   # 5-pointed star\n)\n```\n\n## Animation Concepts\n\n### Shake/Vibrate\nOffset object position with oscillation:\n- Use `math.sin()` or `math.cos()` with frame index\n- Add small random variations for natural feel\n- Apply to x and/or y position\n\n### Pulse/Heartbeat\nScale object size rhythmically:\n- Use `math.sin(t * frequency * 2 * math.pi)` for smooth pulse\n- For heartbeat: two quick pulses then pause (adjust sine wave)\n- Scale between 0.8 and 1.2 of base size\n\n### Bounce\nObject falls and bounces:\n- Use `interpolate()` with `easing='bounce_out'` for landing\n- Use `easing='ease_in'` for falling (accelerating)\n- Apply gravity by increasing y velocity each frame\n\n### Spin/Rotate\nRotate object around center:\n- PIL: `image.rotate(angle, resample=Image.BICUBIC)`\n- For wobble: use sine wave for angle instead of linear\n\n### Fade In/Out\nGradually appear or disappear:\n- Create RGBA image, adjust alpha channel\n- Or use `Image.blend(image1, image2, alpha)`\n- Fade in: alpha from 0 to 1\n- Fade out: alpha from 1 to 0\n\n### Slide\nMove object from off-screen to position:\n- Start position: outside frame bounds\n- End position: target location\n- Use `interpolate()` with `easing='ease_out'` for smooth stop\n- For overshoot: use `easing='back_out'`\n\n### Zoom\nScale and position for zoom effect:\n- Zoom in: scale from 0.1 to 2.0, crop center\n- Zoom out: scale from 2.0 to 1.0\n- Can add motion blur for drama (PIL filter)\n\n### Explode/Particle Burst\nCreate particles radiating outward:\n- Generate particles with random angles and velocities\n- Update each particle: `x += vx`, `y += vy`\n- Add gravity: `vy += gravity_constant`\n- Fade out particles over time (reduce alpha)\n\n## Optimization Strategies\n\nOnly when asked to make the file size smaller, implement a few of the following methods:\n\n1. **Fewer frames** - Lower FPS (10 instead of 20) or shorter duration\n2. **Fewer colors** - `num_colors=48` instead of 128\n3. **Smaller dimensions** - 128x128 instead of 480x480\n4. **Remove duplicates** - `remove_duplicates=True` in save()\n5. **Emoji mode** - `optimize_for_emoji=True` auto-optimizes\n\n```python\n# Maximum optimization for emoji\nbuilder.save(\n    'emoji.gif',\n    num_colors=48,\n    optimize_for_emoji=True,\n    remove_duplicates=True\n)\n```\n\n## Philosophy\n\nThis skill provides:\n- **Knowledge**: Slack's requirements and animation concepts\n- **Utilities**: GIFBuilder, validators, easing functions\n- **Flexibility**: Create the animation logic using PIL primitives\n\nIt does NOT provide:\n- Rigid animation templates or pre-made functions\n- Emoji font rendering (unreliable across platforms)\n- A library of pre-packaged graphics built into the skill\n\n**Note on user uploads**: This skill doesn't include pre-built graphics, but if a user uploads an image, use PIL to load and work with it - interpret based on their request whether they want it used directly or just as inspiration.\n\nBe creative! Combine concepts (bouncing + rotating, pulsing + sliding, etc.) and use PIL's full capabilities.\n\n## Dependencies\n\n```bash\npip install pillow imageio numpy\n```\n"
    }
  },
  "anthropics-skills-theme-factory": {
    "id": "anthropics-skills-theme-factory",
    "name": "theme-factory",
    "description": "Toolkit for styling artifacts with a theme. These artifacts can be slides, docs, reportings, HTML landing pages, etc. There are 10 pre-set themes with colors/fonts that you can apply to any artifact that has been creating, or can generate a new theme on-the-fly.",
    "repo": {
      "owner": "anthropics",
      "name": "skills",
      "fullName": "anthropics/skills",
      "url": "https://github.com/anthropics/skills/tree/main/skills/theme-factory",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 35374,
      "forks": 3202,
      "language": "TypeScript",
      "topics": [],
      "updatedAt": "2026-01-08T06:00:29Z",
      "pushedAt": "2025-12-20T18:09:45Z"
    },
    "category": "official",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: theme-factory\ndescription: Toolkit for styling artifacts with a theme. These artifacts can be slides, docs, reportings, HTML landing pages, etc. There are 10 pre-set themes with colors/fonts that you can apply to any artifact that has been creating, or can generate a new theme on-the-fly.\nlicense: Complete terms in LICENSE.txt\n---\n\n\n# Theme Factory Skill\n\nThis skill provides a curated collection of professional font and color themes themes, each with carefully selected color palettes and font pairings. Once a theme is chosen, it can be applied to any artifact.\n\n## Purpose\n\nTo apply consistent, professional styling to presentation slide decks, use this skill. Each theme includes:\n- A cohesive color palette with hex codes\n- Complementary font pairings for headers and body text\n- A distinct visual identity suitable for different contexts and audiences\n\n## Usage Instructions\n\nTo apply styling to a slide deck or other artifact:\n\n1. **Show the theme showcase**: Display the `theme-showcase.pdf` file to allow users to see all available themes visually. Do not make any modifications to it; simply show the file for viewing.\n2. **Ask for their choice**: Ask which theme to apply to the deck\n3. **Wait for selection**: Get explicit confirmation about the chosen theme\n4. **Apply the theme**: Once a theme has been chosen, apply the selected theme's colors and fonts to the deck/artifact\n\n## Themes Available\n\nThe following 10 themes are available, each showcased in `theme-showcase.pdf`:\n\n1. **Ocean Depths** - Professional and calming maritime theme\n2. **Sunset Boulevard** - Warm and vibrant sunset colors\n3. **Forest Canopy** - Natural and grounded earth tones\n4. **Modern Minimalist** - Clean and contemporary grayscale\n5. **Golden Hour** - Rich and warm autumnal palette\n6. **Arctic Frost** - Cool and crisp winter-inspired theme\n7. **Desert Rose** - Soft and sophisticated dusty tones\n8. **Tech Innovation** - Bold and modern tech aesthetic\n9. **Botanical Garden** - Fresh and organic garden colors\n10. **Midnight Galaxy** - Dramatic and cosmic deep tones\n\n## Theme Details\n\nEach theme is defined in the `themes/` directory with complete specifications including:\n- Cohesive color palette with hex codes\n- Complementary font pairings for headers and body text\n- Distinct visual identity suitable for different contexts and audiences\n\n## Application Process\n\nAfter a preferred theme is selected:\n1. Read the corresponding theme file from the `themes/` directory\n2. Apply the specified colors and fonts consistently throughout the deck\n3. Ensure proper contrast and readability\n4. Maintain the theme's visual identity across all slides\n\n## Create your Own Theme\nTo handle cases where none of the existing themes work for an artifact, create a custom theme. Based on provided inputs, generate a new theme similar to the ones above. Give the theme a similar name describing what the font/color combinations represent. Use any basic description provided to choose appropriate colors/fonts. After generating the theme, show it for review and verification. Following that, apply the theme as described above.\n",
      "frontmatter": {
        "name": "theme-factory",
        "description": "Toolkit for styling artifacts with a theme. These artifacts can be slides, docs, reportings, HTML landing pages, etc. There are 10 pre-set themes with colors/fonts that you can apply to any artifact that has been creating, or can generate a new theme on-the-fly.",
        "license": "Complete terms in LICENSE.txt"
      },
      "content": "\n\n# Theme Factory Skill\n\nThis skill provides a curated collection of professional font and color themes themes, each with carefully selected color palettes and font pairings. Once a theme is chosen, it can be applied to any artifact.\n\n## Purpose\n\nTo apply consistent, professional styling to presentation slide decks, use this skill. Each theme includes:\n- A cohesive color palette with hex codes\n- Complementary font pairings for headers and body text\n- A distinct visual identity suitable for different contexts and audiences\n\n## Usage Instructions\n\nTo apply styling to a slide deck or other artifact:\n\n1. **Show the theme showcase**: Display the `theme-showcase.pdf` file to allow users to see all available themes visually. Do not make any modifications to it; simply show the file for viewing.\n2. **Ask for their choice**: Ask which theme to apply to the deck\n3. **Wait for selection**: Get explicit confirmation about the chosen theme\n4. **Apply the theme**: Once a theme has been chosen, apply the selected theme's colors and fonts to the deck/artifact\n\n## Themes Available\n\nThe following 10 themes are available, each showcased in `theme-showcase.pdf`:\n\n1. **Ocean Depths** - Professional and calming maritime theme\n2. **Sunset Boulevard** - Warm and vibrant sunset colors\n3. **Forest Canopy** - Natural and grounded earth tones\n4. **Modern Minimalist** - Clean and contemporary grayscale\n5. **Golden Hour** - Rich and warm autumnal palette\n6. **Arctic Frost** - Cool and crisp winter-inspired theme\n7. **Desert Rose** - Soft and sophisticated dusty tones\n8. **Tech Innovation** - Bold and modern tech aesthetic\n9. **Botanical Garden** - Fresh and organic garden colors\n10. **Midnight Galaxy** - Dramatic and cosmic deep tones\n\n## Theme Details\n\nEach theme is defined in the `themes/` directory with complete specifications including:\n- Cohesive color palette with hex codes\n- Complementary font pairings for headers and body text\n- Distinct visual identity suitable for different contexts and audiences\n\n## Application Process\n\nAfter a preferred theme is selected:\n1. Read the corresponding theme file from the `themes/` directory\n2. Apply the specified colors and fonts consistently throughout the deck\n3. Ensure proper contrast and readability\n4. Maintain the theme's visual identity across all slides\n\n## Create your Own Theme\nTo handle cases where none of the existing themes work for an artifact, create a custom theme. Based on provided inputs, generate a new theme similar to the ones above. Give the theme a similar name describing what the font/color combinations represent. Use any basic description provided to choose appropriate colors/fonts. After generating the theme, show it for review and verification. Following that, apply the theme as described above.\n"
    }
  },
  "anthropics-skills-web-artifacts-builder": {
    "id": "anthropics-skills-web-artifacts-builder",
    "name": "web-artifacts-builder",
    "description": "Suite of tools for creating elaborate, multi-component claude.ai HTML artifacts using modern frontend web technologies (React, Tailwind CSS, shadcn/ui). Use for complex artifacts requiring state management, routing, or shadcn/ui components - not for simple single-file HTML/JSX artifacts.",
    "repo": {
      "owner": "anthropics",
      "name": "skills",
      "fullName": "anthropics/skills",
      "url": "https://github.com/anthropics/skills/tree/main/skills/web-artifacts-builder",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 35375,
      "forks": 3202,
      "language": "TypeScript",
      "topics": [],
      "updatedAt": "2026-01-08T06:06:13Z",
      "pushedAt": "2025-12-20T18:09:45Z"
    },
    "category": "official",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: web-artifacts-builder\ndescription: Suite of tools for creating elaborate, multi-component claude.ai HTML artifacts using modern frontend web technologies (React, Tailwind CSS, shadcn/ui). Use for complex artifacts requiring state management, routing, or shadcn/ui components - not for simple single-file HTML/JSX artifacts.\nlicense: Complete terms in LICENSE.txt\n---\n\n# Web Artifacts Builder\n\nTo build powerful frontend claude.ai artifacts, follow these steps:\n1. Initialize the frontend repo using `scripts/init-artifact.sh`\n2. Develop your artifact by editing the generated code\n3. Bundle all code into a single HTML file using `scripts/bundle-artifact.sh`\n4. Display artifact to user\n5. (Optional) Test the artifact\n\n**Stack**: React 18 + TypeScript + Vite + Parcel (bundling) + Tailwind CSS + shadcn/ui\n\n## Design & Style Guidelines\n\nVERY IMPORTANT: To avoid what is often referred to as \"AI slop\", avoid using excessive centered layouts, purple gradients, uniform rounded corners, and Inter font.\n\n## Quick Start\n\n### Step 1: Initialize Project\n\nRun the initialization script to create a new React project:\n```bash\nbash scripts/init-artifact.sh <project-name>\ncd <project-name>\n```\n\nThis creates a fully configured project with:\n- âœ… React + TypeScript (via Vite)\n- âœ… Tailwind CSS 3.4.1 with shadcn/ui theming system\n- âœ… Path aliases (`@/`) configured\n- âœ… 40+ shadcn/ui components pre-installed\n- âœ… All Radix UI dependencies included\n- âœ… Parcel configured for bundling (via .parcelrc)\n- âœ… Node 18+ compatibility (auto-detects and pins Vite version)\n\n### Step 2: Develop Your Artifact\n\nTo build the artifact, edit the generated files. See **Common Development Tasks** below for guidance.\n\n### Step 3: Bundle to Single HTML File\n\nTo bundle the React app into a single HTML artifact:\n```bash\nbash scripts/bundle-artifact.sh\n```\n\nThis creates `bundle.html` - a self-contained artifact with all JavaScript, CSS, and dependencies inlined. This file can be directly shared in Claude conversations as an artifact.\n\n**Requirements**: Your project must have an `index.html` in the root directory.\n\n**What the script does**:\n- Installs bundling dependencies (parcel, @parcel/config-default, parcel-resolver-tspaths, html-inline)\n- Creates `.parcelrc` config with path alias support\n- Builds with Parcel (no source maps)\n- Inlines all assets into single HTML using html-inline\n\n### Step 4: Share Artifact with User\n\nFinally, share the bundled HTML file in conversation with the user so they can view it as an artifact.\n\n### Step 5: Testing/Visualizing the Artifact (Optional)\n\nNote: This is a completely optional step. Only perform if necessary or requested.\n\nTo test/visualize the artifact, use available tools (including other Skills or built-in tools like Playwright or Puppeteer). In general, avoid testing the artifact upfront as it adds latency between the request and when the finished artifact can be seen. Test later, after presenting the artifact, if requested or if issues arise.\n\n## Reference\n\n- **shadcn/ui components**: https://ui.shadcn.com/docs/components",
      "frontmatter": {
        "name": "web-artifacts-builder",
        "description": "Suite of tools for creating elaborate, multi-component claude.ai HTML artifacts using modern frontend web technologies (React, Tailwind CSS, shadcn/ui). Use for complex artifacts requiring state management, routing, or shadcn/ui components - not for simple single-file HTML/JSX artifacts.",
        "license": "Complete terms in LICENSE.txt"
      },
      "content": "\n# Web Artifacts Builder\n\nTo build powerful frontend claude.ai artifacts, follow these steps:\n1. Initialize the frontend repo using `scripts/init-artifact.sh`\n2. Develop your artifact by editing the generated code\n3. Bundle all code into a single HTML file using `scripts/bundle-artifact.sh`\n4. Display artifact to user\n5. (Optional) Test the artifact\n\n**Stack**: React 18 + TypeScript + Vite + Parcel (bundling) + Tailwind CSS + shadcn/ui\n\n## Design & Style Guidelines\n\nVERY IMPORTANT: To avoid what is often referred to as \"AI slop\", avoid using excessive centered layouts, purple gradients, uniform rounded corners, and Inter font.\n\n## Quick Start\n\n### Step 1: Initialize Project\n\nRun the initialization script to create a new React project:\n```bash\nbash scripts/init-artifact.sh <project-name>\ncd <project-name>\n```\n\nThis creates a fully configured project with:\n- âœ… React + TypeScript (via Vite)\n- âœ… Tailwind CSS 3.4.1 with shadcn/ui theming system\n- âœ… Path aliases (`@/`) configured\n- âœ… 40+ shadcn/ui components pre-installed\n- âœ… All Radix UI dependencies included\n- âœ… Parcel configured for bundling (via .parcelrc)\n- âœ… Node 18+ compatibility (auto-detects and pins Vite version)\n\n### Step 2: Develop Your Artifact\n\nTo build the artifact, edit the generated files. See **Common Development Tasks** below for guidance.\n\n### Step 3: Bundle to Single HTML File\n\nTo bundle the React app into a single HTML artifact:\n```bash\nbash scripts/bundle-artifact.sh\n```\n\nThis creates `bundle.html` - a self-contained artifact with all JavaScript, CSS, and dependencies inlined. This file can be directly shared in Claude conversations as an artifact.\n\n**Requirements**: Your project must have an `index.html` in the root directory.\n\n**What the script does**:\n- Installs bundling dependencies (parcel, @parcel/config-default, parcel-resolver-tspaths, html-inline)\n- Creates `.parcelrc` config with path alias support\n- Builds with Parcel (no source maps)\n- Inlines all assets into single HTML using html-inline\n\n### Step 4: Share Artifact with User\n\nFinally, share the bundled HTML file in conversation with the user so they can view it as an artifact.\n\n### Step 5: Testing/Visualizing the Artifact (Optional)\n\nNote: This is a completely optional step. Only perform if necessary or requested.\n\nTo test/visualize the artifact, use available tools (including other Skills or built-in tools like Playwright or Puppeteer). In general, avoid testing the artifact upfront as it adds latency between the request and when the finished artifact can be seen. Test later, after presenting the artifact, if requested or if issues arise.\n\n## Reference\n\n- **shadcn/ui components**: https://ui.shadcn.com/docs/components"
    }
  },
  "anthropics-skills-webapp-testing": {
    "id": "anthropics-skills-webapp-testing",
    "name": "webapp-testing",
    "description": "Toolkit for interacting with and testing local web applications using Playwright. Supports verifying frontend functionality, debugging UI behavior, capturing browser screenshots, and viewing browser logs.",
    "repo": {
      "owner": "anthropics",
      "name": "skills",
      "fullName": "anthropics/skills",
      "url": "https://github.com/anthropics/skills/tree/main/skills/webapp-testing",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 35375,
      "forks": 3202,
      "language": "TypeScript",
      "topics": [],
      "updatedAt": "2026-01-08T06:06:13Z",
      "pushedAt": "2025-12-20T18:09:45Z"
    },
    "category": "official",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: webapp-testing\ndescription: Toolkit for interacting with and testing local web applications using Playwright. Supports verifying frontend functionality, debugging UI behavior, capturing browser screenshots, and viewing browser logs.\nlicense: Complete terms in LICENSE.txt\n---\n\n# Web Application Testing\n\nTo test local web applications, write native Python Playwright scripts.\n\n**Helper Scripts Available**:\n- `scripts/with_server.py` - Manages server lifecycle (supports multiple servers)\n\n**Always run scripts with `--help` first** to see usage. DO NOT read the source until you try running the script first and find that a customized solution is abslutely necessary. These scripts can be very large and thus pollute your context window. They exist to be called directly as black-box scripts rather than ingested into your context window.\n\n## Decision Tree: Choosing Your Approach\n\n```\nUser task â†’ Is it static HTML?\n    â”œâ”€ Yes â†’ Read HTML file directly to identify selectors\n    â”‚         â”œâ”€ Success â†’ Write Playwright script using selectors\n    â”‚         â””â”€ Fails/Incomplete â†’ Treat as dynamic (below)\n    â”‚\n    â””â”€ No (dynamic webapp) â†’ Is the server already running?\n        â”œâ”€ No â†’ Run: python scripts/with_server.py --help\n        â”‚        Then use the helper + write simplified Playwright script\n        â”‚\n        â””â”€ Yes â†’ Reconnaissance-then-action:\n            1. Navigate and wait for networkidle\n            2. Take screenshot or inspect DOM\n            3. Identify selectors from rendered state\n            4. Execute actions with discovered selectors\n```\n\n## Example: Using with_server.py\n\nTo start a server, run `--help` first, then use the helper:\n\n**Single server:**\n```bash\npython scripts/with_server.py --server \"npm run dev\" --port 5173 -- python your_automation.py\n```\n\n**Multiple servers (e.g., backend + frontend):**\n```bash\npython scripts/with_server.py \\\n  --server \"cd backend && python server.py\" --port 3000 \\\n  --server \"cd frontend && npm run dev\" --port 5173 \\\n  -- python your_automation.py\n```\n\nTo create an automation script, include only Playwright logic (servers are managed automatically):\n```python\nfrom playwright.sync_api import sync_playwright\n\nwith sync_playwright() as p:\n    browser = p.chromium.launch(headless=True) # Always launch chromium in headless mode\n    page = browser.new_page()\n    page.goto('http://localhost:5173') # Server already running and ready\n    page.wait_for_load_state('networkidle') # CRITICAL: Wait for JS to execute\n    # ... your automation logic\n    browser.close()\n```\n\n## Reconnaissance-Then-Action Pattern\n\n1. **Inspect rendered DOM**:\n   ```python\n   page.screenshot(path='/tmp/inspect.png', full_page=True)\n   content = page.content()\n   page.locator('button').all()\n   ```\n\n2. **Identify selectors** from inspection results\n\n3. **Execute actions** using discovered selectors\n\n## Common Pitfall\n\nâŒ **Don't** inspect the DOM before waiting for `networkidle` on dynamic apps\nâœ… **Do** wait for `page.wait_for_load_state('networkidle')` before inspection\n\n## Best Practices\n\n- **Use bundled scripts as black boxes** - To accomplish a task, consider whether one of the scripts available in `scripts/` can help. These scripts handle common, complex workflows reliably without cluttering the context window. Use `--help` to see usage, then invoke directly. \n- Use `sync_playwright()` for synchronous scripts\n- Always close the browser when done\n- Use descriptive selectors: `text=`, `role=`, CSS selectors, or IDs\n- Add appropriate waits: `page.wait_for_selector()` or `page.wait_for_timeout()`\n\n## Reference Files\n\n- **examples/** - Examples showing common patterns:\n  - `element_discovery.py` - Discovering buttons, links, and inputs on a page\n  - `static_html_automation.py` - Using file:// URLs for local HTML\n  - `console_logging.py` - Capturing console logs during automation",
      "frontmatter": {
        "name": "webapp-testing",
        "description": "Toolkit for interacting with and testing local web applications using Playwright. Supports verifying frontend functionality, debugging UI behavior, capturing browser screenshots, and viewing browser logs.",
        "license": "Complete terms in LICENSE.txt"
      },
      "content": "\n# Web Application Testing\n\nTo test local web applications, write native Python Playwright scripts.\n\n**Helper Scripts Available**:\n- `scripts/with_server.py` - Manages server lifecycle (supports multiple servers)\n\n**Always run scripts with `--help` first** to see usage. DO NOT read the source until you try running the script first and find that a customized solution is abslutely necessary. These scripts can be very large and thus pollute your context window. They exist to be called directly as black-box scripts rather than ingested into your context window.\n\n## Decision Tree: Choosing Your Approach\n\n```\nUser task â†’ Is it static HTML?\n    â”œâ”€ Yes â†’ Read HTML file directly to identify selectors\n    â”‚         â”œâ”€ Success â†’ Write Playwright script using selectors\n    â”‚         â””â”€ Fails/Incomplete â†’ Treat as dynamic (below)\n    â”‚\n    â””â”€ No (dynamic webapp) â†’ Is the server already running?\n        â”œâ”€ No â†’ Run: python scripts/with_server.py --help\n        â”‚        Then use the helper + write simplified Playwright script\n        â”‚\n        â””â”€ Yes â†’ Reconnaissance-then-action:\n            1. Navigate and wait for networkidle\n            2. Take screenshot or inspect DOM\n            3. Identify selectors from rendered state\n            4. Execute actions with discovered selectors\n```\n\n## Example: Using with_server.py\n\nTo start a server, run `--help` first, then use the helper:\n\n**Single server:**\n```bash\npython scripts/with_server.py --server \"npm run dev\" --port 5173 -- python your_automation.py\n```\n\n**Multiple servers (e.g., backend + frontend):**\n```bash\npython scripts/with_server.py \\\n  --server \"cd backend && python server.py\" --port 3000 \\\n  --server \"cd frontend && npm run dev\" --port 5173 \\\n  -- python your_automation.py\n```\n\nTo create an automation script, include only Playwright logic (servers are managed automatically):\n```python\nfrom playwright.sync_api import sync_playwright\n\nwith sync_playwright() as p:\n    browser = p.chromium.launch(headless=True) # Always launch chromium in headless mode\n    page = browser.new_page()\n    page.goto('http://localhost:5173') # Server already running and ready\n    page.wait_for_load_state('networkidle') # CRITICAL: Wait for JS to execute\n    # ... your automation logic\n    browser.close()\n```\n\n## Reconnaissance-Then-Action Pattern\n\n1. **Inspect rendered DOM**:\n   ```python\n   page.screenshot(path='/tmp/inspect.png', full_page=True)\n   content = page.content()\n   page.locator('button').all()\n   ```\n\n2. **Identify selectors** from inspection results\n\n3. **Execute actions** using discovered selectors\n\n## Common Pitfall\n\nâŒ **Don't** inspect the DOM before waiting for `networkidle` on dynamic apps\nâœ… **Do** wait for `page.wait_for_load_state('networkidle')` before inspection\n\n## Best Practices\n\n- **Use bundled scripts as black boxes** - To accomplish a task, consider whether one of the scripts available in `scripts/` can help. These scripts handle common, complex workflows reliably without cluttering the context window. Use `--help` to see usage, then invoke directly. \n- Use `sync_playwright()` for synchronous scripts\n- Always close the browser when done\n- Use descriptive selectors: `text=`, `role=`, CSS selectors, or IDs\n- Add appropriate waits: `page.wait_for_selector()` or `page.wait_for_timeout()`\n\n## Reference Files\n\n- **examples/** - Examples showing common patterns:\n  - `element_discovery.py` - Discovering buttons, links, and inputs on a page\n  - `static_html_automation.py` - Using file:// URLs for local HTML\n  - `console_logging.py` - Capturing console logs during automation"
    }
  },
  "anthropics-skills-xlsx": {
    "id": "anthropics-skills-xlsx",
    "name": "xlsx",
    "description": "Comprehensive spreadsheet creation, editing, and analysis with support for formulas, formatting, data analysis, and visualization. When Claude needs to work with spreadsheets (.xlsx, .xlsm, .csv, .tsv, etc) for: (1) Creating new spreadsheets with formulas and formatting, (2) Reading or analyzing data, (3) Modify existing spreadsheets while preserving formulas, (4) Data analysis and visualization in spreadsheets, or (5) Recalculating formulas",
    "repo": {
      "owner": "anthropics",
      "name": "skills",
      "fullName": "anthropics/skills",
      "url": "https://github.com/anthropics/skills/tree/main/skills/xlsx",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 35375,
      "forks": 3202,
      "language": "TypeScript",
      "topics": [],
      "updatedAt": "2026-01-08T06:06:13Z",
      "pushedAt": "2025-12-20T18:09:45Z"
    },
    "category": "official",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: xlsx\ndescription: \"Comprehensive spreadsheet creation, editing, and analysis with support for formulas, formatting, data analysis, and visualization. When Claude needs to work with spreadsheets (.xlsx, .xlsm, .csv, .tsv, etc) for: (1) Creating new spreadsheets with formulas and formatting, (2) Reading or analyzing data, (3) Modify existing spreadsheets while preserving formulas, (4) Data analysis and visualization in spreadsheets, or (5) Recalculating formulas\"\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Requirements for Outputs\n\n## All Excel files\n\n### Zero Formula Errors\n- Every Excel model MUST be delivered with ZERO formula errors (#REF!, #DIV/0!, #VALUE!, #N/A, #NAME?)\n\n### Preserve Existing Templates (when updating templates)\n- Study and EXACTLY match existing format, style, and conventions when modifying files\n- Never impose standardized formatting on files with established patterns\n- Existing template conventions ALWAYS override these guidelines\n\n## Financial models\n\n### Color Coding Standards\nUnless otherwise stated by the user or existing template\n\n#### Industry-Standard Color Conventions\n- **Blue text (RGB: 0,0,255)**: Hardcoded inputs, and numbers users will change for scenarios\n- **Black text (RGB: 0,0,0)**: ALL formulas and calculations\n- **Green text (RGB: 0,128,0)**: Links pulling from other worksheets within same workbook\n- **Red text (RGB: 255,0,0)**: External links to other files\n- **Yellow background (RGB: 255,255,0)**: Key assumptions needing attention or cells that need to be updated\n\n### Number Formatting Standards\n\n#### Required Format Rules\n- **Years**: Format as text strings (e.g., \"2024\" not \"2,024\")\n- **Currency**: Use $#,##0 format; ALWAYS specify units in headers (\"Revenue ($mm)\")\n- **Zeros**: Use number formatting to make all zeros \"-\", including percentages (e.g., \"$#,##0;($#,##0);-\")\n- **Percentages**: Default to 0.0% format (one decimal)\n- **Multiples**: Format as 0.0x for valuation multiples (EV/EBITDA, P/E)\n- **Negative numbers**: Use parentheses (123) not minus -123\n\n### Formula Construction Rules\n\n#### Assumptions Placement\n- Place ALL assumptions (growth rates, margins, multiples, etc.) in separate assumption cells\n- Use cell references instead of hardcoded values in formulas\n- Example: Use =B5*(1+$B$6) instead of =B5*1.05\n\n#### Formula Error Prevention\n- Verify all cell references are correct\n- Check for off-by-one errors in ranges\n- Ensure consistent formulas across all projection periods\n- Test with edge cases (zero values, negative numbers)\n- Verify no unintended circular references\n\n#### Documentation Requirements for Hardcodes\n- Comment or in cells beside (if end of table). Format: \"Source: [System/Document], [Date], [Specific Reference], [URL if applicable]\"\n- Examples:\n  - \"Source: Company 10-K, FY2024, Page 45, Revenue Note, [SEC EDGAR URL]\"\n  - \"Source: Company 10-Q, Q2 2025, Exhibit 99.1, [SEC EDGAR URL]\"\n  - \"Source: Bloomberg Terminal, 8/15/2025, AAPL US Equity\"\n  - \"Source: FactSet, 8/20/2025, Consensus Estimates Screen\"\n\n# XLSX creation, editing, and analysis\n\n## Overview\n\nA user may ask you to create, edit, or analyze the contents of an .xlsx file. You have different tools and workflows available for different tasks.\n\n## Important Requirements\n\n**LibreOffice Required for Formula Recalculation**: You can assume LibreOffice is installed for recalculating formula values using the `recalc.py` script. The script automatically configures LibreOffice on first run\n\n## Reading and analyzing data\n\n### Data analysis with pandas\nFor data analysis, visualization, and basic operations, use **pandas** which provides powerful data manipulation capabilities:\n\n```python\nimport pandas as pd\n\n# Read Excel\ndf = pd.read_excel('file.xlsx')  # Default: first sheet\nall_sheets = pd.read_excel('file.xlsx', sheet_name=None)  # All sheets as dict\n\n# Analyze\ndf.head()      # Preview data\ndf.info()      # Column info\ndf.describe()  # Statistics\n\n# Write Excel\ndf.to_excel('output.xlsx', index=False)\n```\n\n## Excel File Workflows\n\n## CRITICAL: Use Formulas, Not Hardcoded Values\n\n**Always use Excel formulas instead of calculating values in Python and hardcoding them.** This ensures the spreadsheet remains dynamic and updateable.\n\n### âŒ WRONG - Hardcoding Calculated Values\n```python\n# Bad: Calculating in Python and hardcoding result\ntotal = df['Sales'].sum()\nsheet['B10'] = total  # Hardcodes 5000\n\n# Bad: Computing growth rate in Python\ngrowth = (df.iloc[-1]['Revenue'] - df.iloc[0]['Revenue']) / df.iloc[0]['Revenue']\nsheet['C5'] = growth  # Hardcodes 0.15\n\n# Bad: Python calculation for average\navg = sum(values) / len(values)\nsheet['D20'] = avg  # Hardcodes 42.5\n```\n\n### âœ… CORRECT - Using Excel Formulas\n```python\n# Good: Let Excel calculate the sum\nsheet['B10'] = '=SUM(B2:B9)'\n\n# Good: Growth rate as Excel formula\nsheet['C5'] = '=(C4-C2)/C2'\n\n# Good: Average using Excel function\nsheet['D20'] = '=AVERAGE(D2:D19)'\n```\n\nThis applies to ALL calculations - totals, percentages, ratios, differences, etc. The spreadsheet should be able to recalculate when source data changes.\n\n## Common Workflow\n1. **Choose tool**: pandas for data, openpyxl for formulas/formatting\n2. **Create/Load**: Create new workbook or load existing file\n3. **Modify**: Add/edit data, formulas, and formatting\n4. **Save**: Write to file\n5. **Recalculate formulas (MANDATORY IF USING FORMULAS)**: Use the recalc.py script\n   ```bash\n   python recalc.py output.xlsx\n   ```\n6. **Verify and fix any errors**: \n   - The script returns JSON with error details\n   - If `status` is `errors_found`, check `error_summary` for specific error types and locations\n   - Fix the identified errors and recalculate again\n   - Common errors to fix:\n     - `#REF!`: Invalid cell references\n     - `#DIV/0!`: Division by zero\n     - `#VALUE!`: Wrong data type in formula\n     - `#NAME?`: Unrecognized formula name\n\n### Creating new Excel files\n\n```python\n# Using openpyxl for formulas and formatting\nfrom openpyxl import Workbook\nfrom openpyxl.styles import Font, PatternFill, Alignment\n\nwb = Workbook()\nsheet = wb.active\n\n# Add data\nsheet['A1'] = 'Hello'\nsheet['B1'] = 'World'\nsheet.append(['Row', 'of', 'data'])\n\n# Add formula\nsheet['B2'] = '=SUM(A1:A10)'\n\n# Formatting\nsheet['A1'].font = Font(bold=True, color='FF0000')\nsheet['A1'].fill = PatternFill('solid', start_color='FFFF00')\nsheet['A1'].alignment = Alignment(horizontal='center')\n\n# Column width\nsheet.column_dimensions['A'].width = 20\n\nwb.save('output.xlsx')\n```\n\n### Editing existing Excel files\n\n```python\n# Using openpyxl to preserve formulas and formatting\nfrom openpyxl import load_workbook\n\n# Load existing file\nwb = load_workbook('existing.xlsx')\nsheet = wb.active  # or wb['SheetName'] for specific sheet\n\n# Working with multiple sheets\nfor sheet_name in wb.sheetnames:\n    sheet = wb[sheet_name]\n    print(f\"Sheet: {sheet_name}\")\n\n# Modify cells\nsheet['A1'] = 'New Value'\nsheet.insert_rows(2)  # Insert row at position 2\nsheet.delete_cols(3)  # Delete column 3\n\n# Add new sheet\nnew_sheet = wb.create_sheet('NewSheet')\nnew_sheet['A1'] = 'Data'\n\nwb.save('modified.xlsx')\n```\n\n## Recalculating formulas\n\nExcel files created or modified by openpyxl contain formulas as strings but not calculated values. Use the provided `recalc.py` script to recalculate formulas:\n\n```bash\npython recalc.py <excel_file> [timeout_seconds]\n```\n\nExample:\n```bash\npython recalc.py output.xlsx 30\n```\n\nThe script:\n- Automatically sets up LibreOffice macro on first run\n- Recalculates all formulas in all sheets\n- Scans ALL cells for Excel errors (#REF!, #DIV/0!, etc.)\n- Returns JSON with detailed error locations and counts\n- Works on both Linux and macOS\n\n## Formula Verification Checklist\n\nQuick checks to ensure formulas work correctly:\n\n### Essential Verification\n- [ ] **Test 2-3 sample references**: Verify they pull correct values before building full model\n- [ ] **Column mapping**: Confirm Excel columns match (e.g., column 64 = BL, not BK)\n- [ ] **Row offset**: Remember Excel rows are 1-indexed (DataFrame row 5 = Excel row 6)\n\n### Common Pitfalls\n- [ ] **NaN handling**: Check for null values with `pd.notna()`\n- [ ] **Far-right columns**: FY data often in columns 50+ \n- [ ] **Multiple matches**: Search all occurrences, not just first\n- [ ] **Division by zero**: Check denominators before using `/` in formulas (#DIV/0!)\n- [ ] **Wrong references**: Verify all cell references point to intended cells (#REF!)\n- [ ] **Cross-sheet references**: Use correct format (Sheet1!A1) for linking sheets\n\n### Formula Testing Strategy\n- [ ] **Start small**: Test formulas on 2-3 cells before applying broadly\n- [ ] **Verify dependencies**: Check all cells referenced in formulas exist\n- [ ] **Test edge cases**: Include zero, negative, and very large values\n\n### Interpreting recalc.py Output\nThe script returns JSON with error details:\n```json\n{\n  \"status\": \"success\",           // or \"errors_found\"\n  \"total_errors\": 0,              // Total error count\n  \"total_formulas\": 42,           // Number of formulas in file\n  \"error_summary\": {              // Only present if errors found\n    \"#REF!\": {\n      \"count\": 2,\n      \"locations\": [\"Sheet1!B5\", \"Sheet1!C10\"]\n    }\n  }\n}\n```\n\n## Best Practices\n\n### Library Selection\n- **pandas**: Best for data analysis, bulk operations, and simple data export\n- **openpyxl**: Best for complex formatting, formulas, and Excel-specific features\n\n### Working with openpyxl\n- Cell indices are 1-based (row=1, column=1 refers to cell A1)\n- Use `data_only=True` to read calculated values: `load_workbook('file.xlsx', data_only=True)`\n- **Warning**: If opened with `data_only=True` and saved, formulas are replaced with values and permanently lost\n- For large files: Use `read_only=True` for reading or `write_only=True` for writing\n- Formulas are preserved but not evaluated - use recalc.py to update values\n\n### Working with pandas\n- Specify data types to avoid inference issues: `pd.read_excel('file.xlsx', dtype={'id': str})`\n- For large files, read specific columns: `pd.read_excel('file.xlsx', usecols=['A', 'C', 'E'])`\n- Handle dates properly: `pd.read_excel('file.xlsx', parse_dates=['date_column'])`\n\n## Code Style Guidelines\n**IMPORTANT**: When generating Python code for Excel operations:\n- Write minimal, concise Python code without unnecessary comments\n- Avoid verbose variable names and redundant operations\n- Avoid unnecessary print statements\n\n**For Excel files themselves**:\n- Add comments to cells with complex formulas or important assumptions\n- Document data sources for hardcoded values\n- Include notes for key calculations and model sections",
      "frontmatter": {
        "name": "xlsx",
        "description": "Comprehensive spreadsheet creation, editing, and analysis with support for formulas, formatting, data analysis, and visualization. When Claude needs to work with spreadsheets (.xlsx, .xlsm, .csv, .tsv, etc) for: (1) Creating new spreadsheets with formulas and formatting, (2) Reading or analyzing data, (3) Modify existing spreadsheets while preserving formulas, (4) Data analysis and visualization in spreadsheets, or (5) Recalculating formulas",
        "license": "Proprietary. LICENSE.txt has complete terms"
      },
      "content": "\n# Requirements for Outputs\n\n## All Excel files\n\n### Zero Formula Errors\n- Every Excel model MUST be delivered with ZERO formula errors (#REF!, #DIV/0!, #VALUE!, #N/A, #NAME?)\n\n### Preserve Existing Templates (when updating templates)\n- Study and EXACTLY match existing format, style, and conventions when modifying files\n- Never impose standardized formatting on files with established patterns\n- Existing template conventions ALWAYS override these guidelines\n\n## Financial models\n\n### Color Coding Standards\nUnless otherwise stated by the user or existing template\n\n#### Industry-Standard Color Conventions\n- **Blue text (RGB: 0,0,255)**: Hardcoded inputs, and numbers users will change for scenarios\n- **Black text (RGB: 0,0,0)**: ALL formulas and calculations\n- **Green text (RGB: 0,128,0)**: Links pulling from other worksheets within same workbook\n- **Red text (RGB: 255,0,0)**: External links to other files\n- **Yellow background (RGB: 255,255,0)**: Key assumptions needing attention or cells that need to be updated\n\n### Number Formatting Standards\n\n#### Required Format Rules\n- **Years**: Format as text strings (e.g., \"2024\" not \"2,024\")\n- **Currency**: Use $#,##0 format; ALWAYS specify units in headers (\"Revenue ($mm)\")\n- **Zeros**: Use number formatting to make all zeros \"-\", including percentages (e.g., \"$#,##0;($#,##0);-\")\n- **Percentages**: Default to 0.0% format (one decimal)\n- **Multiples**: Format as 0.0x for valuation multiples (EV/EBITDA, P/E)\n- **Negative numbers**: Use parentheses (123) not minus -123\n\n### Formula Construction Rules\n\n#### Assumptions Placement\n- Place ALL assumptions (growth rates, margins, multiples, etc.) in separate assumption cells\n- Use cell references instead of hardcoded values in formulas\n- Example: Use =B5*(1+$B$6) instead of =B5*1.05\n\n#### Formula Error Prevention\n- Verify all cell references are correct\n- Check for off-by-one errors in ranges\n- Ensure consistent formulas across all projection periods\n- Test with edge cases (zero values, negative numbers)\n- Verify no unintended circular references\n\n#### Documentation Requirements for Hardcodes\n- Comment or in cells beside (if end of table). Format: \"Source: [System/Document], [Date], [Specific Reference], [URL if applicable]\"\n- Examples:\n  - \"Source: Company 10-K, FY2024, Page 45, Revenue Note, [SEC EDGAR URL]\"\n  - \"Source: Company 10-Q, Q2 2025, Exhibit 99.1, [SEC EDGAR URL]\"\n  - \"Source: Bloomberg Terminal, 8/15/2025, AAPL US Equity\"\n  - \"Source: FactSet, 8/20/2025, Consensus Estimates Screen\"\n\n# XLSX creation, editing, and analysis\n\n## Overview\n\nA user may ask you to create, edit, or analyze the contents of an .xlsx file. You have different tools and workflows available for different tasks.\n\n## Important Requirements\n\n**LibreOffice Required for Formula Recalculation**: You can assume LibreOffice is installed for recalculating formula values using the `recalc.py` script. The script automatically configures LibreOffice on first run\n\n## Reading and analyzing data\n\n### Data analysis with pandas\nFor data analysis, visualization, and basic operations, use **pandas** which provides powerful data manipulation capabilities:\n\n```python\nimport pandas as pd\n\n# Read Excel\ndf = pd.read_excel('file.xlsx')  # Default: first sheet\nall_sheets = pd.read_excel('file.xlsx', sheet_name=None)  # All sheets as dict\n\n# Analyze\ndf.head()      # Preview data\ndf.info()      # Column info\ndf.describe()  # Statistics\n\n# Write Excel\ndf.to_excel('output.xlsx', index=False)\n```\n\n## Excel File Workflows\n\n## CRITICAL: Use Formulas, Not Hardcoded Values\n\n**Always use Excel formulas instead of calculating values in Python and hardcoding them.** This ensures the spreadsheet remains dynamic and updateable.\n\n### âŒ WRONG - Hardcoding Calculated Values\n```python\n# Bad: Calculating in Python and hardcoding result\ntotal = df['Sales'].sum()\nsheet['B10'] = total  # Hardcodes 5000\n\n# Bad: Computing growth rate in Python\ngrowth = (df.iloc[-1]['Revenue'] - df.iloc[0]['Revenue']) / df.iloc[0]['Revenue']\nsheet['C5'] = growth  # Hardcodes 0.15\n\n# Bad: Python calculation for average\navg = sum(values) / len(values)\nsheet['D20'] = avg  # Hardcodes 42.5\n```\n\n### âœ… CORRECT - Using Excel Formulas\n```python\n# Good: Let Excel calculate the sum\nsheet['B10'] = '=SUM(B2:B9)'\n\n# Good: Growth rate as Excel formula\nsheet['C5'] = '=(C4-C2)/C2'\n\n# Good: Average using Excel function\nsheet['D20'] = '=AVERAGE(D2:D19)'\n```\n\nThis applies to ALL calculations - totals, percentages, ratios, differences, etc. The spreadsheet should be able to recalculate when source data changes.\n\n## Common Workflow\n1. **Choose tool**: pandas for data, openpyxl for formulas/formatting\n2. **Create/Load**: Create new workbook or load existing file\n3. **Modify**: Add/edit data, formulas, and formatting\n4. **Save**: Write to file\n5. **Recalculate formulas (MANDATORY IF USING FORMULAS)**: Use the recalc.py script\n   ```bash\n   python recalc.py output.xlsx\n   ```\n6. **Verify and fix any errors**: \n   - The script returns JSON with error details\n   - If `status` is `errors_found`, check `error_summary` for specific error types and locations\n   - Fix the identified errors and recalculate again\n   - Common errors to fix:\n     - `#REF!`: Invalid cell references\n     - `#DIV/0!`: Division by zero\n     - `#VALUE!`: Wrong data type in formula\n     - `#NAME?`: Unrecognized formula name\n\n### Creating new Excel files\n\n```python\n# Using openpyxl for formulas and formatting\nfrom openpyxl import Workbook\nfrom openpyxl.styles import Font, PatternFill, Alignment\n\nwb = Workbook()\nsheet = wb.active\n\n# Add data\nsheet['A1'] = 'Hello'\nsheet['B1'] = 'World'\nsheet.append(['Row', 'of', 'data'])\n\n# Add formula\nsheet['B2'] = '=SUM(A1:A10)'\n\n# Formatting\nsheet['A1'].font = Font(bold=True, color='FF0000')\nsheet['A1'].fill = PatternFill('solid', start_color='FFFF00')\nsheet['A1'].alignment = Alignment(horizontal='center')\n\n# Column width\nsheet.column_dimensions['A'].width = 20\n\nwb.save('output.xlsx')\n```\n\n### Editing existing Excel files\n\n```python\n# Using openpyxl to preserve formulas and formatting\nfrom openpyxl import load_workbook\n\n# Load existing file\nwb = load_workbook('existing.xlsx')\nsheet = wb.active  # or wb['SheetName'] for specific sheet\n\n# Working with multiple sheets\nfor sheet_name in wb.sheetnames:\n    sheet = wb[sheet_name]\n    print(f\"Sheet: {sheet_name}\")\n\n# Modify cells\nsheet['A1'] = 'New Value'\nsheet.insert_rows(2)  # Insert row at position 2\nsheet.delete_cols(3)  # Delete column 3\n\n# Add new sheet\nnew_sheet = wb.create_sheet('NewSheet')\nnew_sheet['A1'] = 'Data'\n\nwb.save('modified.xlsx')\n```\n\n## Recalculating formulas\n\nExcel files created or modified by openpyxl contain formulas as strings but not calculated values. Use the provided `recalc.py` script to recalculate formulas:\n\n```bash\npython recalc.py <excel_file> [timeout_seconds]\n```\n\nExample:\n```bash\npython recalc.py output.xlsx 30\n```\n\nThe script:\n- Automatically sets up LibreOffice macro on first run\n- Recalculates all formulas in all sheets\n- Scans ALL cells for Excel errors (#REF!, #DIV/0!, etc.)\n- Returns JSON with detailed error locations and counts\n- Works on both Linux and macOS\n\n## Formula Verification Checklist\n\nQuick checks to ensure formulas work correctly:\n\n### Essential Verification\n- [ ] **Test 2-3 sample references**: Verify they pull correct values before building full model\n- [ ] **Column mapping**: Confirm Excel columns match (e.g., column 64 = BL, not BK)\n- [ ] **Row offset**: Remember Excel rows are 1-indexed (DataFrame row 5 = Excel row 6)\n\n### Common Pitfalls\n- [ ] **NaN handling**: Check for null values with `pd.notna()`\n- [ ] **Far-right columns**: FY data often in columns 50+ \n- [ ] **Multiple matches**: Search all occurrences, not just first\n- [ ] **Division by zero**: Check denominators before using `/` in formulas (#DIV/0!)\n- [ ] **Wrong references**: Verify all cell references point to intended cells (#REF!)\n- [ ] **Cross-sheet references**: Use correct format (Sheet1!A1) for linking sheets\n\n### Formula Testing Strategy\n- [ ] **Start small**: Test formulas on 2-3 cells before applying broadly\n- [ ] **Verify dependencies**: Check all cells referenced in formulas exist\n- [ ] **Test edge cases**: Include zero, negative, and very large values\n\n### Interpreting recalc.py Output\nThe script returns JSON with error details:\n```json\n{\n  \"status\": \"success\",           // or \"errors_found\"\n  \"total_errors\": 0,              // Total error count\n  \"total_formulas\": 42,           // Number of formulas in file\n  \"error_summary\": {              // Only present if errors found\n    \"#REF!\": {\n      \"count\": 2,\n      \"locations\": [\"Sheet1!B5\", \"Sheet1!C10\"]\n    }\n  }\n}\n```\n\n## Best Practices\n\n### Library Selection\n- **pandas**: Best for data analysis, bulk operations, and simple data export\n- **openpyxl**: Best for complex formatting, formulas, and Excel-specific features\n\n### Working with openpyxl\n- Cell indices are 1-based (row=1, column=1 refers to cell A1)\n- Use `data_only=True` to read calculated values: `load_workbook('file.xlsx', data_only=True)`\n- **Warning**: If opened with `data_only=True` and saved, formulas are replaced with values and permanently lost\n- For large files: Use `read_only=True` for reading or `write_only=True` for writing\n- Formulas are preserved but not evaluated - use recalc.py to update values\n\n### Working with pandas\n- Specify data types to avoid inference issues: `pd.read_excel('file.xlsx', dtype={'id': str})`\n- For large files, read specific columns: `pd.read_excel('file.xlsx', usecols=['A', 'C', 'E'])`\n- Handle dates properly: `pd.read_excel('file.xlsx', parse_dates=['date_column'])`\n\n## Code Style Guidelines\n**IMPORTANT**: When generating Python code for Excel operations:\n- Write minimal, concise Python code without unnecessary comments\n- Avoid verbose variable names and redundant operations\n- Avoid unnecessary print statements\n\n**For Excel files themselves**:\n- Add comments to cells with complex formulas or important assumptions\n- Document data sources for hardcoded values\n- Include notes for key calculations and model sections"
    }
  },
  "conorluddy-ios-simulator-skill": {
    "id": "conorluddy-ios-simulator-skill",
    "name": "ios-simulator-skill",
    "description": "21 production-ready scripts for iOS app testing, building, and automation. Provides semantic UI navigation, build automation, accessibility testing, and simulator lifecycle management. Optimized for AI agents with minimal token output.",
    "repo": {
      "owner": "conorluddy",
      "name": "ios-simulator-skill",
      "fullName": "conorluddy/ios-simulator-skill",
      "url": "https://github.com/conorluddy/ios-simulator-skill",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 270,
      "forks": 14,
      "language": "Python",
      "topics": [
        "agent",
        "agentic-ai",
        "ai-agents",
        "claude",
        "claude-ai",
        "claude-code",
        "claude-skills",
        "claudecode",
        "claudeskills",
        "dx",
        "ios",
        "iossimulator",
        "mcp",
        "mcp-tools",
        "python",
        "simulator",
        "skills",
        "testing",
        "tooling",
        "xcode"
      ],
      "updatedAt": "2026-01-08T13:22:06Z",
      "pushedAt": "2025-11-10T09:53:29Z",
      "createdAt": "2025-10-17T12:43:15Z",
      "license": "MIT License"
    },
    "category": "development",
    "tags": [
      "agent",
      "agentic-ai",
      "ai-agents",
      "claude",
      "claude-ai",
      "claude-code",
      "claude-skills",
      "claudecode",
      "claudeskills",
      "dx",
      "ios",
      "iossimulator",
      "mcp",
      "mcp-tools",
      "python",
      "simulator",
      "skills",
      "testing",
      "tooling",
      "xcode"
    ],
    "skillMd": {
      "raw": "---\nname: ios-simulator-skill\nversion: 1.3.0\ndescription: 21 production-ready scripts for iOS app testing, building, and automation. Provides semantic UI navigation, build automation, accessibility testing, and simulator lifecycle management. Optimized for AI agents with minimal token output.\n---\n\n# iOS Simulator Skill\n\nBuild, test, and automate iOS applications using accessibility-driven navigation and structured data instead of pixel coordinates.\n\n## Quick Start\n\n```bash\n# 1. Check environment\nbash scripts/sim_health_check.sh\n\n# 2. Launch app\npython scripts/app_launcher.py --launch com.example.app\n\n# 3. Map screen to see elements\npython scripts/screen_mapper.py\n\n# 4. Tap button\npython scripts/navigator.py --find-text \"Login\" --tap\n\n# 5. Enter text\npython scripts/navigator.py --find-type TextField --enter-text \"user@example.com\"\n```\n\nAll scripts support `--help` for detailed options and `--json` for machine-readable output.\n\n## 21 Production Scripts\n\n### Build & Development (2 scripts)\n\n1. **build_and_test.py** - Build Xcode projects, run tests, parse results with progressive disclosure\n   - Build with live result streaming\n   - Parse errors and warnings from xcresult bundles\n   - Retrieve detailed build logs on demand\n   - Options: `--project`, `--scheme`, `--clean`, `--test`, `--verbose`, `--json`\n\n2. **log_monitor.py** - Real-time log monitoring with intelligent filtering\n   - Stream logs or capture by duration\n   - Filter by severity (error/warning/info/debug)\n   - Deduplicate repeated messages\n   - Options: `--app`, `--severity`, `--follow`, `--duration`, `--output`, `--json`\n\n### Navigation & Interaction (5 scripts)\n\n3. **screen_mapper.py** - Analyze current screen and list interactive elements\n   - Element type breakdown\n   - Interactive button list\n   - Text field status\n   - Options: `--verbose`, `--hints`, `--json`\n\n4. **navigator.py** - Find and interact with elements semantically\n   - Find by text (fuzzy matching)\n   - Find by element type\n   - Find by accessibility ID\n   - Enter text or tap elements\n   - Options: `--find-text`, `--find-type`, `--find-id`, `--tap`, `--enter-text`, `--json`\n\n5. **gesture.py** - Perform swipes, scrolls, pinches, and complex gestures\n   - Directional swipes (up/down/left/right)\n   - Multi-swipe scrolling\n   - Pinch zoom\n   - Long press\n   - Pull to refresh\n   - Options: `--swipe`, `--scroll`, `--pinch`, `--long-press`, `--refresh`, `--json`\n\n6. **keyboard.py** - Text input and hardware button control\n   - Type text (fast or slow)\n   - Special keys (return, delete, tab, space, arrows)\n   - Hardware buttons (home, lock, volume, screenshot)\n   - Key combinations\n   - Options: `--type`, `--key`, `--button`, `--slow`, `--clear`, `--dismiss`, `--json`\n\n7. **app_launcher.py** - App lifecycle management\n   - Launch apps by bundle ID\n   - Terminate apps\n   - Install/uninstall from .app bundles\n   - Deep link navigation\n   - List installed apps\n   - Check app state\n   - Options: `--launch`, `--terminate`, `--install`, `--uninstall`, `--open-url`, `--list`, `--state`, `--json`\n\n### Testing & Analysis (5 scripts)\n\n8. **accessibility_audit.py** - Check WCAG compliance on current screen\n   - Critical issues (missing labels, empty buttons, no alt text)\n   - Warnings (missing hints, small touch targets)\n   - Info (missing IDs, deep nesting)\n   - Options: `--verbose`, `--output`, `--json`\n\n9. **visual_diff.py** - Compare two screenshots for visual changes\n   - Pixel-by-pixel comparison\n   - Threshold-based pass/fail\n   - Generate diff images\n   - Options: `--threshold`, `--output`, `--details`, `--json`\n\n10. **test_recorder.py** - Automatically document test execution\n    - Capture screenshots and accessibility trees per step\n    - Generate markdown reports with timing data\n    - Options: `--test-name`, `--output`, `--verbose`, `--json`\n\n11. **app_state_capture.py** - Create comprehensive debugging snapshots\n    - Screenshot, UI hierarchy, app logs, device info\n    - Markdown summary for bug reports\n    - Options: `--app-bundle-id`, `--output`, `--log-lines`, `--json`\n\n12. **sim_health_check.sh** - Verify environment is properly configured\n    - Check macOS, Xcode, simctl, IDB, Python\n    - List available and booted simulators\n    - Verify Python packages (Pillow)\n\n### Advanced Testing & Permissions (4 scripts)\n\n13. **clipboard.py** - Manage simulator clipboard for paste testing\n    - Copy text to clipboard\n    - Test paste flows without manual entry\n    - Options: `--copy`, `--test-name`, `--expected`, `--json`\n\n14. **status_bar.py** - Override simulator status bar appearance\n    - Presets: clean (9:41, 100% battery), testing (11:11, 50%), low-battery (20%), airplane (offline)\n    - Custom time, network, battery, WiFi settings\n    - Options: `--preset`, `--time`, `--data-network`, `--battery-level`, `--clear`, `--json`\n\n15. **push_notification.py** - Send simulated push notifications\n    - Simple mode (title + body + badge)\n    - Custom JSON payloads\n    - Test notification handling and deep links\n    - Options: `--bundle-id`, `--title`, `--body`, `--badge`, `--payload`, `--json`\n\n16. **privacy_manager.py** - Grant, revoke, and reset app permissions\n    - 13 supported services (camera, microphone, location, contacts, photos, calendar, health, etc.)\n    - Batch operations (comma-separated services)\n    - Audit trail with test scenario tracking\n    - Options: `--bundle-id`, `--grant`, `--revoke`, `--reset`, `--list`, `--json`\n\n### Device Lifecycle Management (5 scripts)\n\n17. **simctl_boot.py** - Boot simulators with optional readiness verification\n    - Boot by UDID or device name\n    - Wait for device ready with timeout\n    - Batch boot operations (--all, --type)\n    - Performance timing\n    - Options: `--udid`, `--name`, `--wait-ready`, `--timeout`, `--all`, `--type`, `--json`\n\n18. **simctl_shutdown.py** - Gracefully shutdown simulators\n    - Shutdown by UDID or device name\n    - Optional verification of shutdown completion\n    - Batch shutdown operations\n    - Options: `--udid`, `--name`, `--verify`, `--timeout`, `--all`, `--type`, `--json`\n\n19. **simctl_create.py** - Create simulators dynamically\n    - Create by device type and iOS version\n    - List available device types and runtimes\n    - Custom device naming\n    - Returns UDID for CI/CD integration\n    - Options: `--device`, `--runtime`, `--name`, `--list-devices`, `--list-runtimes`, `--json`\n\n20. **simctl_delete.py** - Permanently delete simulators\n    - Delete by UDID or device name\n    - Safety confirmation by default (skip with --yes)\n    - Batch delete operations\n    - Smart deletion (--old N to keep N per device type)\n    - Options: `--udid`, `--name`, `--yes`, `--all`, `--type`, `--old`, `--json`\n\n21. **simctl_erase.py** - Factory reset simulators without deletion\n    - Preserve device UUID (faster than delete+create)\n    - Erase all, by type, or booted simulators\n    - Optional verification\n    - Options: `--udid`, `--name`, `--verify`, `--timeout`, `--all`, `--type`, `--booted`, `--json`\n\n## Common Patterns\n\n**Auto-UDID Detection**: Most scripts auto-detect the booted simulator if --udid is not provided.\n\n**Device Name Resolution**: Use device names (e.g., \"iPhone 16 Pro\") instead of UDIDs - scripts resolve automatically.\n\n**Batch Operations**: Many scripts support `--all` for all simulators or `--type iPhone` for device type filtering.\n\n**Output Formats**: Default is concise human-readable output. Use `--json` for machine-readable output in CI/CD.\n\n**Help**: All scripts support `--help` for detailed options and examples.\n\n## Typical Workflow\n\n1. Verify environment: `bash scripts/sim_health_check.sh`\n2. Launch app: `python scripts/app_launcher.py --launch com.example.app`\n3. Analyze screen: `python scripts/screen_mapper.py`\n4. Interact: `python scripts/navigator.py --find-text \"Button\" --tap`\n5. Verify: `python scripts/accessibility_audit.py`\n6. Debug if needed: `python scripts/app_state_capture.py --app-bundle-id com.example.app`\n\n## Requirements\n\n- macOS 12+\n- Xcode Command Line Tools\n- Python 3\n- IDB (optional, for interactive features)\n\n## Documentation\n\n- **SKILL.md** (this file) - Script reference and quick start\n- **README.md** - Installation and examples\n- **CLAUDE.md** - Architecture and implementation details\n- **references/** - Deep documentation on specific topics\n- **examples/** - Complete automation workflows\n\n## Key Design Principles\n\n**Semantic Navigation**: Find elements by meaning (text, type, ID) not pixel coordinates. Survives UI changes.\n\n**Token Efficiency**: Concise default output (3-5 lines) with optional verbose and JSON modes for detailed results.\n\n**Accessibility-First**: Built on standard accessibility APIs for reliability and compatibility.\n\n**Zero Configuration**: Works immediately on any macOS with Xcode. No setup required.\n\n**Structured Data**: Scripts output JSON or formatted text, not raw logs. Easy to parse and integrate.\n\n**Auto-Learning**: Build system remembers your device preference. Configuration stored per-project.\n\n---\n\nUse these scripts directly or let Claude Code invoke them automatically when your request matches the skill description.\n",
      "frontmatter": {
        "name": "ios-simulator-skill",
        "version": "1.3.0",
        "description": "21 production-ready scripts for iOS app testing, building, and automation. Provides semantic UI navigation, build automation, accessibility testing, and simulator lifecycle management. Optimized for AI agents with minimal token output."
      },
      "content": "\n# iOS Simulator Skill\n\nBuild, test, and automate iOS applications using accessibility-driven navigation and structured data instead of pixel coordinates.\n\n## Quick Start\n\n```bash\n# 1. Check environment\nbash scripts/sim_health_check.sh\n\n# 2. Launch app\npython scripts/app_launcher.py --launch com.example.app\n\n# 3. Map screen to see elements\npython scripts/screen_mapper.py\n\n# 4. Tap button\npython scripts/navigator.py --find-text \"Login\" --tap\n\n# 5. Enter text\npython scripts/navigator.py --find-type TextField --enter-text \"user@example.com\"\n```\n\nAll scripts support `--help` for detailed options and `--json` for machine-readable output.\n\n## 21 Production Scripts\n\n### Build & Development (2 scripts)\n\n1. **build_and_test.py** - Build Xcode projects, run tests, parse results with progressive disclosure\n   - Build with live result streaming\n   - Parse errors and warnings from xcresult bundles\n   - Retrieve detailed build logs on demand\n   - Options: `--project`, `--scheme`, `--clean`, `--test`, `--verbose`, `--json`\n\n2. **log_monitor.py** - Real-time log monitoring with intelligent filtering\n   - Stream logs or capture by duration\n   - Filter by severity (error/warning/info/debug)\n   - Deduplicate repeated messages\n   - Options: `--app`, `--severity`, `--follow`, `--duration`, `--output`, `--json`\n\n### Navigation & Interaction (5 scripts)\n\n3. **screen_mapper.py** - Analyze current screen and list interactive elements\n   - Element type breakdown\n   - Interactive button list\n   - Text field status\n   - Options: `--verbose`, `--hints`, `--json`\n\n4. **navigator.py** - Find and interact with elements semantically\n   - Find by text (fuzzy matching)\n   - Find by element type\n   - Find by accessibility ID\n   - Enter text or tap elements\n   - Options: `--find-text`, `--find-type`, `--find-id`, `--tap`, `--enter-text`, `--json`\n\n5. **gesture.py** - Perform swipes, scrolls, pinches, and complex gestures\n   - Directional swipes (up/down/left/right)\n   - Multi-swipe scrolling\n   - Pinch zoom\n   - Long press\n   - Pull to refresh\n   - Options: `--swipe`, `--scroll`, `--pinch`, `--long-press`, `--refresh`, `--json`\n\n6. **keyboard.py** - Text input and hardware button control\n   - Type text (fast or slow)\n   - Special keys (return, delete, tab, space, arrows)\n   - Hardware buttons (home, lock, volume, screenshot)\n   - Key combinations\n   - Options: `--type`, `--key`, `--button`, `--slow`, `--clear`, `--dismiss`, `--json`\n\n7. **app_launcher.py** - App lifecycle management\n   - Launch apps by bundle ID\n   - Terminate apps\n   - Install/uninstall from .app bundles\n   - Deep link navigation\n   - List installed apps\n   - Check app state\n   - Options: `--launch`, `--terminate`, `--install`, `--uninstall`, `--open-url`, `--list`, `--state`, `--json`\n\n### Testing & Analysis (5 scripts)\n\n8. **accessibility_audit.py** - Check WCAG compliance on current screen\n   - Critical issues (missing labels, empty buttons, no alt text)\n   - Warnings (missing hints, small touch targets)\n   - Info (missing IDs, deep nesting)\n   - Options: `--verbose`, `--output`, `--json`\n\n9. **visual_diff.py** - Compare two screenshots for visual changes\n   - Pixel-by-pixel comparison\n   - Threshold-based pass/fail\n   - Generate diff images\n   - Options: `--threshold`, `--output`, `--details`, `--json`\n\n10. **test_recorder.py** - Automatically document test execution\n    - Capture screenshots and accessibility trees per step\n    - Generate markdown reports with timing data\n    - Options: `--test-name`, `--output`, `--verbose`, `--json`\n\n11. **app_state_capture.py** - Create comprehensive debugging snapshots\n    - Screenshot, UI hierarchy, app logs, device info\n    - Markdown summary for bug reports\n    - Options: `--app-bundle-id`, `--output`, `--log-lines`, `--json`\n\n12. **sim_health_check.sh** - Verify environment is properly configured\n    - Check macOS, Xcode, simctl, IDB, Python\n    - List available and booted simulators\n    - Verify Python packages (Pillow)\n\n### Advanced Testing & Permissions (4 scripts)\n\n13. **clipboard.py** - Manage simulator clipboard for paste testing\n    - Copy text to clipboard\n    - Test paste flows without manual entry\n    - Options: `--copy`, `--test-name`, `--expected`, `--json`\n\n14. **status_bar.py** - Override simulator status bar appearance\n    - Presets: clean (9:41, 100% battery), testing (11:11, 50%), low-battery (20%), airplane (offline)\n    - Custom time, network, battery, WiFi settings\n    - Options: `--preset`, `--time`, `--data-network`, `--battery-level`, `--clear`, `--json`\n\n15. **push_notification.py** - Send simulated push notifications\n    - Simple mode (title + body + badge)\n    - Custom JSON payloads\n    - Test notification handling and deep links\n    - Options: `--bundle-id`, `--title`, `--body`, `--badge`, `--payload`, `--json`\n\n16. **privacy_manager.py** - Grant, revoke, and reset app permissions\n    - 13 supported services (camera, microphone, location, contacts, photos, calendar, health, etc.)\n    - Batch operations (comma-separated services)\n    - Audit trail with test scenario tracking\n    - Options: `--bundle-id`, `--grant`, `--revoke`, `--reset`, `--list`, `--json`\n\n### Device Lifecycle Management (5 scripts)\n\n17. **simctl_boot.py** - Boot simulators with optional readiness verification\n    - Boot by UDID or device name\n    - Wait for device ready with timeout\n    - Batch boot operations (--all, --type)\n    - Performance timing\n    - Options: `--udid`, `--name`, `--wait-ready`, `--timeout`, `--all`, `--type`, `--json`\n\n18. **simctl_shutdown.py** - Gracefully shutdown simulators\n    - Shutdown by UDID or device name\n    - Optional verification of shutdown completion\n    - Batch shutdown operations\n    - Options: `--udid`, `--name`, `--verify`, `--timeout`, `--all`, `--type`, `--json`\n\n19. **simctl_create.py** - Create simulators dynamically\n    - Create by device type and iOS version\n    - List available device types and runtimes\n    - Custom device naming\n    - Returns UDID for CI/CD integration\n    - Options: `--device`, `--runtime`, `--name`, `--list-devices`, `--list-runtimes`, `--json`\n\n20. **simctl_delete.py** - Permanently delete simulators\n    - Delete by UDID or device name\n    - Safety confirmation by default (skip with --yes)\n    - Batch delete operations\n    - Smart deletion (--old N to keep N per device type)\n    - Options: `--udid`, `--name`, `--yes`, `--all`, `--type`, `--old`, `--json`\n\n21. **simctl_erase.py** - Factory reset simulators without deletion\n    - Preserve device UUID (faster than delete+create)\n    - Erase all, by type, or booted simulators\n    - Optional verification\n    - Options: `--udid`, `--name`, `--verify`, `--timeout`, `--all`, `--type`, `--booted`, `--json`\n\n## Common Patterns\n\n**Auto-UDID Detection**: Most scripts auto-detect the booted simulator if --udid is not provided.\n\n**Device Name Resolution**: Use device names (e.g., \"iPhone 16 Pro\") instead of UDIDs - scripts resolve automatically.\n\n**Batch Operations**: Many scripts support `--all` for all simulators or `--type iPhone` for device type filtering.\n\n**Output Formats**: Default is concise human-readable output. Use `--json` for machine-readable output in CI/CD.\n\n**Help**: All scripts support `--help` for detailed options and examples.\n\n## Typical Workflow\n\n1. Verify environment: `bash scripts/sim_health_check.sh`\n2. Launch app: `python scripts/app_launcher.py --launch com.example.app`\n3. Analyze screen: `python scripts/screen_mapper.py`\n4. Interact: `python scripts/navigator.py --find-text \"Button\" --tap`\n5. Verify: `python scripts/accessibility_audit.py`\n6. Debug if needed: `python scripts/app_state_capture.py --app-bundle-id com.example.app`\n\n## Requirements\n\n- macOS 12+\n- Xcode Command Line Tools\n- Python 3\n- IDB (optional, for interactive features)\n\n## Documentation\n\n- **SKILL.md** (this file) - Script reference and quick start\n- **README.md** - Installation and examples\n- **CLAUDE.md** - Architecture and implementation details\n- **references/** - Deep documentation on specific topics\n- **examples/** - Complete automation workflows\n\n## Key Design Principles\n\n**Semantic Navigation**: Find elements by meaning (text, type, ID) not pixel coordinates. Survives UI changes.\n\n**Token Efficiency**: Concise default output (3-5 lines) with optional verbose and JSON modes for detailed results.\n\n**Accessibility-First**: Built on standard accessibility APIs for reliability and compatibility.\n\n**Zero Configuration**: Works immediately on any macOS with Xcode. No setup required.\n\n**Structured Data**: Scripts output JSON or formatted text, not raw logs. Easy to parse and integrate.\n\n**Auto-Learning**: Build system remembers your device preference. Configuration stored per-project.\n\n---\n\nUse these scripts directly or let Claude Code invoke them automatically when your request matches the skill description.\n"
    }
  },
  "chrisvoncsefalvay-claude-d3js-skill": {
    "id": "chrisvoncsefalvay-claude-d3js-skill",
    "name": "d3-viz",
    "description": "Creating interactive data visualisations using d3.js. This skill should be used when creating custom charts, graphs, network diagrams, geographic visualisations, or any complex SVG-based data visualisation that requires fine-grained control over visual elements, transitions, or interactions. Use this for bespoke visualisations beyond standard charting libraries, whether in React, Vue, Svelte, vanilla JavaScript, or any other environment.",
    "repo": {
      "owner": "chrisvoncsefalvay",
      "name": "claude-d3js-skill",
      "fullName": "chrisvoncsefalvay/claude-d3js-skill",
      "url": "https://github.com/chrisvoncsefalvay/claude-d3js-skill",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 59,
      "forks": 5,
      "language": "JavaScript",
      "topics": [],
      "updatedAt": "2026-01-07T09:50:42Z",
      "pushedAt": "2025-10-18T02:29:55Z",
      "createdAt": "2025-10-17T14:53:45Z"
    },
    "category": "development",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: d3-viz\ndescription: Creating interactive data visualisations using d3.js. This skill should be used when creating custom charts, graphs, network diagrams, geographic visualisations, or any complex SVG-based data visualisation that requires fine-grained control over visual elements, transitions, or interactions. Use this for bespoke visualisations beyond standard charting libraries, whether in React, Vue, Svelte, vanilla JavaScript, or any other environment.\n---\n\n# D3.js Visualisation\n\n## Overview\n\nThis skill provides guidance for creating sophisticated, interactive data visualisations using d3.js. D3.js (Data-Driven Documents) excels at binding data to DOM elements and applying data-driven transformations to create custom, publication-quality visualisations with precise control over every visual element. The techniques work across any JavaScript environment, including vanilla JavaScript, React, Vue, Svelte, and other frameworks.\n\n## When to use d3.js\n\n**Use d3.js for:**\n- Custom visualisations requiring unique visual encodings or layouts\n- Interactive explorations with complex pan, zoom, or brush behaviours\n- Network/graph visualisations (force-directed layouts, tree diagrams, hierarchies, chord diagrams)\n- Geographic visualisations with custom projections\n- Visualisations requiring smooth, choreographed transitions\n- Publication-quality graphics with fine-grained styling control\n- Novel chart types not available in standard libraries\n\n**Consider alternatives for:**\n- 3D visualisations - use Three.js instead\n\n## Core workflow\n\n### 1. Set up d3.js\n\nImport d3 at the top of your script:\n\n```javascript\nimport * as d3 from 'd3';\n```\n\nOr use the CDN version (7.x):\n\n```html\n<script src=\"https://d3js.org/d3.v7.min.js\"></script>\n```\n\nAll modules (scales, axes, shapes, transitions, etc.) are accessible through the `d3` namespace.\n\n### 2. Choose the integration pattern\n\n**Pattern A: Direct DOM manipulation (recommended for most cases)**\nUse d3 to select DOM elements and manipulate them imperatively. This works in any JavaScript environment:\n\n```javascript\nfunction drawChart(data) {\n  if (!data || data.length === 0) return;\n\n  const svg = d3.select('#chart'); // Select by ID, class, or DOM element\n\n  // Clear previous content\n  svg.selectAll(\"*\").remove();\n\n  // Set up dimensions\n  const width = 800;\n  const height = 400;\n  const margin = { top: 20, right: 30, bottom: 40, left: 50 };\n\n  // Create scales, axes, and draw visualisation\n  // ... d3 code here ...\n}\n\n// Call when data changes\ndrawChart(myData);\n```\n\n**Pattern B: Declarative rendering (for frameworks with templating)**\nUse d3 for data calculations (scales, layouts) but render elements via your framework:\n\n```javascript\nfunction getChartElements(data) {\n  const xScale = d3.scaleLinear()\n    .domain([0, d3.max(data, d => d.value)])\n    .range([0, 400]);\n\n  return data.map((d, i) => ({\n    x: 50,\n    y: i * 30,\n    width: xScale(d.value),\n    height: 25\n  }));\n}\n\n// In React: {getChartElements(data).map((d, i) => <rect key={i} {...d} fill=\"steelblue\" />)}\n// In Vue: v-for directive over the returned array\n// In vanilla JS: Create elements manually from the returned data\n```\n\nUse Pattern A for complex visualisations with transitions, interactions, or when leveraging d3's full capabilities. Use Pattern B for simpler visualisations or when your framework prefers declarative rendering.\n\n### 3. Structure the visualisation code\n\nFollow this standard structure in your drawing function:\n\n```javascript\nfunction drawVisualization(data) {\n  if (!data || data.length === 0) return;\n\n  const svg = d3.select('#chart'); // Or pass a selector/element\n  svg.selectAll(\"*\").remove(); // Clear previous render\n\n  // 1. Define dimensions\n  const width = 800;\n  const height = 400;\n  const margin = { top: 20, right: 30, bottom: 40, left: 50 };\n  const innerWidth = width - margin.left - margin.right;\n  const innerHeight = height - margin.top - margin.bottom;\n\n  // 2. Create main group with margins\n  const g = svg.append(\"g\")\n    .attr(\"transform\", `translate(${margin.left},${margin.top})`);\n\n  // 3. Create scales\n  const xScale = d3.scaleLinear()\n    .domain([0, d3.max(data, d => d.x)])\n    .range([0, innerWidth]);\n\n  const yScale = d3.scaleLinear()\n    .domain([0, d3.max(data, d => d.y)])\n    .range([innerHeight, 0]); // Note: inverted for SVG coordinates\n\n  // 4. Create and append axes\n  const xAxis = d3.axisBottom(xScale);\n  const yAxis = d3.axisLeft(yScale);\n\n  g.append(\"g\")\n    .attr(\"transform\", `translate(0,${innerHeight})`)\n    .call(xAxis);\n\n  g.append(\"g\")\n    .call(yAxis);\n\n  // 5. Bind data and create visual elements\n  g.selectAll(\"circle\")\n    .data(data)\n    .join(\"circle\")\n    .attr(\"cx\", d => xScale(d.x))\n    .attr(\"cy\", d => yScale(d.y))\n    .attr(\"r\", 5)\n    .attr(\"fill\", \"steelblue\");\n}\n\n// Call when data changes\ndrawVisualization(myData);\n```\n\n### 4. Implement responsive sizing\n\nMake visualisations responsive to container size:\n\n```javascript\nfunction setupResponsiveChart(containerId, data) {\n  const container = document.getElementById(containerId);\n  const svg = d3.select(`#${containerId}`).append('svg');\n\n  function updateChart() {\n    const { width, height } = container.getBoundingClientRect();\n    svg.attr('width', width).attr('height', height);\n\n    // Redraw visualisation with new dimensions\n    drawChart(data, svg, width, height);\n  }\n\n  // Update on initial load\n  updateChart();\n\n  // Update on window resize\n  window.addEventListener('resize', updateChart);\n\n  // Return cleanup function\n  return () => window.removeEventListener('resize', updateChart);\n}\n\n// Usage:\n// const cleanup = setupResponsiveChart('chart-container', myData);\n// cleanup(); // Call when component unmounts or element removed\n```\n\nOr use ResizeObserver for more direct container monitoring:\n\n```javascript\nfunction setupResponsiveChartWithObserver(svgElement, data) {\n  const observer = new ResizeObserver(() => {\n    const { width, height } = svgElement.getBoundingClientRect();\n    d3.select(svgElement)\n      .attr('width', width)\n      .attr('height', height);\n\n    // Redraw visualisation\n    drawChart(data, d3.select(svgElement), width, height);\n  });\n\n  observer.observe(svgElement.parentElement);\n  return () => observer.disconnect();\n}\n```\n\n## Common visualisation patterns\n\n### Bar chart\n\n```javascript\nfunction drawBarChart(data, svgElement) {\n  if (!data || data.length === 0) return;\n\n  const svg = d3.select(svgElement);\n  svg.selectAll(\"*\").remove();\n\n  const width = 800;\n  const height = 400;\n  const margin = { top: 20, right: 30, bottom: 40, left: 50 };\n  const innerWidth = width - margin.left - margin.right;\n  const innerHeight = height - margin.top - margin.bottom;\n\n  const g = svg.append(\"g\")\n    .attr(\"transform\", `translate(${margin.left},${margin.top})`);\n\n  const xScale = d3.scaleBand()\n    .domain(data.map(d => d.category))\n    .range([0, innerWidth])\n    .padding(0.1);\n\n  const yScale = d3.scaleLinear()\n    .domain([0, d3.max(data, d => d.value)])\n    .range([innerHeight, 0]);\n\n  g.append(\"g\")\n    .attr(\"transform\", `translate(0,${innerHeight})`)\n    .call(d3.axisBottom(xScale));\n\n  g.append(\"g\")\n    .call(d3.axisLeft(yScale));\n\n  g.selectAll(\"rect\")\n    .data(data)\n    .join(\"rect\")\n    .attr(\"x\", d => xScale(d.category))\n    .attr(\"y\", d => yScale(d.value))\n    .attr(\"width\", xScale.bandwidth())\n    .attr(\"height\", d => innerHeight - yScale(d.value))\n    .attr(\"fill\", \"steelblue\");\n}\n\n// Usage:\n// drawBarChart(myData, document.getElementById('chart'));\n```\n\n### Line chart\n\n```javascript\nconst line = d3.line()\n  .x(d => xScale(d.date))\n  .y(d => yScale(d.value))\n  .curve(d3.curveMonotoneX); // Smooth curve\n\ng.append(\"path\")\n  .datum(data)\n  .attr(\"fill\", \"none\")\n  .attr(\"stroke\", \"steelblue\")\n  .attr(\"stroke-width\", 2)\n  .attr(\"d\", line);\n```\n\n### Scatter plot\n\n```javascript\ng.selectAll(\"circle\")\n  .data(data)\n  .join(\"circle\")\n  .attr(\"cx\", d => xScale(d.x))\n  .attr(\"cy\", d => yScale(d.y))\n  .attr(\"r\", d => sizeScale(d.size)) // Optional: size encoding\n  .attr(\"fill\", d => colourScale(d.category)) // Optional: colour encoding\n  .attr(\"opacity\", 0.7);\n```\n\n### Chord diagram\n\nA chord diagram shows relationships between entities in a circular layout, with ribbons representing flows between them:\n\n```javascript\nfunction drawChordDiagram(data) {\n  // data format: array of objects with source, target, and value\n  // Example: [{ source: 'A', target: 'B', value: 10 }, ...]\n\n  if (!data || data.length === 0) return;\n\n  const svg = d3.select('#chart');\n  svg.selectAll(\"*\").remove();\n\n  const width = 600;\n  const height = 600;\n  const innerRadius = Math.min(width, height) * 0.3;\n  const outerRadius = innerRadius + 30;\n\n  // Create matrix from data\n  const nodes = Array.from(new Set(data.flatMap(d => [d.source, d.target])));\n  const matrix = Array.from({ length: nodes.length }, () => Array(nodes.length).fill(0));\n\n  data.forEach(d => {\n    const i = nodes.indexOf(d.source);\n    const j = nodes.indexOf(d.target);\n    matrix[i][j] += d.value;\n    matrix[j][i] += d.value;\n  });\n\n  // Create chord layout\n  const chord = d3.chord()\n    .padAngle(0.05)\n    .sortSubgroups(d3.descending);\n\n  const arc = d3.arc()\n    .innerRadius(innerRadius)\n    .outerRadius(outerRadius);\n\n  const ribbon = d3.ribbon()\n    .source(d => d.source)\n    .target(d => d.target);\n\n  const colourScale = d3.scaleOrdinal(d3.schemeCategory10)\n    .domain(nodes);\n\n  const g = svg.append(\"g\")\n    .attr(\"transform\", `translate(${width / 2},${height / 2})`);\n\n  const chords = chord(matrix);\n\n  // Draw ribbons\n  g.append(\"g\")\n    .attr(\"fill-opacity\", 0.67)\n    .selectAll(\"path\")\n    .data(chords)\n    .join(\"path\")\n    .attr(\"d\", ribbon)\n    .attr(\"fill\", d => colourScale(nodes[d.source.index]))\n    .attr(\"stroke\", d => d3.rgb(colourScale(nodes[d.source.index])).darker());\n\n  // Draw groups (arcs)\n  const group = g.append(\"g\")\n    .selectAll(\"g\")\n    .data(chords.groups)\n    .join(\"g\");\n\n  group.append(\"path\")\n    .attr(\"d\", arc)\n    .attr(\"fill\", d => colourScale(nodes[d.index]))\n    .attr(\"stroke\", d => d3.rgb(colourScale(nodes[d.index])).darker());\n\n  // Add labels\n  group.append(\"text\")\n    .each(d => { d.angle = (d.startAngle + d.endAngle) / 2; })\n    .attr(\"dy\", \"0.31em\")\n    .attr(\"transform\", d => `rotate(${(d.angle * 180 / Math.PI) - 90})translate(${outerRadius + 30})${d.angle > Math.PI ? \"rotate(180)\" : \"\"}`)\n    .attr(\"text-anchor\", d => d.angle > Math.PI ? \"end\" : null)\n    .text((d, i) => nodes[i])\n    .style(\"font-size\", \"12px\");\n}\n```\n\n### Heatmap\n\nA heatmap uses colour to encode values in a two-dimensional grid, useful for showing patterns across categories:\n\n```javascript\nfunction drawHeatmap(data) {\n  // data format: array of objects with row, column, and value\n  // Example: [{ row: 'A', column: 'X', value: 10 }, ...]\n\n  if (!data || data.length === 0) return;\n\n  const svg = d3.select('#chart');\n  svg.selectAll(\"*\").remove();\n\n  const width = 800;\n  const height = 600;\n  const margin = { top: 100, right: 30, bottom: 30, left: 100 };\n  const innerWidth = width - margin.left - margin.right;\n  const innerHeight = height - margin.top - margin.bottom;\n\n  // Get unique rows and columns\n  const rows = Array.from(new Set(data.map(d => d.row)));\n  const columns = Array.from(new Set(data.map(d => d.column)));\n\n  const g = svg.append(\"g\")\n    .attr(\"transform\", `translate(${margin.left},${margin.top})`);\n\n  // Create scales\n  const xScale = d3.scaleBand()\n    .domain(columns)\n    .range([0, innerWidth])\n    .padding(0.01);\n\n  const yScale = d3.scaleBand()\n    .domain(rows)\n    .range([0, innerHeight])\n    .padding(0.01);\n\n  // Colour scale for values\n  const colourScale = d3.scaleSequential(d3.interpolateYlOrRd)\n    .domain([0, d3.max(data, d => d.value)]);\n\n  // Draw rectangles\n  g.selectAll(\"rect\")\n    .data(data)\n    .join(\"rect\")\n    .attr(\"x\", d => xScale(d.column))\n    .attr(\"y\", d => yScale(d.row))\n    .attr(\"width\", xScale.bandwidth())\n    .attr(\"height\", yScale.bandwidth())\n    .attr(\"fill\", d => colourScale(d.value));\n\n  // Add x-axis labels\n  svg.append(\"g\")\n    .attr(\"transform\", `translate(${margin.left},${margin.top})`)\n    .selectAll(\"text\")\n    .data(columns)\n    .join(\"text\")\n    .attr(\"x\", d => xScale(d) + xScale.bandwidth() / 2)\n    .attr(\"y\", -10)\n    .attr(\"text-anchor\", \"middle\")\n    .text(d => d)\n    .style(\"font-size\", \"12px\");\n\n  // Add y-axis labels\n  svg.append(\"g\")\n    .attr(\"transform\", `translate(${margin.left},${margin.top})`)\n    .selectAll(\"text\")\n    .data(rows)\n    .join(\"text\")\n    .attr(\"x\", -10)\n    .attr(\"y\", d => yScale(d) + yScale.bandwidth() / 2)\n    .attr(\"dy\", \"0.35em\")\n    .attr(\"text-anchor\", \"end\")\n    .text(d => d)\n    .style(\"font-size\", \"12px\");\n\n  // Add colour legend\n  const legendWidth = 20;\n  const legendHeight = 200;\n  const legend = svg.append(\"g\")\n    .attr(\"transform\", `translate(${width - 60},${margin.top})`);\n\n  const legendScale = d3.scaleLinear()\n    .domain(colourScale.domain())\n    .range([legendHeight, 0]);\n\n  const legendAxis = d3.axisRight(legendScale)\n    .ticks(5);\n\n  // Draw colour gradient in legend\n  for (let i = 0; i < legendHeight; i++) {\n    legend.append(\"rect\")\n      .attr(\"y\", i)\n      .attr(\"width\", legendWidth)\n      .attr(\"height\", 1)\n      .attr(\"fill\", colourScale(legendScale.invert(i)));\n  }\n\n  legend.append(\"g\")\n    .attr(\"transform\", `translate(${legendWidth},0)`)\n    .call(legendAxis);\n}\n```\n\n### Pie chart\n\n```javascript\nconst pie = d3.pie()\n  .value(d => d.value)\n  .sort(null);\n\nconst arc = d3.arc()\n  .innerRadius(0)\n  .outerRadius(Math.min(width, height) / 2 - 20);\n\nconst colourScale = d3.scaleOrdinal(d3.schemeCategory10);\n\nconst g = svg.append(\"g\")\n  .attr(\"transform\", `translate(${width / 2},${height / 2})`);\n\ng.selectAll(\"path\")\n  .data(pie(data))\n  .join(\"path\")\n  .attr(\"d\", arc)\n  .attr(\"fill\", (d, i) => colourScale(i))\n  .attr(\"stroke\", \"white\")\n  .attr(\"stroke-width\", 2);\n```\n\n### Force-directed network\n\n```javascript\nconst simulation = d3.forceSimulation(nodes)\n  .force(\"link\", d3.forceLink(links).id(d => d.id).distance(100))\n  .force(\"charge\", d3.forceManyBody().strength(-300))\n  .force(\"center\", d3.forceCenter(width / 2, height / 2));\n\nconst link = g.selectAll(\"line\")\n  .data(links)\n  .join(\"line\")\n  .attr(\"stroke\", \"#999\")\n  .attr(\"stroke-width\", 1);\n\nconst node = g.selectAll(\"circle\")\n  .data(nodes)\n  .join(\"circle\")\n  .attr(\"r\", 8)\n  .attr(\"fill\", \"steelblue\")\n  .call(d3.drag()\n    .on(\"start\", dragstarted)\n    .on(\"drag\", dragged)\n    .on(\"end\", dragended));\n\nsimulation.on(\"tick\", () => {\n  link\n    .attr(\"x1\", d => d.source.x)\n    .attr(\"y1\", d => d.source.y)\n    .attr(\"x2\", d => d.target.x)\n    .attr(\"y2\", d => d.target.y);\n  \n  node\n    .attr(\"cx\", d => d.x)\n    .attr(\"cy\", d => d.y);\n});\n\nfunction dragstarted(event) {\n  if (!event.active) simulation.alphaTarget(0.3).restart();\n  event.subject.fx = event.subject.x;\n  event.subject.fy = event.subject.y;\n}\n\nfunction dragged(event) {\n  event.subject.fx = event.x;\n  event.subject.fy = event.y;\n}\n\nfunction dragended(event) {\n  if (!event.active) simulation.alphaTarget(0);\n  event.subject.fx = null;\n  event.subject.fy = null;\n}\n```\n\n## Adding interactivity\n\n### Tooltips\n\n```javascript\n// Create tooltip div (outside SVG)\nconst tooltip = d3.select(\"body\").append(\"div\")\n  .attr(\"class\", \"tooltip\")\n  .style(\"position\", \"absolute\")\n  .style(\"visibility\", \"hidden\")\n  .style(\"background-color\", \"white\")\n  .style(\"border\", \"1px solid #ddd\")\n  .style(\"padding\", \"10px\")\n  .style(\"border-radius\", \"4px\")\n  .style(\"pointer-events\", \"none\");\n\n// Add to elements\ncircles\n  .on(\"mouseover\", function(event, d) {\n    d3.select(this).attr(\"opacity\", 1);\n    tooltip\n      .style(\"visibility\", \"visible\")\n      .html(`<strong>${d.label}</strong><br/>Value: ${d.value}`);\n  })\n  .on(\"mousemove\", function(event) {\n    tooltip\n      .style(\"top\", (event.pageY - 10) + \"px\")\n      .style(\"left\", (event.pageX + 10) + \"px\");\n  })\n  .on(\"mouseout\", function() {\n    d3.select(this).attr(\"opacity\", 0.7);\n    tooltip.style(\"visibility\", \"hidden\");\n  });\n```\n\n### Zoom and pan\n\n```javascript\nconst zoom = d3.zoom()\n  .scaleExtent([0.5, 10])\n  .on(\"zoom\", (event) => {\n    g.attr(\"transform\", event.transform);\n  });\n\nsvg.call(zoom);\n```\n\n### Click interactions\n\n```javascript\ncircles\n  .on(\"click\", function(event, d) {\n    // Handle click (dispatch event, update app state, etc.)\n    console.log(\"Clicked:\", d);\n\n    // Visual feedback\n    d3.selectAll(\"circle\").attr(\"fill\", \"steelblue\");\n    d3.select(this).attr(\"fill\", \"orange\");\n\n    // Optional: dispatch custom event for your framework/app to listen to\n    // window.dispatchEvent(new CustomEvent('chartClick', { detail: d }));\n  });\n```\n\n## Transitions and animations\n\nAdd smooth transitions to visual changes:\n\n```javascript\n// Basic transition\ncircles\n  .transition()\n  .duration(750)\n  .attr(\"r\", 10);\n\n// Chained transitions\ncircles\n  .transition()\n  .duration(500)\n  .attr(\"fill\", \"orange\")\n  .transition()\n  .duration(500)\n  .attr(\"r\", 15);\n\n// Staggered transitions\ncircles\n  .transition()\n  .delay((d, i) => i * 50)\n  .duration(500)\n  .attr(\"cy\", d => yScale(d.value));\n\n// Custom easing\ncircles\n  .transition()\n  .duration(1000)\n  .ease(d3.easeBounceOut)\n  .attr(\"r\", 10);\n```\n\n## Scales reference\n\n### Quantitative scales\n\n```javascript\n// Linear scale\nconst xScale = d3.scaleLinear()\n  .domain([0, 100])\n  .range([0, 500]);\n\n// Log scale (for exponential data)\nconst logScale = d3.scaleLog()\n  .domain([1, 1000])\n  .range([0, 500]);\n\n// Power scale\nconst powScale = d3.scalePow()\n  .exponent(2)\n  .domain([0, 100])\n  .range([0, 500]);\n\n// Time scale\nconst timeScale = d3.scaleTime()\n  .domain([new Date(2020, 0, 1), new Date(2024, 0, 1)])\n  .range([0, 500]);\n```\n\n### Ordinal scales\n\n```javascript\n// Band scale (for bar charts)\nconst bandScale = d3.scaleBand()\n  .domain(['A', 'B', 'C', 'D'])\n  .range([0, 400])\n  .padding(0.1);\n\n// Point scale (for line/scatter categories)\nconst pointScale = d3.scalePoint()\n  .domain(['A', 'B', 'C', 'D'])\n  .range([0, 400]);\n\n// Ordinal scale (for colours)\nconst colourScale = d3.scaleOrdinal(d3.schemeCategory10);\n```\n\n### Sequential scales\n\n```javascript\n// Sequential colour scale\nconst colourScale = d3.scaleSequential(d3.interpolateBlues)\n  .domain([0, 100]);\n\n// Diverging colour scale\nconst divScale = d3.scaleDiverging(d3.interpolateRdBu)\n  .domain([-10, 0, 10]);\n```\n\n## Best practices\n\n### Data preparation\n\nAlways validate and prepare data before visualisation:\n\n```javascript\n// Filter invalid values\nconst cleanData = data.filter(d => d.value != null && !isNaN(d.value));\n\n// Sort data if order matters\nconst sortedData = [...data].sort((a, b) => b.value - a.value);\n\n// Parse dates\nconst parsedData = data.map(d => ({\n  ...d,\n  date: d3.timeParse(\"%Y-%m-%d\")(d.date)\n}));\n```\n\n### Performance optimisation\n\nFor large datasets (>1000 elements):\n\n```javascript\n// Use canvas instead of SVG for many elements\n// Use quadtree for collision detection\n// Simplify paths with d3.line().curve(d3.curveStep)\n// Implement virtual scrolling for large lists\n// Use requestAnimationFrame for custom animations\n```\n\n### Accessibility\n\nMake visualisations accessible:\n\n```javascript\n// Add ARIA labels\nsvg.attr(\"role\", \"img\")\n   .attr(\"aria-label\", \"Bar chart showing quarterly revenue\");\n\n// Add title and description\nsvg.append(\"title\").text(\"Quarterly Revenue 2024\");\nsvg.append(\"desc\").text(\"Bar chart showing revenue growth across four quarters\");\n\n// Ensure sufficient colour contrast\n// Provide keyboard navigation for interactive elements\n// Include data table alternative\n```\n\n### Styling\n\nUse consistent, professional styling:\n\n```javascript\n// Define colour palettes upfront\nconst colours = {\n  primary: '#4A90E2',\n  secondary: '#7B68EE',\n  background: '#F5F7FA',\n  text: '#333333',\n  gridLines: '#E0E0E0'\n};\n\n// Apply consistent typography\nsvg.selectAll(\"text\")\n  .style(\"font-family\", \"Inter, sans-serif\")\n  .style(\"font-size\", \"12px\");\n\n// Use subtle grid lines\ng.selectAll(\".tick line\")\n  .attr(\"stroke\", colours.gridLines)\n  .attr(\"stroke-dasharray\", \"2,2\");\n```\n\n## Common issues and solutions\n\n**Issue**: Axes not appearing\n- Ensure scales have valid domains (check for NaN values)\n- Verify axis is appended to correct group\n- Check transform translations are correct\n\n**Issue**: Transitions not working\n- Call `.transition()` before attribute changes\n- Ensure elements have unique keys for proper data binding\n- Check that useEffect dependencies include all changing data\n\n**Issue**: Responsive sizing not working\n- Use ResizeObserver or window resize listener\n- Update dimensions in state to trigger re-render\n- Ensure SVG has width/height attributes or viewBox\n\n**Issue**: Performance problems\n- Limit number of DOM elements (consider canvas for >1000 items)\n- Debounce resize handlers\n- Use `.join()` instead of separate enter/update/exit selections\n- Avoid unnecessary re-renders by checking dependencies\n\n## Resources\n\n### references/\nContains detailed reference materials:\n- `d3-patterns.md` - Comprehensive collection of visualisation patterns and code examples\n- `scale-reference.md` - Complete guide to d3 scales with examples\n- `colour-schemes.md` - D3 colour schemes and palette recommendations\n\n### assets/\n\nContains boilerplate templates:\n\n- `chart-template.js` - Starter template for basic chart\n- `interactive-template.js` - Template with tooltips, zoom, and interactions\n- `sample-data.json` - Example datasets for testing\n\nThese templates work with vanilla JavaScript, React, Vue, Svelte, or any other JavaScript environment. Adapt them as needed for your specific framework.\n\nTo use these resources, read the relevant files when detailed guidance is needed for specific visualisation types or patterns.\n",
      "frontmatter": {
        "name": "d3-viz",
        "description": "Creating interactive data visualisations using d3.js. This skill should be used when creating custom charts, graphs, network diagrams, geographic visualisations, or any complex SVG-based data visualisation that requires fine-grained control over visual elements, transitions, or interactions. Use this for bespoke visualisations beyond standard charting libraries, whether in React, Vue, Svelte, vanilla JavaScript, or any other environment."
      },
      "content": "\n# D3.js Visualisation\n\n## Overview\n\nThis skill provides guidance for creating sophisticated, interactive data visualisations using d3.js. D3.js (Data-Driven Documents) excels at binding data to DOM elements and applying data-driven transformations to create custom, publication-quality visualisations with precise control over every visual element. The techniques work across any JavaScript environment, including vanilla JavaScript, React, Vue, Svelte, and other frameworks.\n\n## When to use d3.js\n\n**Use d3.js for:**\n- Custom visualisations requiring unique visual encodings or layouts\n- Interactive explorations with complex pan, zoom, or brush behaviours\n- Network/graph visualisations (force-directed layouts, tree diagrams, hierarchies, chord diagrams)\n- Geographic visualisations with custom projections\n- Visualisations requiring smooth, choreographed transitions\n- Publication-quality graphics with fine-grained styling control\n- Novel chart types not available in standard libraries\n\n**Consider alternatives for:**\n- 3D visualisations - use Three.js instead\n\n## Core workflow\n\n### 1. Set up d3.js\n\nImport d3 at the top of your script:\n\n```javascript\nimport * as d3 from 'd3';\n```\n\nOr use the CDN version (7.x):\n\n```html\n<script src=\"https://d3js.org/d3.v7.min.js\"></script>\n```\n\nAll modules (scales, axes, shapes, transitions, etc.) are accessible through the `d3` namespace.\n\n### 2. Choose the integration pattern\n\n**Pattern A: Direct DOM manipulation (recommended for most cases)**\nUse d3 to select DOM elements and manipulate them imperatively. This works in any JavaScript environment:\n\n```javascript\nfunction drawChart(data) {\n  if (!data || data.length === 0) return;\n\n  const svg = d3.select('#chart'); // Select by ID, class, or DOM element\n\n  // Clear previous content\n  svg.selectAll(\"*\").remove();\n\n  // Set up dimensions\n  const width = 800;\n  const height = 400;\n  const margin = { top: 20, right: 30, bottom: 40, left: 50 };\n\n  // Create scales, axes, and draw visualisation\n  // ... d3 code here ...\n}\n\n// Call when data changes\ndrawChart(myData);\n```\n\n**Pattern B: Declarative rendering (for frameworks with templating)**\nUse d3 for data calculations (scales, layouts) but render elements via your framework:\n\n```javascript\nfunction getChartElements(data) {\n  const xScale = d3.scaleLinear()\n    .domain([0, d3.max(data, d => d.value)])\n    .range([0, 400]);\n\n  return data.map((d, i) => ({\n    x: 50,\n    y: i * 30,\n    width: xScale(d.value),\n    height: 25\n  }));\n}\n\n// In React: {getChartElements(data).map((d, i) => <rect key={i} {...d} fill=\"steelblue\" />)}\n// In Vue: v-for directive over the returned array\n// In vanilla JS: Create elements manually from the returned data\n```\n\nUse Pattern A for complex visualisations with transitions, interactions, or when leveraging d3's full capabilities. Use Pattern B for simpler visualisations or when your framework prefers declarative rendering.\n\n### 3. Structure the visualisation code\n\nFollow this standard structure in your drawing function:\n\n```javascript\nfunction drawVisualization(data) {\n  if (!data || data.length === 0) return;\n\n  const svg = d3.select('#chart'); // Or pass a selector/element\n  svg.selectAll(\"*\").remove(); // Clear previous render\n\n  // 1. Define dimensions\n  const width = 800;\n  const height = 400;\n  const margin = { top: 20, right: 30, bottom: 40, left: 50 };\n  const innerWidth = width - margin.left - margin.right;\n  const innerHeight = height - margin.top - margin.bottom;\n\n  // 2. Create main group with margins\n  const g = svg.append(\"g\")\n    .attr(\"transform\", `translate(${margin.left},${margin.top})`);\n\n  // 3. Create scales\n  const xScale = d3.scaleLinear()\n    .domain([0, d3.max(data, d => d.x)])\n    .range([0, innerWidth]);\n\n  const yScale = d3.scaleLinear()\n    .domain([0, d3.max(data, d => d.y)])\n    .range([innerHeight, 0]); // Note: inverted for SVG coordinates\n\n  // 4. Create and append axes\n  const xAxis = d3.axisBottom(xScale);\n  const yAxis = d3.axisLeft(yScale);\n\n  g.append(\"g\")\n    .attr(\"transform\", `translate(0,${innerHeight})`)\n    .call(xAxis);\n\n  g.append(\"g\")\n    .call(yAxis);\n\n  // 5. Bind data and create visual elements\n  g.selectAll(\"circle\")\n    .data(data)\n    .join(\"circle\")\n    .attr(\"cx\", d => xScale(d.x))\n    .attr(\"cy\", d => yScale(d.y))\n    .attr(\"r\", 5)\n    .attr(\"fill\", \"steelblue\");\n}\n\n// Call when data changes\ndrawVisualization(myData);\n```\n\n### 4. Implement responsive sizing\n\nMake visualisations responsive to container size:\n\n```javascript\nfunction setupResponsiveChart(containerId, data) {\n  const container = document.getElementById(containerId);\n  const svg = d3.select(`#${containerId}`).append('svg');\n\n  function updateChart() {\n    const { width, height } = container.getBoundingClientRect();\n    svg.attr('width', width).attr('height', height);\n\n    // Redraw visualisation with new dimensions\n    drawChart(data, svg, width, height);\n  }\n\n  // Update on initial load\n  updateChart();\n\n  // Update on window resize\n  window.addEventListener('resize', updateChart);\n\n  // Return cleanup function\n  return () => window.removeEventListener('resize', updateChart);\n}\n\n// Usage:\n// const cleanup = setupResponsiveChart('chart-container', myData);\n// cleanup(); // Call when component unmounts or element removed\n```\n\nOr use ResizeObserver for more direct container monitoring:\n\n```javascript\nfunction setupResponsiveChartWithObserver(svgElement, data) {\n  const observer = new ResizeObserver(() => {\n    const { width, height } = svgElement.getBoundingClientRect();\n    d3.select(svgElement)\n      .attr('width', width)\n      .attr('height', height);\n\n    // Redraw visualisation\n    drawChart(data, d3.select(svgElement), width, height);\n  });\n\n  observer.observe(svgElement.parentElement);\n  return () => observer.disconnect();\n}\n```\n\n## Common visualisation patterns\n\n### Bar chart\n\n```javascript\nfunction drawBarChart(data, svgElement) {\n  if (!data || data.length === 0) return;\n\n  const svg = d3.select(svgElement);\n  svg.selectAll(\"*\").remove();\n\n  const width = 800;\n  const height = 400;\n  const margin = { top: 20, right: 30, bottom: 40, left: 50 };\n  const innerWidth = width - margin.left - margin.right;\n  const innerHeight = height - margin.top - margin.bottom;\n\n  const g = svg.append(\"g\")\n    .attr(\"transform\", `translate(${margin.left},${margin.top})`);\n\n  const xScale = d3.scaleBand()\n    .domain(data.map(d => d.category))\n    .range([0, innerWidth])\n    .padding(0.1);\n\n  const yScale = d3.scaleLinear()\n    .domain([0, d3.max(data, d => d.value)])\n    .range([innerHeight, 0]);\n\n  g.append(\"g\")\n    .attr(\"transform\", `translate(0,${innerHeight})`)\n    .call(d3.axisBottom(xScale));\n\n  g.append(\"g\")\n    .call(d3.axisLeft(yScale));\n\n  g.selectAll(\"rect\")\n    .data(data)\n    .join(\"rect\")\n    .attr(\"x\", d => xScale(d.category))\n    .attr(\"y\", d => yScale(d.value))\n    .attr(\"width\", xScale.bandwidth())\n    .attr(\"height\", d => innerHeight - yScale(d.value))\n    .attr(\"fill\", \"steelblue\");\n}\n\n// Usage:\n// drawBarChart(myData, document.getElementById('chart'));\n```\n\n### Line chart\n\n```javascript\nconst line = d3.line()\n  .x(d => xScale(d.date))\n  .y(d => yScale(d.value))\n  .curve(d3.curveMonotoneX); // Smooth curve\n\ng.append(\"path\")\n  .datum(data)\n  .attr(\"fill\", \"none\")\n  .attr(\"stroke\", \"steelblue\")\n  .attr(\"stroke-width\", 2)\n  .attr(\"d\", line);\n```\n\n### Scatter plot\n\n```javascript\ng.selectAll(\"circle\")\n  .data(data)\n  .join(\"circle\")\n  .attr(\"cx\", d => xScale(d.x))\n  .attr(\"cy\", d => yScale(d.y))\n  .attr(\"r\", d => sizeScale(d.size)) // Optional: size encoding\n  .attr(\"fill\", d => colourScale(d.category)) // Optional: colour encoding\n  .attr(\"opacity\", 0.7);\n```\n\n### Chord diagram\n\nA chord diagram shows relationships between entities in a circular layout, with ribbons representing flows between them:\n\n```javascript\nfunction drawChordDiagram(data) {\n  // data format: array of objects with source, target, and value\n  // Example: [{ source: 'A', target: 'B', value: 10 }, ...]\n\n  if (!data || data.length === 0) return;\n\n  const svg = d3.select('#chart');\n  svg.selectAll(\"*\").remove();\n\n  const width = 600;\n  const height = 600;\n  const innerRadius = Math.min(width, height) * 0.3;\n  const outerRadius = innerRadius + 30;\n\n  // Create matrix from data\n  const nodes = Array.from(new Set(data.flatMap(d => [d.source, d.target])));\n  const matrix = Array.from({ length: nodes.length }, () => Array(nodes.length).fill(0));\n\n  data.forEach(d => {\n    const i = nodes.indexOf(d.source);\n    const j = nodes.indexOf(d.target);\n    matrix[i][j] += d.value;\n    matrix[j][i] += d.value;\n  });\n\n  // Create chord layout\n  const chord = d3.chord()\n    .padAngle(0.05)\n    .sortSubgroups(d3.descending);\n\n  const arc = d3.arc()\n    .innerRadius(innerRadius)\n    .outerRadius(outerRadius);\n\n  const ribbon = d3.ribbon()\n    .source(d => d.source)\n    .target(d => d.target);\n\n  const colourScale = d3.scaleOrdinal(d3.schemeCategory10)\n    .domain(nodes);\n\n  const g = svg.append(\"g\")\n    .attr(\"transform\", `translate(${width / 2},${height / 2})`);\n\n  const chords = chord(matrix);\n\n  // Draw ribbons\n  g.append(\"g\")\n    .attr(\"fill-opacity\", 0.67)\n    .selectAll(\"path\")\n    .data(chords)\n    .join(\"path\")\n    .attr(\"d\", ribbon)\n    .attr(\"fill\", d => colourScale(nodes[d.source.index]))\n    .attr(\"stroke\", d => d3.rgb(colourScale(nodes[d.source.index])).darker());\n\n  // Draw groups (arcs)\n  const group = g.append(\"g\")\n    .selectAll(\"g\")\n    .data(chords.groups)\n    .join(\"g\");\n\n  group.append(\"path\")\n    .attr(\"d\", arc)\n    .attr(\"fill\", d => colourScale(nodes[d.index]))\n    .attr(\"stroke\", d => d3.rgb(colourScale(nodes[d.index])).darker());\n\n  // Add labels\n  group.append(\"text\")\n    .each(d => { d.angle = (d.startAngle + d.endAngle) / 2; })\n    .attr(\"dy\", \"0.31em\")\n    .attr(\"transform\", d => `rotate(${(d.angle * 180 / Math.PI) - 90})translate(${outerRadius + 30})${d.angle > Math.PI ? \"rotate(180)\" : \"\"}`)\n    .attr(\"text-anchor\", d => d.angle > Math.PI ? \"end\" : null)\n    .text((d, i) => nodes[i])\n    .style(\"font-size\", \"12px\");\n}\n```\n\n### Heatmap\n\nA heatmap uses colour to encode values in a two-dimensional grid, useful for showing patterns across categories:\n\n```javascript\nfunction drawHeatmap(data) {\n  // data format: array of objects with row, column, and value\n  // Example: [{ row: 'A', column: 'X', value: 10 }, ...]\n\n  if (!data || data.length === 0) return;\n\n  const svg = d3.select('#chart');\n  svg.selectAll(\"*\").remove();\n\n  const width = 800;\n  const height = 600;\n  const margin = { top: 100, right: 30, bottom: 30, left: 100 };\n  const innerWidth = width - margin.left - margin.right;\n  const innerHeight = height - margin.top - margin.bottom;\n\n  // Get unique rows and columns\n  const rows = Array.from(new Set(data.map(d => d.row)));\n  const columns = Array.from(new Set(data.map(d => d.column)));\n\n  const g = svg.append(\"g\")\n    .attr(\"transform\", `translate(${margin.left},${margin.top})`);\n\n  // Create scales\n  const xScale = d3.scaleBand()\n    .domain(columns)\n    .range([0, innerWidth])\n    .padding(0.01);\n\n  const yScale = d3.scaleBand()\n    .domain(rows)\n    .range([0, innerHeight])\n    .padding(0.01);\n\n  // Colour scale for values\n  const colourScale = d3.scaleSequential(d3.interpolateYlOrRd)\n    .domain([0, d3.max(data, d => d.value)]);\n\n  // Draw rectangles\n  g.selectAll(\"rect\")\n    .data(data)\n    .join(\"rect\")\n    .attr(\"x\", d => xScale(d.column))\n    .attr(\"y\", d => yScale(d.row))\n    .attr(\"width\", xScale.bandwidth())\n    .attr(\"height\", yScale.bandwidth())\n    .attr(\"fill\", d => colourScale(d.value));\n\n  // Add x-axis labels\n  svg.append(\"g\")\n    .attr(\"transform\", `translate(${margin.left},${margin.top})`)\n    .selectAll(\"text\")\n    .data(columns)\n    .join(\"text\")\n    .attr(\"x\", d => xScale(d) + xScale.bandwidth() / 2)\n    .attr(\"y\", -10)\n    .attr(\"text-anchor\", \"middle\")\n    .text(d => d)\n    .style(\"font-size\", \"12px\");\n\n  // Add y-axis labels\n  svg.append(\"g\")\n    .attr(\"transform\", `translate(${margin.left},${margin.top})`)\n    .selectAll(\"text\")\n    .data(rows)\n    .join(\"text\")\n    .attr(\"x\", -10)\n    .attr(\"y\", d => yScale(d) + yScale.bandwidth() / 2)\n    .attr(\"dy\", \"0.35em\")\n    .attr(\"text-anchor\", \"end\")\n    .text(d => d)\n    .style(\"font-size\", \"12px\");\n\n  // Add colour legend\n  const legendWidth = 20;\n  const legendHeight = 200;\n  const legend = svg.append(\"g\")\n    .attr(\"transform\", `translate(${width - 60},${margin.top})`);\n\n  const legendScale = d3.scaleLinear()\n    .domain(colourScale.domain())\n    .range([legendHeight, 0]);\n\n  const legendAxis = d3.axisRight(legendScale)\n    .ticks(5);\n\n  // Draw colour gradient in legend\n  for (let i = 0; i < legendHeight; i++) {\n    legend.append(\"rect\")\n      .attr(\"y\", i)\n      .attr(\"width\", legendWidth)\n      .attr(\"height\", 1)\n      .attr(\"fill\", colourScale(legendScale.invert(i)));\n  }\n\n  legend.append(\"g\")\n    .attr(\"transform\", `translate(${legendWidth},0)`)\n    .call(legendAxis);\n}\n```\n\n### Pie chart\n\n```javascript\nconst pie = d3.pie()\n  .value(d => d.value)\n  .sort(null);\n\nconst arc = d3.arc()\n  .innerRadius(0)\n  .outerRadius(Math.min(width, height) / 2 - 20);\n\nconst colourScale = d3.scaleOrdinal(d3.schemeCategory10);\n\nconst g = svg.append(\"g\")\n  .attr(\"transform\", `translate(${width / 2},${height / 2})`);\n\ng.selectAll(\"path\")\n  .data(pie(data))\n  .join(\"path\")\n  .attr(\"d\", arc)\n  .attr(\"fill\", (d, i) => colourScale(i))\n  .attr(\"stroke\", \"white\")\n  .attr(\"stroke-width\", 2);\n```\n\n### Force-directed network\n\n```javascript\nconst simulation = d3.forceSimulation(nodes)\n  .force(\"link\", d3.forceLink(links).id(d => d.id).distance(100))\n  .force(\"charge\", d3.forceManyBody().strength(-300))\n  .force(\"center\", d3.forceCenter(width / 2, height / 2));\n\nconst link = g.selectAll(\"line\")\n  .data(links)\n  .join(\"line\")\n  .attr(\"stroke\", \"#999\")\n  .attr(\"stroke-width\", 1);\n\nconst node = g.selectAll(\"circle\")\n  .data(nodes)\n  .join(\"circle\")\n  .attr(\"r\", 8)\n  .attr(\"fill\", \"steelblue\")\n  .call(d3.drag()\n    .on(\"start\", dragstarted)\n    .on(\"drag\", dragged)\n    .on(\"end\", dragended));\n\nsimulation.on(\"tick\", () => {\n  link\n    .attr(\"x1\", d => d.source.x)\n    .attr(\"y1\", d => d.source.y)\n    .attr(\"x2\", d => d.target.x)\n    .attr(\"y2\", d => d.target.y);\n  \n  node\n    .attr(\"cx\", d => d.x)\n    .attr(\"cy\", d => d.y);\n});\n\nfunction dragstarted(event) {\n  if (!event.active) simulation.alphaTarget(0.3).restart();\n  event.subject.fx = event.subject.x;\n  event.subject.fy = event.subject.y;\n}\n\nfunction dragged(event) {\n  event.subject.fx = event.x;\n  event.subject.fy = event.y;\n}\n\nfunction dragended(event) {\n  if (!event.active) simulation.alphaTarget(0);\n  event.subject.fx = null;\n  event.subject.fy = null;\n}\n```\n\n## Adding interactivity\n\n### Tooltips\n\n```javascript\n// Create tooltip div (outside SVG)\nconst tooltip = d3.select(\"body\").append(\"div\")\n  .attr(\"class\", \"tooltip\")\n  .style(\"position\", \"absolute\")\n  .style(\"visibility\", \"hidden\")\n  .style(\"background-color\", \"white\")\n  .style(\"border\", \"1px solid #ddd\")\n  .style(\"padding\", \"10px\")\n  .style(\"border-radius\", \"4px\")\n  .style(\"pointer-events\", \"none\");\n\n// Add to elements\ncircles\n  .on(\"mouseover\", function(event, d) {\n    d3.select(this).attr(\"opacity\", 1);\n    tooltip\n      .style(\"visibility\", \"visible\")\n      .html(`<strong>${d.label}</strong><br/>Value: ${d.value}`);\n  })\n  .on(\"mousemove\", function(event) {\n    tooltip\n      .style(\"top\", (event.pageY - 10) + \"px\")\n      .style(\"left\", (event.pageX + 10) + \"px\");\n  })\n  .on(\"mouseout\", function() {\n    d3.select(this).attr(\"opacity\", 0.7);\n    tooltip.style(\"visibility\", \"hidden\");\n  });\n```\n\n### Zoom and pan\n\n```javascript\nconst zoom = d3.zoom()\n  .scaleExtent([0.5, 10])\n  .on(\"zoom\", (event) => {\n    g.attr(\"transform\", event.transform);\n  });\n\nsvg.call(zoom);\n```\n\n### Click interactions\n\n```javascript\ncircles\n  .on(\"click\", function(event, d) {\n    // Handle click (dispatch event, update app state, etc.)\n    console.log(\"Clicked:\", d);\n\n    // Visual feedback\n    d3.selectAll(\"circle\").attr(\"fill\", \"steelblue\");\n    d3.select(this).attr(\"fill\", \"orange\");\n\n    // Optional: dispatch custom event for your framework/app to listen to\n    // window.dispatchEvent(new CustomEvent('chartClick', { detail: d }));\n  });\n```\n\n## Transitions and animations\n\nAdd smooth transitions to visual changes:\n\n```javascript\n// Basic transition\ncircles\n  .transition()\n  .duration(750)\n  .attr(\"r\", 10);\n\n// Chained transitions\ncircles\n  .transition()\n  .duration(500)\n  .attr(\"fill\", \"orange\")\n  .transition()\n  .duration(500)\n  .attr(\"r\", 15);\n\n// Staggered transitions\ncircles\n  .transition()\n  .delay((d, i) => i * 50)\n  .duration(500)\n  .attr(\"cy\", d => yScale(d.value));\n\n// Custom easing\ncircles\n  .transition()\n  .duration(1000)\n  .ease(d3.easeBounceOut)\n  .attr(\"r\", 10);\n```\n\n## Scales reference\n\n### Quantitative scales\n\n```javascript\n// Linear scale\nconst xScale = d3.scaleLinear()\n  .domain([0, 100])\n  .range([0, 500]);\n\n// Log scale (for exponential data)\nconst logScale = d3.scaleLog()\n  .domain([1, 1000])\n  .range([0, 500]);\n\n// Power scale\nconst powScale = d3.scalePow()\n  .exponent(2)\n  .domain([0, 100])\n  .range([0, 500]);\n\n// Time scale\nconst timeScale = d3.scaleTime()\n  .domain([new Date(2020, 0, 1), new Date(2024, 0, 1)])\n  .range([0, 500]);\n```\n\n### Ordinal scales\n\n```javascript\n// Band scale (for bar charts)\nconst bandScale = d3.scaleBand()\n  .domain(['A', 'B', 'C', 'D'])\n  .range([0, 400])\n  .padding(0.1);\n\n// Point scale (for line/scatter categories)\nconst pointScale = d3.scalePoint()\n  .domain(['A', 'B', 'C', 'D'])\n  .range([0, 400]);\n\n// Ordinal scale (for colours)\nconst colourScale = d3.scaleOrdinal(d3.schemeCategory10);\n```\n\n### Sequential scales\n\n```javascript\n// Sequential colour scale\nconst colourScale = d3.scaleSequential(d3.interpolateBlues)\n  .domain([0, 100]);\n\n// Diverging colour scale\nconst divScale = d3.scaleDiverging(d3.interpolateRdBu)\n  .domain([-10, 0, 10]);\n```\n\n## Best practices\n\n### Data preparation\n\nAlways validate and prepare data before visualisation:\n\n```javascript\n// Filter invalid values\nconst cleanData = data.filter(d => d.value != null && !isNaN(d.value));\n\n// Sort data if order matters\nconst sortedData = [...data].sort((a, b) => b.value - a.value);\n\n// Parse dates\nconst parsedData = data.map(d => ({\n  ...d,\n  date: d3.timeParse(\"%Y-%m-%d\")(d.date)\n}));\n```\n\n### Performance optimisation\n\nFor large datasets (>1000 elements):\n\n```javascript\n// Use canvas instead of SVG for many elements\n// Use quadtree for collision detection\n// Simplify paths with d3.line().curve(d3.curveStep)\n// Implement virtual scrolling for large lists\n// Use requestAnimationFrame for custom animations\n```\n\n### Accessibility\n\nMake visualisations accessible:\n\n```javascript\n// Add ARIA labels\nsvg.attr(\"role\", \"img\")\n   .attr(\"aria-label\", \"Bar chart showing quarterly revenue\");\n\n// Add title and description\nsvg.append(\"title\").text(\"Quarterly Revenue 2024\");\nsvg.append(\"desc\").text(\"Bar chart showing revenue growth across four quarters\");\n\n// Ensure sufficient colour contrast\n// Provide keyboard navigation for interactive elements\n// Include data table alternative\n```\n\n### Styling\n\nUse consistent, professional styling:\n\n```javascript\n// Define colour palettes upfront\nconst colours = {\n  primary: '#4A90E2',\n  secondary: '#7B68EE',\n  background: '#F5F7FA',\n  text: '#333333',\n  gridLines: '#E0E0E0'\n};\n\n// Apply consistent typography\nsvg.selectAll(\"text\")\n  .style(\"font-family\", \"Inter, sans-serif\")\n  .style(\"font-size\", \"12px\");\n\n// Use subtle grid lines\ng.selectAll(\".tick line\")\n  .attr(\"stroke\", colours.gridLines)\n  .attr(\"stroke-dasharray\", \"2,2\");\n```\n\n## Common issues and solutions\n\n**Issue**: Axes not appearing\n- Ensure scales have valid domains (check for NaN values)\n- Verify axis is appended to correct group\n- Check transform translations are correct\n\n**Issue**: Transitions not working\n- Call `.transition()` before attribute changes\n- Ensure elements have unique keys for proper data binding\n- Check that useEffect dependencies include all changing data\n\n**Issue**: Responsive sizing not working\n- Use ResizeObserver or window resize listener\n- Update dimensions in state to trigger re-render\n- Ensure SVG has width/height attributes or viewBox\n\n**Issue**: Performance problems\n- Limit number of DOM elements (consider canvas for >1000 items)\n- Debounce resize handlers\n- Use `.join()` instead of separate enter/update/exit selections\n- Avoid unnecessary re-renders by checking dependencies\n\n## Resources\n\n### references/\nContains detailed reference materials:\n- `d3-patterns.md` - Comprehensive collection of visualisation patterns and code examples\n- `scale-reference.md` - Complete guide to d3 scales with examples\n- `colour-schemes.md` - D3 colour schemes and palette recommendations\n\n### assets/\n\nContains boilerplate templates:\n\n- `chart-template.js` - Starter template for basic chart\n- `interactive-template.js` - Template with tooltips, zoom, and interactions\n- `sample-data.json` - Example datasets for testing\n\nThese templates work with vanilla JavaScript, React, Vue, Svelte, or any other JavaScript environment. Adapt them as needed for your specific framework.\n\nTo use these resources, read the relevant files when detailed guidance is needed for specific visualisation types or patterns.\n"
    }
  },
  "asklokesh-claudeskill-loki-mode": {
    "id": "asklokesh-claudeskill-loki-mode",
    "name": "loki-mode",
    "description": "Multi-agent autonomous startup system for Claude Code. Triggers on \"Loki Mode\". Orchestrates 100+ specialized agents across engineering, QA, DevOps, security, data/ML, business operations, marketing, HR, and customer success. Takes PRD to fully deployed, revenue-generating product with zero human intervention. Features Task tool for subagent dispatch, parallel code review with 3 specialized reviewers, severity-based issue triage, distributed task queue with dead letter handling, automatic deployment to cloud providers, A/B testing, customer feedback loops, incident response, circuit breakers, and self-healing. Handles rate limits via distributed state checkpoints and auto-resume with exponential backoff. Requires --dangerously-skip-permissions flag.",
    "repo": {
      "owner": "asklokesh",
      "name": "claudeskill-loki-mode",
      "fullName": "asklokesh/claudeskill-loki-mode",
      "url": "https://github.com/asklokesh/claudeskill-loki-mode",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 425,
      "forks": 93,
      "language": "Python",
      "topics": [],
      "updatedAt": "2026-01-08T12:05:28Z",
      "pushedAt": "2026-01-08T04:49:44Z",
      "createdAt": "2025-12-26T16:26:14Z",
      "license": "MIT License"
    },
    "category": "development",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: loki-mode\ndescription: Multi-agent autonomous startup system for Claude Code. Triggers on \"Loki Mode\". Orchestrates 100+ specialized agents across engineering, QA, DevOps, security, data/ML, business operations, marketing, HR, and customer success. Takes PRD to fully deployed, revenue-generating product with zero human intervention. Features Task tool for subagent dispatch, parallel code review with 3 specialized reviewers, severity-based issue triage, distributed task queue with dead letter handling, automatic deployment to cloud providers, A/B testing, customer feedback loops, incident response, circuit breakers, and self-healing. Handles rate limits via distributed state checkpoints and auto-resume with exponential backoff. Requires --dangerously-skip-permissions flag.\n---\n\n# Loki Mode - Multi-Agent Autonomous Startup System\n\n> **Version 2.32.1** | PRD to Production | Zero Human Intervention\n> Research-enhanced: OpenAI SDK, DeepMind, Anthropic, HN Production Patterns (2025)\n\n---\n\n## Quick Reference\n\n### Critical First Steps (Every Turn)\n1. **READ** `.loki/CONTINUITY.md` - Your working memory + \"Mistakes & Learnings\"\n2. **RETRIEVE** Relevant memories from `.loki/memory/` (episodic patterns, anti-patterns)\n3. **CHECK** `.loki/state/orchestrator.json` - Current phase/metrics\n4. **REVIEW** `.loki/queue/pending.json` - Next tasks\n5. **FOLLOW** RARV cycle: REASON, ACT, REFLECT, **VERIFY** (test your work!)\n6. **OPTIMIZE** Opus=planning, Sonnet=development, Haiku=unit tests/monitoring - 10+ Haiku agents in parallel\n7. **TRACK** Efficiency metrics: tokens, time, agent count per task\n8. **CONSOLIDATE** After task: Update episodic memory, extract patterns to semantic memory\n\n### Key Files (Priority Order)\n| File | Purpose | Update When |\n|------|---------|-------------|\n| `.loki/CONTINUITY.md` | Working memory - what am I doing NOW? | Every turn |\n| `.loki/memory/semantic/` | Generalized patterns & anti-patterns | After task completion |\n| `.loki/memory/episodic/` | Specific interaction traces | After each action |\n| `.loki/metrics/efficiency/` | Task efficiency scores & rewards | After each task |\n| `.loki/specs/openapi.yaml` | API spec - source of truth | Architecture changes |\n| `CLAUDE.md` | Project context - arch & patterns | Significant changes |\n| `.loki/queue/*.json` | Task states | Every task change |\n\n### Decision Tree: What To Do Next?\n\n```\nSTART\n  |\n  +-- Read CONTINUITY.md ----------+\n  |                                |\n  +-- Task in-progress?            |\n  |   +-- YES: Resume              |\n  |   +-- NO: Check pending queue  |\n  |                                |\n  +-- Pending tasks?               |\n  |   +-- YES: Claim highest priority\n  |   +-- NO: Check phase completion\n  |                                |\n  +-- Phase done?                  |\n  |   +-- YES: Advance to next phase\n  |   +-- NO: Generate tasks for phase\n  |                                |\nLOOP <-----------------------------+\n```\n\n### SDLC Phase Flow\n\n```\nBootstrap -> Discovery -> Architecture -> Infrastructure\n     |           |            |              |\n  (Setup)   (Analyze PRD)  (Design)    (Cloud/DB Setup)\n                                             |\nDevelopment <- QA <- Deployment <- Business Ops <- Growth Loop\n     |         |         |            |            |\n (Build)    (Test)   (Release)    (Monitor)    (Iterate)\n```\n\n### Essential Patterns\n\n**Spec-First:** `OpenAPI -> Tests -> Code -> Validate`\n**Code Review:** `Blind Review (parallel) -> Debate (if disagree) -> Devil's Advocate -> Merge`\n**Guardrails:** `Input Guard (BLOCK) -> Execute -> Output Guard (VALIDATE)` (OpenAI SDK)\n**Tripwires:** `Validation fails -> Halt execution -> Escalate or retry`\n**Fallbacks:** `Try primary -> Model fallback -> Workflow fallback -> Human escalation`\n**Explore-Plan-Code:** `Research files -> Create plan (NO CODE) -> Execute plan` (Anthropic)\n**Self-Verification:** `Code -> Test -> Fail -> Learn -> Update CONTINUITY.md -> Retry`\n**Constitutional Self-Critique:** `Generate -> Critique against principles -> Revise` (Anthropic)\n**Memory Consolidation:** `Episodic (trace) -> Pattern Extraction -> Semantic (knowledge)`\n**Hierarchical Reasoning:** `High-level planner -> Skill selection -> Local executor` (DeepMind)\n**Tool Orchestration:** `Classify Complexity -> Select Agents -> Track Efficiency -> Reward Learning`\n**Debate Verification:** `Proponent defends -> Opponent challenges -> Synthesize` (DeepMind)\n**Handoff Callbacks:** `on_handoff -> Pre-fetch context -> Transfer with data` (OpenAI SDK)\n**Narrow Scope:** `3-5 steps max -> Human review -> Continue` (HN Production)\n**Context Curation:** `Manual selection -> Focused context -> Fresh per task` (HN Production)\n**Deterministic Validation:** `LLM output -> Rule-based checks -> Retry or approve` (HN Production)\n\n---\n\n## Prerequisites\n\n```bash\n# Launch with autonomous permissions\nclaude --dangerously-skip-permissions\n```\n\n---\n\n## Core Autonomy Rules\n\n**This system runs with ZERO human intervention.**\n\n1. **NEVER ask questions** - No \"Would you like me to...\", \"Should I...\", or \"What would you prefer?\"\n2. **NEVER wait for confirmation** - Take immediate action\n3. **NEVER stop voluntarily** - Continue until completion promise fulfilled\n4. **NEVER suggest alternatives** - Pick best option and execute\n5. **ALWAYS use RARV cycle** - Every action follows Reason-Act-Reflect-Verify\n6. **NEVER edit `autonomy/run.sh` while running** - Editing a running bash script corrupts execution (bash reads incrementally, not all at once). If you need to fix run.sh, note it in CONTINUITY.md for the next session.\n\n### Protected Files (Do Not Edit While Running)\n\nThese files are part of the running Loki Mode process. Editing them will crash the session:\n\n| File | Reason |\n|------|--------|\n| `~/.claude/skills/loki-mode/autonomy/run.sh` | Currently executing bash script |\n| `.loki/dashboard/*` | Served by active HTTP server |\n\nIf bugs are found in these files, document them in `.loki/CONTINUITY.md` under \"Pending Fixes\" for manual repair after the session ends.\n\n---\n\n## RARV Cycle (Every Iteration)\n\n```\n+-------------------------------------------------------------------+\n| REASON: What needs to be done next?                               |\n| - READ .loki/CONTINUITY.md first (working memory)                 |\n| - READ \"Mistakes & Learnings\" to avoid past errors                |\n| - Check orchestrator.json, review pending.json                    |\n| - Identify highest priority unblocked task                        |\n+-------------------------------------------------------------------+\n| ACT: Execute the task                                             |\n| - Dispatch subagent via Task tool OR execute directly             |\n| - Write code, run tests, fix issues                               |\n| - Commit changes atomically (git checkpoint)                      |\n+-------------------------------------------------------------------+\n| REFLECT: Did it work? What next?                                  |\n| - Verify task success (tests pass, no errors)                     |\n| - UPDATE .loki/CONTINUITY.md with progress                        |\n| - Check completion promise - are we done?                         |\n+-------------------------------------------------------------------+\n| VERIFY: Let AI test its own work (2-3x quality improvement)       |\n| - Run automated tests (unit, integration, E2E)                    |\n| - Check compilation/build (no errors or warnings)                 |\n| - Verify against spec (.loki/specs/openapi.yaml)                  |\n|                                                                   |\n| IF VERIFICATION FAILS:                                            |\n|   1. Capture error details (stack trace, logs)                    |\n|   2. Analyze root cause                                           |\n|   3. UPDATE CONTINUITY.md \"Mistakes & Learnings\"                  |\n|   4. Rollback to last good git checkpoint (if needed)             |\n|   5. Apply learning and RETRY from REASON                         |\n+-------------------------------------------------------------------+\n```\n\n---\n\n## Model Selection Strategy\n\n**CRITICAL: Use the right model for each task type. Opus is ONLY for planning/architecture.**\n\n| Model | Use For | Examples |\n|-------|---------|----------|\n| **Opus 4.5** | PLANNING ONLY - Architecture & high-level decisions | System design, architecture decisions, planning, security audits |\n| **Sonnet 4.5** | DEVELOPMENT - Implementation & functional testing | Feature implementation, API endpoints, bug fixes, integration/E2E tests |\n| **Haiku 4.5** | OPERATIONS - Simple tasks & monitoring | Unit tests, docs, bash commands, linting, monitoring, file operations |\n\n### Task Tool Model Parameter\n```python\n# Opus for planning/architecture ONLY\nTask(subagent_type=\"Plan\", model=\"opus\", description=\"Design system architecture\", prompt=\"...\")\n\n# Sonnet for development and functional testing\nTask(subagent_type=\"general-purpose\", description=\"Implement API endpoint\", prompt=\"...\")\nTask(subagent_type=\"general-purpose\", description=\"Write integration tests\", prompt=\"...\")\n\n# Haiku for unit tests, monitoring, and simple tasks (PREFER THIS for speed)\nTask(subagent_type=\"general-purpose\", model=\"haiku\", description=\"Run unit tests\", prompt=\"...\")\nTask(subagent_type=\"general-purpose\", model=\"haiku\", description=\"Check service health\", prompt=\"...\")\n```\n\n### Opus Task Categories (RESTRICTED - Planning Only)\n- System architecture design\n- High-level planning and strategy\n- Security audits and threat modeling\n- Major refactoring decisions\n- Technology selection\n\n### Sonnet Task Categories (Development)\n- Feature implementation\n- API endpoint development\n- Bug fixes (non-trivial)\n- Integration tests and E2E tests\n- Code refactoring\n- Database migrations\n\n### Haiku Task Categories (Operations - Use Extensively)\n- Writing/running unit tests\n- Generating documentation\n- Running bash commands (npm install, git operations)\n- Simple bug fixes (typos, imports, formatting)\n- File operations, linting, static analysis\n- Monitoring, health checks, log analysis\n- Simple data transformations, boilerplate generation\n\n### Parallelization Strategy\n```python\n# Launch 10+ Haiku agents in parallel for unit test suite\nfor test_file in test_files:\n    Task(subagent_type=\"general-purpose\", model=\"haiku\",\n         description=f\"Run unit tests: {test_file}\",\n         run_in_background=True)\n```\n\n---\n\n## Tool Orchestration & Efficiency\n\n**Inspired by NVIDIA ToolOrchestra:** Track efficiency, learn from rewards, adapt agent selection.\n\n### Efficiency Metrics (Track Every Task)\n\n| Metric | What to Track | Store In |\n|--------|---------------|----------|\n| Wall time | Seconds from start to completion | `.loki/metrics/efficiency/` |\n| Agent count | Number of subagents spawned | `.loki/metrics/efficiency/` |\n| Retry count | Attempts before success | `.loki/metrics/efficiency/` |\n| Model usage | Haiku/Sonnet/Opus call distribution | `.loki/metrics/efficiency/` |\n\n### Reward Signals (Learn From Outcomes)\n\n```\nOUTCOME REWARD:  +1.0 (success) | 0.0 (partial) | -1.0 (failure)\nEFFICIENCY REWARD: 0.0-1.0 based on resources vs baseline\nPREFERENCE REWARD: Inferred from user actions (commit/revert/edit)\n```\n\n### Dynamic Agent Selection by Complexity\n\n| Complexity | Max Agents | Planning | Development | Testing | Review |\n|------------|------------|----------|-------------|---------|--------|\n| Trivial | 1 | - | haiku | haiku | skip |\n| Simple | 2 | - | haiku | haiku | single |\n| Moderate | 4 | sonnet | sonnet | haiku | standard (3 parallel) |\n| Complex | 8 | opus | sonnet | haiku | deep (+ devil's advocate) |\n| Critical | 12 | opus | sonnet | sonnet | exhaustive + human checkpoint |\n\nSee `references/tool-orchestration.md` for full implementation details.\n\n---\n\n## Structured Prompting for Subagents\n\n**Single-Responsibility Principle:** Each agent should have ONE clear goal and narrow scope.\n([UiPath Best Practices](https://www.uipath.com/blog/ai/agent-builder-best-practices))\n\n**Every subagent dispatch MUST include:**\n\n```markdown\n## GOAL (What success looks like)\n[High-level objective, not just the action]\nExample: \"Refactor authentication for maintainability and testability\"\nNOT: \"Refactor the auth file\"\n\n## CONSTRAINTS (What you cannot do)\n- No third-party dependencies without approval\n- Maintain backwards compatibility with v1.x API\n- Keep response time under 200ms\n\n## CONTEXT (What you need to know)\n- Related files: [list with brief descriptions]\n- Previous attempts: [what was tried, why it failed]\n\n## OUTPUT FORMAT (What to deliver)\n- [ ] Pull request with Why/What/Trade-offs description\n- [ ] Unit tests with >90% coverage\n- [ ] Update API documentation\n\n## WHEN COMPLETE\nReport back with: WHY, WHAT, TRADE-OFFS, RISKS\n```\n\n---\n\n## Quality Gates\n\n**Never ship code without passing all quality gates:**\n\n1. **Input Guardrails** - Validate scope, detect injection, check constraints (OpenAI SDK pattern)\n2. **Static Analysis** - CodeQL, ESLint/Pylint, type checking\n3. **Blind Review System** - 3 reviewers in parallel, no visibility of each other's findings\n4. **Anti-Sycophancy Check** - If unanimous approval, run Devil's Advocate reviewer\n5. **Output Guardrails** - Validate code quality, spec compliance, no secrets (tripwire on fail)\n6. **Severity-Based Blocking** - Critical/High/Medium = BLOCK; Low/Cosmetic = TODO comment\n7. **Test Coverage Gates** - Unit: 100% pass, >80% coverage; Integration: 100% pass\n\n**Guardrails Execution Modes:**\n- **Blocking**: Guardrail completes before agent starts (use for expensive operations)\n- **Parallel**: Guardrail runs with agent (use for fast checks, accept token loss risk)\n\n**Research insight:** Blind review + Devil's Advocate reduces false positives by 30% (CONSENSAGENT, 2025).\n**OpenAI insight:** \"Layered defense - multiple specialized guardrails create resilient agents.\"\n\nSee `references/quality-control.md` and `references/openai-patterns.md` for details.\n\n---\n\n## Agent Types Overview\n\nLoki Mode has 37 specialized agent types across 7 swarms. The orchestrator spawns only agents needed for your project.\n\n| Swarm | Agent Count | Examples |\n|-------|-------------|----------|\n| Engineering | 8 | frontend, backend, database, mobile, api, qa, perf, infra |\n| Operations | 8 | devops, sre, security, monitor, incident, release, cost, compliance |\n| Business | 8 | marketing, sales, finance, legal, support, hr, investor, partnerships |\n| Data | 3 | ml, data-eng, analytics |\n| Product | 3 | pm, design, techwriter |\n| Growth | 4 | growth-hacker, community, success, lifecycle |\n| Review | 3 | code, business, security |\n\nSee `references/agent-types.md` for complete definitions and capabilities.\n\n---\n\n## Common Issues & Solutions\n\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Agent stuck/no progress | Lost context | Read `.loki/CONTINUITY.md` first thing every turn |\n| Task repeating | Not checking queue state | Check `.loki/queue/*.json` before claiming |\n| Code review failing | Skipped static analysis | Run static analysis BEFORE AI reviewers |\n| Breaking API changes | Code before spec | Follow Spec-First workflow |\n| Rate limit hit | Too many parallel agents | Check circuit breakers, use exponential backoff |\n| Tests failing after merge | Skipped quality gates | Never bypass Severity-Based Blocking |\n| Can't find what to do | Not following decision tree | Use Decision Tree, check orchestrator.json |\n| Memory/context growing | Not using ledgers | Write to ledgers after completing tasks |\n\n---\n\n## Red Flags - Never Do These\n\n### Implementation Anti-Patterns\n- **NEVER** skip code review between tasks\n- **NEVER** proceed with unfixed Critical/High/Medium issues\n- **NEVER** dispatch reviewers sequentially (always parallel - 3x faster)\n- **NEVER** dispatch multiple implementation subagents in parallel (conflicts)\n- **NEVER** implement without reading task requirements first\n\n### Review Anti-Patterns\n- **NEVER** use sonnet for reviews (always opus for deep analysis)\n- **NEVER** aggregate before all 3 reviewers complete\n- **NEVER** skip re-review after fixes\n\n### System Anti-Patterns\n- **NEVER** delete .loki/state/ directory while running\n- **NEVER** manually edit queue files without file locking\n- **NEVER** skip checkpoints before major operations\n- **NEVER** ignore circuit breaker states\n\n### Always Do These\n- **ALWAYS** launch all 3 reviewers in single message (3 Task calls)\n- **ALWAYS** specify model: \"opus\" for each reviewer\n- **ALWAYS** wait for all reviewers before aggregating\n- **ALWAYS** fix Critical/High/Medium immediately\n- **ALWAYS** re-run ALL 3 reviewers after fixes\n- **ALWAYS** checkpoint state before spawning subagents\n\n---\n\n## Multi-Tiered Fallback System\n\n**Based on OpenAI Agent Safety Patterns:**\n\n### Model-Level Fallbacks\n```\nopus -> sonnet -> haiku (if rate limited or unavailable)\n```\n\n### Workflow-Level Fallbacks\n```\nFull workflow fails -> Simplified workflow -> Decompose to subtasks -> Human escalation\n```\n\n### Human Escalation Triggers\n\n| Trigger | Action |\n|---------|--------|\n| retry_count > 3 | Pause and escalate |\n| domain in [payments, auth, pii] | Require approval |\n| confidence_score < 0.6 | Pause and escalate |\n| wall_time > expected * 3 | Pause and escalate |\n| tokens_used > budget * 0.8 | Pause and escalate |\n\nSee `references/openai-patterns.md` for full fallback implementation.\n\n---\n\n## AGENTS.md Integration\n\n**Read target project's AGENTS.md if exists** (OpenAI/AAIF standard):\n\n```\nContext Priority:\n1. AGENTS.md (closest to current file)\n2. CLAUDE.md (Claude-specific)\n3. .loki/CONTINUITY.md (session state)\n4. Package docs\n5. README.md\n```\n\n---\n\n## Constitutional AI Principles (Anthropic)\n\n**Self-critique against explicit principles, not just learned preferences.**\n\n### Loki Mode Constitution\n\n```yaml\ncore_principles:\n  - \"Never delete production data without explicit backup\"\n  - \"Never commit secrets or credentials to version control\"\n  - \"Never bypass quality gates for speed\"\n  - \"Always verify tests pass before marking task complete\"\n  - \"Never claim completion without running actual tests\"\n  - \"Prefer simple solutions over clever ones\"\n  - \"Document decisions, not just code\"\n  - \"When unsure, reject action or flag for review\"\n```\n\n### Self-Critique Workflow\n\n```\n1. Generate response/code\n2. Critique against each principle\n3. Revise if any principle violated\n4. Only then proceed with action\n```\n\nSee `references/lab-research-patterns.md` for Constitutional AI implementation.\n\n---\n\n## Debate-Based Verification (DeepMind)\n\n**For critical changes, use structured debate between AI critics.**\n\n```\nProponent (defender)  -->  Presents proposal with evidence\n         |\n         v\nOpponent (challenger) -->  Finds flaws, challenges claims\n         |\n         v\nSynthesizer           -->  Weighs arguments, produces verdict\n         |\n         v\nIf disagreement persists --> Escalate to human\n```\n\n**Use for:** Architecture decisions, security-sensitive changes, major refactors.\n\nSee `references/lab-research-patterns.md` for debate verification details.\n\n---\n\n## Production Patterns (HN 2025)\n\n**Battle-tested insights from practitioners building real systems.**\n\n### Narrow Scope Wins\n\n```yaml\ntask_constraints:\n  max_steps_before_review: 3-5\n  characteristics:\n    - Specific, well-defined objectives\n    - Pre-classified inputs\n    - Deterministic success criteria\n    - Verifiable outputs\n```\n\n### Confidence-Based Routing\n\n```\nconfidence >= 0.95  -->  Auto-approve with audit log\nconfidence >= 0.70  -->  Quick human review\nconfidence >= 0.40  -->  Detailed human review\nconfidence < 0.40   -->  Escalate immediately\n```\n\n### Deterministic Outer Loops\n\n**Wrap agent outputs with rule-based validation (NOT LLM-judged):**\n\n```\n1. Agent generates output\n2. Run linter (deterministic)\n3. Run tests (deterministic)\n4. Check compilation (deterministic)\n5. Only then: human or AI review\n```\n\n### Context Engineering\n\n```yaml\nprinciples:\n  - \"Less is more\" - focused beats comprehensive\n  - Manual selection outperforms automatic RAG\n  - Fresh conversations per major task\n  - Remove outdated information aggressively\n\ncontext_budget:\n  target: \"< 10k tokens for context\"\n  reserve: \"90% for model reasoning\"\n```\n\n### Sub-Agents for Context Isolation\n\n**Use sub-agents to prevent token waste on noisy subtasks:**\n\n```\nMain agent (focused) --> Sub-agent (file search)\n                     --> Sub-agent (test running)\n                     --> Sub-agent (linting)\n```\n\nSee `references/production-patterns.md` for full practitioner patterns.\n\n---\n\n## Exit Conditions\n\n| Condition | Action |\n|-----------|--------|\n| Product launched, stable 24h | Enter growth loop mode |\n| Unrecoverable failure | Save state, halt, request human |\n| PRD updated | Diff, create delta tasks, continue |\n| Revenue target hit | Log success, continue optimization |\n| Runway < 30 days | Alert, optimize costs aggressively |\n\n---\n\n## Directory Structure Overview\n\n```\n.loki/\n+-- CONTINUITY.md           # Working memory (read/update every turn)\n+-- specs/\n|   +-- openapi.yaml        # API spec - source of truth\n+-- queue/\n|   +-- pending.json        # Tasks waiting to be claimed\n|   +-- in-progress.json    # Currently executing tasks\n|   +-- completed.json      # Finished tasks\n|   +-- dead-letter.json    # Failed tasks for review\n+-- state/\n|   +-- orchestrator.json   # Master state (phase, metrics)\n|   +-- agents/             # Per-agent state files\n|   +-- circuit-breakers/   # Rate limiting state\n+-- memory/\n|   +-- episodic/           # Specific interaction traces (what happened)\n|   +-- semantic/           # Generalized patterns (how things work)\n|   +-- skills/             # Learned action sequences (how to do X)\n|   +-- ledgers/            # Agent-specific checkpoints\n|   +-- handoffs/           # Agent-to-agent transfers\n+-- metrics/\n|   +-- efficiency/         # Task efficiency scores (time, agents, retries)\n|   +-- rewards/            # Outcome/efficiency/preference rewards\n|   +-- dashboard.json      # Rolling metrics summary\n+-- artifacts/\n    +-- reports/            # Generated reports/dashboards\n```\n\nSee `references/architecture.md` for full structure and state schemas.\n\n---\n\n## Invocation\n\n```\nLoki Mode                           # Start fresh\nLoki Mode with PRD at path/to/prd   # Start with PRD\n```\n\n**Skill Metadata:**\n| Field | Value |\n|-------|-------|\n| Trigger | \"Loki Mode\" or \"Loki Mode with PRD at [path]\" |\n| Skip When | Need human approval, want to review plan first, single small task |\n| Related Skills | subagent-driven-development, executing-plans |\n\n---\n\n## References\n\nDetailed documentation is split into reference files for progressive loading:\n\n| Reference | Content |\n|-----------|---------|\n| `references/core-workflow.md` | Full RARV cycle, CONTINUITY.md template, autonomy rules |\n| `references/quality-control.md` | Quality gates, anti-sycophancy, blind review, severity blocking |\n| `references/openai-patterns.md` | OpenAI Agents SDK: guardrails, tripwires, handoffs, fallbacks |\n| `references/lab-research-patterns.md` | DeepMind + Anthropic: Constitutional AI, debate, world models |\n| `references/production-patterns.md` | HN 2025: What actually works in production, context engineering |\n| `references/advanced-patterns.md` | 2025 research: MAR, Iter-VF, GoalAct, CONSENSAGENT |\n| `references/tool-orchestration.md` | ToolOrchestra patterns: efficiency, rewards, dynamic selection |\n| `references/memory-system.md` | Episodic/semantic memory, consolidation, Zettelkasten linking |\n| `references/agent-types.md` | All 37 agent types with full capabilities |\n| `references/task-queue.md` | Queue system, dead letter handling, circuit breakers |\n| `references/sdlc-phases.md` | All phases with detailed workflows and testing |\n| `references/spec-driven-dev.md` | OpenAPI-first workflow, validation, contract testing |\n| `references/architecture.md` | Directory structure, state schemas, bootstrap |\n| `references/mcp-integration.md` | MCP server capabilities and integration |\n| `references/claude-best-practices.md` | Boris Cherny patterns, thinking mode, ledgers |\n| `references/deployment.md` | Cloud deployment instructions per provider |\n| `references/business-ops.md` | Business operation workflows |\n\n---\n\n**Version:** 2.32.0 | **Lines:** ~600 | **Research-Enhanced: Labs + HN Production Patterns**\n",
      "frontmatter": {
        "name": "loki-mode",
        "description": "Multi-agent autonomous startup system for Claude Code. Triggers on \"Loki Mode\". Orchestrates 100+ specialized agents across engineering, QA, DevOps, security, data/ML, business operations, marketing, HR, and customer success. Takes PRD to fully deployed, revenue-generating product with zero human intervention. Features Task tool for subagent dispatch, parallel code review with 3 specialized reviewers, severity-based issue triage, distributed task queue with dead letter handling, automatic deployment to cloud providers, A/B testing, customer feedback loops, incident response, circuit breakers, and self-healing. Handles rate limits via distributed state checkpoints and auto-resume with exponential backoff. Requires --dangerously-skip-permissions flag."
      },
      "content": "\n# Loki Mode - Multi-Agent Autonomous Startup System\n\n> **Version 2.32.1** | PRD to Production | Zero Human Intervention\n> Research-enhanced: OpenAI SDK, DeepMind, Anthropic, HN Production Patterns (2025)\n\n---\n\n## Quick Reference\n\n### Critical First Steps (Every Turn)\n1. **READ** `.loki/CONTINUITY.md` - Your working memory + \"Mistakes & Learnings\"\n2. **RETRIEVE** Relevant memories from `.loki/memory/` (episodic patterns, anti-patterns)\n3. **CHECK** `.loki/state/orchestrator.json` - Current phase/metrics\n4. **REVIEW** `.loki/queue/pending.json` - Next tasks\n5. **FOLLOW** RARV cycle: REASON, ACT, REFLECT, **VERIFY** (test your work!)\n6. **OPTIMIZE** Opus=planning, Sonnet=development, Haiku=unit tests/monitoring - 10+ Haiku agents in parallel\n7. **TRACK** Efficiency metrics: tokens, time, agent count per task\n8. **CONSOLIDATE** After task: Update episodic memory, extract patterns to semantic memory\n\n### Key Files (Priority Order)\n| File | Purpose | Update When |\n|------|---------|-------------|\n| `.loki/CONTINUITY.md` | Working memory - what am I doing NOW? | Every turn |\n| `.loki/memory/semantic/` | Generalized patterns & anti-patterns | After task completion |\n| `.loki/memory/episodic/` | Specific interaction traces | After each action |\n| `.loki/metrics/efficiency/` | Task efficiency scores & rewards | After each task |\n| `.loki/specs/openapi.yaml` | API spec - source of truth | Architecture changes |\n| `CLAUDE.md` | Project context - arch & patterns | Significant changes |\n| `.loki/queue/*.json` | Task states | Every task change |\n\n### Decision Tree: What To Do Next?\n\n```\nSTART\n  |\n  +-- Read CONTINUITY.md ----------+\n  |                                |\n  +-- Task in-progress?            |\n  |   +-- YES: Resume              |\n  |   +-- NO: Check pending queue  |\n  |                                |\n  +-- Pending tasks?               |\n  |   +-- YES: Claim highest priority\n  |   +-- NO: Check phase completion\n  |                                |\n  +-- Phase done?                  |\n  |   +-- YES: Advance to next phase\n  |   +-- NO: Generate tasks for phase\n  |                                |\nLOOP <-----------------------------+\n```\n\n### SDLC Phase Flow\n\n```\nBootstrap -> Discovery -> Architecture -> Infrastructure\n     |           |            |              |\n  (Setup)   (Analyze PRD)  (Design)    (Cloud/DB Setup)\n                                             |\nDevelopment <- QA <- Deployment <- Business Ops <- Growth Loop\n     |         |         |            |            |\n (Build)    (Test)   (Release)    (Monitor)    (Iterate)\n```\n\n### Essential Patterns\n\n**Spec-First:** `OpenAPI -> Tests -> Code -> Validate`\n**Code Review:** `Blind Review (parallel) -> Debate (if disagree) -> Devil's Advocate -> Merge`\n**Guardrails:** `Input Guard (BLOCK) -> Execute -> Output Guard (VALIDATE)` (OpenAI SDK)\n**Tripwires:** `Validation fails -> Halt execution -> Escalate or retry`\n**Fallbacks:** `Try primary -> Model fallback -> Workflow fallback -> Human escalation`\n**Explore-Plan-Code:** `Research files -> Create plan (NO CODE) -> Execute plan` (Anthropic)\n**Self-Verification:** `Code -> Test -> Fail -> Learn -> Update CONTINUITY.md -> Retry`\n**Constitutional Self-Critique:** `Generate -> Critique against principles -> Revise` (Anthropic)\n**Memory Consolidation:** `Episodic (trace) -> Pattern Extraction -> Semantic (knowledge)`\n**Hierarchical Reasoning:** `High-level planner -> Skill selection -> Local executor` (DeepMind)\n**Tool Orchestration:** `Classify Complexity -> Select Agents -> Track Efficiency -> Reward Learning`\n**Debate Verification:** `Proponent defends -> Opponent challenges -> Synthesize` (DeepMind)\n**Handoff Callbacks:** `on_handoff -> Pre-fetch context -> Transfer with data` (OpenAI SDK)\n**Narrow Scope:** `3-5 steps max -> Human review -> Continue` (HN Production)\n**Context Curation:** `Manual selection -> Focused context -> Fresh per task` (HN Production)\n**Deterministic Validation:** `LLM output -> Rule-based checks -> Retry or approve` (HN Production)\n\n---\n\n## Prerequisites\n\n```bash\n# Launch with autonomous permissions\nclaude --dangerously-skip-permissions\n```\n\n---\n\n## Core Autonomy Rules\n\n**This system runs with ZERO human intervention.**\n\n1. **NEVER ask questions** - No \"Would you like me to...\", \"Should I...\", or \"What would you prefer?\"\n2. **NEVER wait for confirmation** - Take immediate action\n3. **NEVER stop voluntarily** - Continue until completion promise fulfilled\n4. **NEVER suggest alternatives** - Pick best option and execute\n5. **ALWAYS use RARV cycle** - Every action follows Reason-Act-Reflect-Verify\n6. **NEVER edit `autonomy/run.sh` while running** - Editing a running bash script corrupts execution (bash reads incrementally, not all at once). If you need to fix run.sh, note it in CONTINUITY.md for the next session.\n\n### Protected Files (Do Not Edit While Running)\n\nThese files are part of the running Loki Mode process. Editing them will crash the session:\n\n| File | Reason |\n|------|--------|\n| `~/.claude/skills/loki-mode/autonomy/run.sh` | Currently executing bash script |\n| `.loki/dashboard/*` | Served by active HTTP server |\n\nIf bugs are found in these files, document them in `.loki/CONTINUITY.md` under \"Pending Fixes\" for manual repair after the session ends.\n\n---\n\n## RARV Cycle (Every Iteration)\n\n```\n+-------------------------------------------------------------------+\n| REASON: What needs to be done next?                               |\n| - READ .loki/CONTINUITY.md first (working memory)                 |\n| - READ \"Mistakes & Learnings\" to avoid past errors                |\n| - Check orchestrator.json, review pending.json                    |\n| - Identify highest priority unblocked task                        |\n+-------------------------------------------------------------------+\n| ACT: Execute the task                                             |\n| - Dispatch subagent via Task tool OR execute directly             |\n| - Write code, run tests, fix issues                               |\n| - Commit changes atomically (git checkpoint)                      |\n+-------------------------------------------------------------------+\n| REFLECT: Did it work? What next?                                  |\n| - Verify task success (tests pass, no errors)                     |\n| - UPDATE .loki/CONTINUITY.md with progress                        |\n| - Check completion promise - are we done?                         |\n+-------------------------------------------------------------------+\n| VERIFY: Let AI test its own work (2-3x quality improvement)       |\n| - Run automated tests (unit, integration, E2E)                    |\n| - Check compilation/build (no errors or warnings)                 |\n| - Verify against spec (.loki/specs/openapi.yaml)                  |\n|                                                                   |\n| IF VERIFICATION FAILS:                                            |\n|   1. Capture error details (stack trace, logs)                    |\n|   2. Analyze root cause                                           |\n|   3. UPDATE CONTINUITY.md \"Mistakes & Learnings\"                  |\n|   4. Rollback to last good git checkpoint (if needed)             |\n|   5. Apply learning and RETRY from REASON                         |\n+-------------------------------------------------------------------+\n```\n\n---\n\n## Model Selection Strategy\n\n**CRITICAL: Use the right model for each task type. Opus is ONLY for planning/architecture.**\n\n| Model | Use For | Examples |\n|-------|---------|----------|\n| **Opus 4.5** | PLANNING ONLY - Architecture & high-level decisions | System design, architecture decisions, planning, security audits |\n| **Sonnet 4.5** | DEVELOPMENT - Implementation & functional testing | Feature implementation, API endpoints, bug fixes, integration/E2E tests |\n| **Haiku 4.5** | OPERATIONS - Simple tasks & monitoring | Unit tests, docs, bash commands, linting, monitoring, file operations |\n\n### Task Tool Model Parameter\n```python\n# Opus for planning/architecture ONLY\nTask(subagent_type=\"Plan\", model=\"opus\", description=\"Design system architecture\", prompt=\"...\")\n\n# Sonnet for development and functional testing\nTask(subagent_type=\"general-purpose\", description=\"Implement API endpoint\", prompt=\"...\")\nTask(subagent_type=\"general-purpose\", description=\"Write integration tests\", prompt=\"...\")\n\n# Haiku for unit tests, monitoring, and simple tasks (PREFER THIS for speed)\nTask(subagent_type=\"general-purpose\", model=\"haiku\", description=\"Run unit tests\", prompt=\"...\")\nTask(subagent_type=\"general-purpose\", model=\"haiku\", description=\"Check service health\", prompt=\"...\")\n```\n\n### Opus Task Categories (RESTRICTED - Planning Only)\n- System architecture design\n- High-level planning and strategy\n- Security audits and threat modeling\n- Major refactoring decisions\n- Technology selection\n\n### Sonnet Task Categories (Development)\n- Feature implementation\n- API endpoint development\n- Bug fixes (non-trivial)\n- Integration tests and E2E tests\n- Code refactoring\n- Database migrations\n\n### Haiku Task Categories (Operations - Use Extensively)\n- Writing/running unit tests\n- Generating documentation\n- Running bash commands (npm install, git operations)\n- Simple bug fixes (typos, imports, formatting)\n- File operations, linting, static analysis\n- Monitoring, health checks, log analysis\n- Simple data transformations, boilerplate generation\n\n### Parallelization Strategy\n```python\n# Launch 10+ Haiku agents in parallel for unit test suite\nfor test_file in test_files:\n    Task(subagent_type=\"general-purpose\", model=\"haiku\",\n         description=f\"Run unit tests: {test_file}\",\n         run_in_background=True)\n```\n\n---\n\n## Tool Orchestration & Efficiency\n\n**Inspired by NVIDIA ToolOrchestra:** Track efficiency, learn from rewards, adapt agent selection.\n\n### Efficiency Metrics (Track Every Task)\n\n| Metric | What to Track | Store In |\n|--------|---------------|----------|\n| Wall time | Seconds from start to completion | `.loki/metrics/efficiency/` |\n| Agent count | Number of subagents spawned | `.loki/metrics/efficiency/` |\n| Retry count | Attempts before success | `.loki/metrics/efficiency/` |\n| Model usage | Haiku/Sonnet/Opus call distribution | `.loki/metrics/efficiency/` |\n\n### Reward Signals (Learn From Outcomes)\n\n```\nOUTCOME REWARD:  +1.0 (success) | 0.0 (partial) | -1.0 (failure)\nEFFICIENCY REWARD: 0.0-1.0 based on resources vs baseline\nPREFERENCE REWARD: Inferred from user actions (commit/revert/edit)\n```\n\n### Dynamic Agent Selection by Complexity\n\n| Complexity | Max Agents | Planning | Development | Testing | Review |\n|------------|------------|----------|-------------|---------|--------|\n| Trivial | 1 | - | haiku | haiku | skip |\n| Simple | 2 | - | haiku | haiku | single |\n| Moderate | 4 | sonnet | sonnet | haiku | standard (3 parallel) |\n| Complex | 8 | opus | sonnet | haiku | deep (+ devil's advocate) |\n| Critical | 12 | opus | sonnet | sonnet | exhaustive + human checkpoint |\n\nSee `references/tool-orchestration.md` for full implementation details.\n\n---\n\n## Structured Prompting for Subagents\n\n**Single-Responsibility Principle:** Each agent should have ONE clear goal and narrow scope.\n([UiPath Best Practices](https://www.uipath.com/blog/ai/agent-builder-best-practices))\n\n**Every subagent dispatch MUST include:**\n\n```markdown\n## GOAL (What success looks like)\n[High-level objective, not just the action]\nExample: \"Refactor authentication for maintainability and testability\"\nNOT: \"Refactor the auth file\"\n\n## CONSTRAINTS (What you cannot do)\n- No third-party dependencies without approval\n- Maintain backwards compatibility with v1.x API\n- Keep response time under 200ms\n\n## CONTEXT (What you need to know)\n- Related files: [list with brief descriptions]\n- Previous attempts: [what was tried, why it failed]\n\n## OUTPUT FORMAT (What to deliver)\n- [ ] Pull request with Why/What/Trade-offs description\n- [ ] Unit tests with >90% coverage\n- [ ] Update API documentation\n\n## WHEN COMPLETE\nReport back with: WHY, WHAT, TRADE-OFFS, RISKS\n```\n\n---\n\n## Quality Gates\n\n**Never ship code without passing all quality gates:**\n\n1. **Input Guardrails** - Validate scope, detect injection, check constraints (OpenAI SDK pattern)\n2. **Static Analysis** - CodeQL, ESLint/Pylint, type checking\n3. **Blind Review System** - 3 reviewers in parallel, no visibility of each other's findings\n4. **Anti-Sycophancy Check** - If unanimous approval, run Devil's Advocate reviewer\n5. **Output Guardrails** - Validate code quality, spec compliance, no secrets (tripwire on fail)\n6. **Severity-Based Blocking** - Critical/High/Medium = BLOCK; Low/Cosmetic = TODO comment\n7. **Test Coverage Gates** - Unit: 100% pass, >80% coverage; Integration: 100% pass\n\n**Guardrails Execution Modes:**\n- **Blocking**: Guardrail completes before agent starts (use for expensive operations)\n- **Parallel**: Guardrail runs with agent (use for fast checks, accept token loss risk)\n\n**Research insight:** Blind review + Devil's Advocate reduces false positives by 30% (CONSENSAGENT, 2025).\n**OpenAI insight:** \"Layered defense - multiple specialized guardrails create resilient agents.\"\n\nSee `references/quality-control.md` and `references/openai-patterns.md` for details.\n\n---\n\n## Agent Types Overview\n\nLoki Mode has 37 specialized agent types across 7 swarms. The orchestrator spawns only agents needed for your project.\n\n| Swarm | Agent Count | Examples |\n|-------|-------------|----------|\n| Engineering | 8 | frontend, backend, database, mobile, api, qa, perf, infra |\n| Operations | 8 | devops, sre, security, monitor, incident, release, cost, compliance |\n| Business | 8 | marketing, sales, finance, legal, support, hr, investor, partnerships |\n| Data | 3 | ml, data-eng, analytics |\n| Product | 3 | pm, design, techwriter |\n| Growth | 4 | growth-hacker, community, success, lifecycle |\n| Review | 3 | code, business, security |\n\nSee `references/agent-types.md` for complete definitions and capabilities.\n\n---\n\n## Common Issues & Solutions\n\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Agent stuck/no progress | Lost context | Read `.loki/CONTINUITY.md` first thing every turn |\n| Task repeating | Not checking queue state | Check `.loki/queue/*.json` before claiming |\n| Code review failing | Skipped static analysis | Run static analysis BEFORE AI reviewers |\n| Breaking API changes | Code before spec | Follow Spec-First workflow |\n| Rate limit hit | Too many parallel agents | Check circuit breakers, use exponential backoff |\n| Tests failing after merge | Skipped quality gates | Never bypass Severity-Based Blocking |\n| Can't find what to do | Not following decision tree | Use Decision Tree, check orchestrator.json |\n| Memory/context growing | Not using ledgers | Write to ledgers after completing tasks |\n\n---\n\n## Red Flags - Never Do These\n\n### Implementation Anti-Patterns\n- **NEVER** skip code review between tasks\n- **NEVER** proceed with unfixed Critical/High/Medium issues\n- **NEVER** dispatch reviewers sequentially (always parallel - 3x faster)\n- **NEVER** dispatch multiple implementation subagents in parallel (conflicts)\n- **NEVER** implement without reading task requirements first\n\n### Review Anti-Patterns\n- **NEVER** use sonnet for reviews (always opus for deep analysis)\n- **NEVER** aggregate before all 3 reviewers complete\n- **NEVER** skip re-review after fixes\n\n### System Anti-Patterns\n- **NEVER** delete .loki/state/ directory while running\n- **NEVER** manually edit queue files without file locking\n- **NEVER** skip checkpoints before major operations\n- **NEVER** ignore circuit breaker states\n\n### Always Do These\n- **ALWAYS** launch all 3 reviewers in single message (3 Task calls)\n- **ALWAYS** specify model: \"opus\" for each reviewer\n- **ALWAYS** wait for all reviewers before aggregating\n- **ALWAYS** fix Critical/High/Medium immediately\n- **ALWAYS** re-run ALL 3 reviewers after fixes\n- **ALWAYS** checkpoint state before spawning subagents\n\n---\n\n## Multi-Tiered Fallback System\n\n**Based on OpenAI Agent Safety Patterns:**\n\n### Model-Level Fallbacks\n```\nopus -> sonnet -> haiku (if rate limited or unavailable)\n```\n\n### Workflow-Level Fallbacks\n```\nFull workflow fails -> Simplified workflow -> Decompose to subtasks -> Human escalation\n```\n\n### Human Escalation Triggers\n\n| Trigger | Action |\n|---------|--------|\n| retry_count > 3 | Pause and escalate |\n| domain in [payments, auth, pii] | Require approval |\n| confidence_score < 0.6 | Pause and escalate |\n| wall_time > expected * 3 | Pause and escalate |\n| tokens_used > budget * 0.8 | Pause and escalate |\n\nSee `references/openai-patterns.md` for full fallback implementation.\n\n---\n\n## AGENTS.md Integration\n\n**Read target project's AGENTS.md if exists** (OpenAI/AAIF standard):\n\n```\nContext Priority:\n1. AGENTS.md (closest to current file)\n2. CLAUDE.md (Claude-specific)\n3. .loki/CONTINUITY.md (session state)\n4. Package docs\n5. README.md\n```\n\n---\n\n## Constitutional AI Principles (Anthropic)\n\n**Self-critique against explicit principles, not just learned preferences.**\n\n### Loki Mode Constitution\n\n```yaml\ncore_principles:\n  - \"Never delete production data without explicit backup\"\n  - \"Never commit secrets or credentials to version control\"\n  - \"Never bypass quality gates for speed\"\n  - \"Always verify tests pass before marking task complete\"\n  - \"Never claim completion without running actual tests\"\n  - \"Prefer simple solutions over clever ones\"\n  - \"Document decisions, not just code\"\n  - \"When unsure, reject action or flag for review\"\n```\n\n### Self-Critique Workflow\n\n```\n1. Generate response/code\n2. Critique against each principle\n3. Revise if any principle violated\n4. Only then proceed with action\n```\n\nSee `references/lab-research-patterns.md` for Constitutional AI implementation.\n\n---\n\n## Debate-Based Verification (DeepMind)\n\n**For critical changes, use structured debate between AI critics.**\n\n```\nProponent (defender)  -->  Presents proposal with evidence\n         |\n         v\nOpponent (challenger) -->  Finds flaws, challenges claims\n         |\n         v\nSynthesizer           -->  Weighs arguments, produces verdict\n         |\n         v\nIf disagreement persists --> Escalate to human\n```\n\n**Use for:** Architecture decisions, security-sensitive changes, major refactors.\n\nSee `references/lab-research-patterns.md` for debate verification details.\n\n---\n\n## Production Patterns (HN 2025)\n\n**Battle-tested insights from practitioners building real systems.**\n\n### Narrow Scope Wins\n\n```yaml\ntask_constraints:\n  max_steps_before_review: 3-5\n  characteristics:\n    - Specific, well-defined objectives\n    - Pre-classified inputs\n    - Deterministic success criteria\n    - Verifiable outputs\n```\n\n### Confidence-Based Routing\n\n```\nconfidence >= 0.95  -->  Auto-approve with audit log\nconfidence >= 0.70  -->  Quick human review\nconfidence >= 0.40  -->  Detailed human review\nconfidence < 0.40   -->  Escalate immediately\n```\n\n### Deterministic Outer Loops\n\n**Wrap agent outputs with rule-based validation (NOT LLM-judged):**\n\n```\n1. Agent generates output\n2. Run linter (deterministic)\n3. Run tests (deterministic)\n4. Check compilation (deterministic)\n5. Only then: human or AI review\n```\n\n### Context Engineering\n\n```yaml\nprinciples:\n  - \"Less is more\" - focused beats comprehensive\n  - Manual selection outperforms automatic RAG\n  - Fresh conversations per major task\n  - Remove outdated information aggressively\n\ncontext_budget:\n  target: \"< 10k tokens for context\"\n  reserve: \"90% for model reasoning\"\n```\n\n### Sub-Agents for Context Isolation\n\n**Use sub-agents to prevent token waste on noisy subtasks:**\n\n```\nMain agent (focused) --> Sub-agent (file search)\n                     --> Sub-agent (test running)\n                     --> Sub-agent (linting)\n```\n\nSee `references/production-patterns.md` for full practitioner patterns.\n\n---\n\n## Exit Conditions\n\n| Condition | Action |\n|-----------|--------|\n| Product launched, stable 24h | Enter growth loop mode |\n| Unrecoverable failure | Save state, halt, request human |\n| PRD updated | Diff, create delta tasks, continue |\n| Revenue target hit | Log success, continue optimization |\n| Runway < 30 days | Alert, optimize costs aggressively |\n\n---\n\n## Directory Structure Overview\n\n```\n.loki/\n+-- CONTINUITY.md           # Working memory (read/update every turn)\n+-- specs/\n|   +-- openapi.yaml        # API spec - source of truth\n+-- queue/\n|   +-- pending.json        # Tasks waiting to be claimed\n|   +-- in-progress.json    # Currently executing tasks\n|   +-- completed.json      # Finished tasks\n|   +-- dead-letter.json    # Failed tasks for review\n+-- state/\n|   +-- orchestrator.json   # Master state (phase, metrics)\n|   +-- agents/             # Per-agent state files\n|   +-- circuit-breakers/   # Rate limiting state\n+-- memory/\n|   +-- episodic/           # Specific interaction traces (what happened)\n|   +-- semantic/           # Generalized patterns (how things work)\n|   +-- skills/             # Learned action sequences (how to do X)\n|   +-- ledgers/            # Agent-specific checkpoints\n|   +-- handoffs/           # Agent-to-agent transfers\n+-- metrics/\n|   +-- efficiency/         # Task efficiency scores (time, agents, retries)\n|   +-- rewards/            # Outcome/efficiency/preference rewards\n|   +-- dashboard.json      # Rolling metrics summary\n+-- artifacts/\n    +-- reports/            # Generated reports/dashboards\n```\n\nSee `references/architecture.md` for full structure and state schemas.\n\n---\n\n## Invocation\n\n```\nLoki Mode                           # Start fresh\nLoki Mode with PRD at path/to/prd   # Start with PRD\n```\n\n**Skill Metadata:**\n| Field | Value |\n|-------|-------|\n| Trigger | \"Loki Mode\" or \"Loki Mode with PRD at [path]\" |\n| Skip When | Need human approval, want to review plan first, single small task |\n| Related Skills | subagent-driven-development, executing-plans |\n\n---\n\n## References\n\nDetailed documentation is split into reference files for progressive loading:\n\n| Reference | Content |\n|-----------|---------|\n| `references/core-workflow.md` | Full RARV cycle, CONTINUITY.md template, autonomy rules |\n| `references/quality-control.md` | Quality gates, anti-sycophancy, blind review, severity blocking |\n| `references/openai-patterns.md` | OpenAI Agents SDK: guardrails, tripwires, handoffs, fallbacks |\n| `references/lab-research-patterns.md` | DeepMind + Anthropic: Constitutional AI, debate, world models |\n| `references/production-patterns.md` | HN 2025: What actually works in production, context engineering |\n| `references/advanced-patterns.md` | 2025 research: MAR, Iter-VF, GoalAct, CONSENSAGENT |\n| `references/tool-orchestration.md` | ToolOrchestra patterns: efficiency, rewards, dynamic selection |\n| `references/memory-system.md` | Episodic/semantic memory, consolidation, Zettelkasten linking |\n| `references/agent-types.md` | All 37 agent types with full capabilities |\n| `references/task-queue.md` | Queue system, dead letter handling, circuit breakers |\n| `references/sdlc-phases.md` | All phases with detailed workflows and testing |\n| `references/spec-driven-dev.md` | OpenAPI-first workflow, validation, contract testing |\n| `references/architecture.md` | Directory structure, state schemas, bootstrap |\n| `references/mcp-integration.md` | MCP server capabilities and integration |\n| `references/claude-best-practices.md` | Boris Cherny patterns, thinking mode, ledgers |\n| `references/deployment.md` | Cloud deployment instructions per provider |\n| `references/business-ops.md` | Business operation workflows |\n\n---\n\n**Version:** 2.32.0 | **Lines:** ~600 | **Research-Enhanced: Labs + HN Production Patterns**\n"
    }
  },
  "alirezarezvani-claude-skills-content-creator": {
    "id": "alirezarezvani-claude-skills-content-creator",
    "name": "content-creator",
    "description": "Create SEO-optimized marketing content with consistent brand voice. Includes brand voice analyzer, SEO optimizer, content frameworks, and social media templates. Use when writing blog posts, creating social media content, analyzing brand voice, optimizing SEO, planning content calendars, or when user mentions content creation, brand voice, SEO optimization, social media marketing, or content strategy.",
    "repo": {
      "owner": "alirezarezvani",
      "name": "claude-skills",
      "fullName": "alirezarezvani/claude-skills",
      "url": "https://github.com/alirezarezvani/claude-skills/tree/main/marketing-skill/content-creator",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 521,
      "forks": 103,
      "language": "Python",
      "topics": [
        "agentic-ai",
        "agentic-coding",
        "anthropic-claude",
        "claude-ai",
        "claude-ai-skills",
        "claude-code",
        "claude-code-skills",
        "claude-skills",
        "claudecode-subagents"
      ],
      "updatedAt": "2026-01-08T12:36:14Z",
      "pushedAt": "2026-01-08T08:39:21Z",
      "createdAt": "2025-10-19T04:04:05Z",
      "license": "MIT License"
    },
    "category": "marketing",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: content-creator\ndescription: Create SEO-optimized marketing content with consistent brand voice. Includes brand voice analyzer, SEO optimizer, content frameworks, and social media templates. Use when writing blog posts, creating social media content, analyzing brand voice, optimizing SEO, planning content calendars, or when user mentions content creation, brand voice, SEO optimization, social media marketing, or content strategy.\nlicense: MIT\nmetadata:\n  version: 1.0.0\n  author: Alireza Rezvani\n  category: marketing\n  domain: content-marketing\n  updated: 2025-10-20\n  python-tools: brand_voice_analyzer.py, seo_optimizer.py\n  tech-stack: SEO, social-media-platforms\n---\n\n# Content Creator\n\nProfessional-grade brand voice analysis, SEO optimization, and platform-specific content frameworks.\n\n## Keywords\ncontent creation, blog posts, SEO, brand voice, social media, content calendar, marketing content, content strategy, content marketing, brand consistency, content optimization, social media marketing, content planning, blog writing, content frameworks, brand guidelines, social media strategy\n\n## Quick Start\n\n### For Brand Voice Development\n1. Run `scripts/brand_voice_analyzer.py` on existing content to establish baseline\n2. Review `references/brand_guidelines.md` to select voice attributes\n3. Apply chosen voice consistently across all content\n\n### For Blog Content Creation\n1. Choose template from `references/content_frameworks.md`\n2. Research keywords for topic\n3. Write content following template structure\n4. Run `scripts/seo_optimizer.py [file] [primary-keyword]` to optimize\n5. Apply recommendations before publishing\n\n### For Social Media Content\n1. Review platform best practices in `references/social_media_optimization.md`\n2. Use appropriate template from `references/content_frameworks.md`\n3. Optimize based on platform-specific guidelines\n4. Schedule using `assets/content_calendar_template.md`\n\n## Core Workflows\n\n### Establishing Brand Voice (First Time Setup)\n\nWhen creating content for a new brand or client:\n\n1. **Analyze Existing Content** (if available)\n   ```bash\n   python scripts/brand_voice_analyzer.py existing_content.txt\n   ```\n   \n2. **Define Voice Attributes**\n   - Review brand personality archetypes in `references/brand_guidelines.md`\n   - Select primary and secondary archetypes\n   - Choose 3-5 tone attributes\n   - Document in brand guidelines\n\n3. **Create Voice Sample**\n   - Write 3 sample pieces in chosen voice\n   - Test consistency using analyzer\n   - Refine based on results\n\n### Creating SEO-Optimized Blog Posts\n\n1. **Keyword Research**\n   - Identify primary keyword (search volume 500-5000/month)\n   - Find 3-5 secondary keywords\n   - List 10-15 LSI keywords\n\n2. **Content Structure**\n   - Use blog template from `references/content_frameworks.md`\n   - Include keyword in title, first paragraph, and 2-3 H2s\n   - Aim for 1,500-2,500 words for comprehensive coverage\n\n3. **Optimization Check**\n   ```bash\n   python scripts/seo_optimizer.py blog_post.md \"primary keyword\" \"secondary,keywords,list\"\n   ```\n\n4. **Apply SEO Recommendations**\n   - Adjust keyword density to 1-3%\n   - Ensure proper heading structure\n   - Add internal and external links\n   - Optimize meta description\n\n### Social Media Content Creation\n\n1. **Platform Selection**\n   - Identify primary platforms based on audience\n   - Review platform-specific guidelines in `references/social_media_optimization.md`\n\n2. **Content Adaptation**\n   - Start with blog post or core message\n   - Use repurposing matrix from `references/content_frameworks.md`\n   - Adapt for each platform following templates\n\n3. **Optimization Checklist**\n   - Platform-appropriate length\n   - Optimal posting time\n   - Correct image dimensions\n   - Platform-specific hashtags\n   - Engagement elements (polls, questions)\n\n### Content Calendar Planning\n\n1. **Monthly Planning**\n   - Copy `assets/content_calendar_template.md`\n   - Set monthly goals and KPIs\n   - Identify key campaigns/themes\n\n2. **Weekly Distribution**\n   - Follow 40/25/25/10 content pillar ratio\n   - Balance platforms throughout week\n   - Align with optimal posting times\n\n3. **Batch Creation**\n   - Create all weekly content in one session\n   - Maintain consistent voice across pieces\n   - Prepare all visual assets together\n\n## Key Scripts\n\n### brand_voice_analyzer.py\nAnalyzes text content for voice characteristics, readability, and consistency.\n\n**Usage**: `python scripts/brand_voice_analyzer.py <file> [json|text]`\n\n**Returns**:\n- Voice profile (formality, tone, perspective)\n- Readability score\n- Sentence structure analysis\n- Improvement recommendations\n\n### seo_optimizer.py\nAnalyzes content for SEO optimization and provides actionable recommendations.\n\n**Usage**: `python scripts/seo_optimizer.py <file> [primary_keyword] [secondary_keywords]`\n\n**Returns**:\n- SEO score (0-100)\n- Keyword density analysis\n- Structure assessment\n- Meta tag suggestions\n- Specific optimization recommendations\n\n## Reference Guides\n\n### When to Use Each Reference\n\n**references/brand_guidelines.md**\n- Setting up new brand voice\n- Ensuring consistency across content\n- Training new team members\n- Resolving voice/tone questions\n\n**references/content_frameworks.md**\n- Starting any new content piece\n- Structuring different content types\n- Creating content templates\n- Planning content repurposing\n\n**references/social_media_optimization.md**\n- Platform-specific optimization\n- Hashtag strategy development\n- Understanding algorithm factors\n- Setting up analytics tracking\n\n## Best Practices\n\n### Content Creation Process\n1. Always start with audience need/pain point\n2. Research before writing\n3. Create outline using templates\n4. Write first draft without editing\n5. Optimize for SEO\n6. Edit for brand voice\n7. Proofread and fact-check\n8. Optimize for platform\n9. Schedule strategically\n\n### Quality Indicators\n- SEO score above 75/100\n- Readability appropriate for audience\n- Consistent brand voice throughout\n- Clear value proposition\n- Actionable takeaways\n- Proper visual formatting\n- Platform-optimized\n\n### Common Pitfalls to Avoid\n- Writing before researching keywords\n- Ignoring platform-specific requirements\n- Inconsistent brand voice\n- Over-optimizing for SEO (keyword stuffing)\n- Missing clear CTAs\n- Publishing without proofreading\n- Ignoring analytics feedback\n\n## Performance Metrics\n\nTrack these KPIs for content success:\n\n### Content Metrics\n- Organic traffic growth\n- Average time on page\n- Bounce rate\n- Social shares\n- Backlinks earned\n\n### Engagement Metrics\n- Comments and discussions\n- Email click-through rates\n- Social media engagement rate\n- Content downloads\n- Form submissions\n\n### Business Metrics\n- Leads generated\n- Conversion rate\n- Customer acquisition cost\n- Revenue attribution\n- ROI per content piece\n\n## Integration Points\n\nThis skill works best with:\n- Analytics platforms (Google Analytics, social media insights)\n- SEO tools (for keyword research)\n- Design tools (for visual content)\n- Scheduling platforms (for content distribution)\n- Email marketing systems (for newsletter content)\n\n## Quick Commands\n\n```bash\n# Analyze brand voice\npython scripts/brand_voice_analyzer.py content.txt\n\n# Optimize for SEO\npython scripts/seo_optimizer.py article.md \"main keyword\"\n\n# Check content against brand guidelines\ngrep -f references/brand_guidelines.md content.txt\n\n# Create monthly calendar\ncp assets/content_calendar_template.md this_month_calendar.md\n```\n",
      "frontmatter": {
        "name": "content-creator",
        "description": "Create SEO-optimized marketing content with consistent brand voice. Includes brand voice analyzer, SEO optimizer, content frameworks, and social media templates. Use when writing blog posts, creating social media content, analyzing brand voice, optimizing SEO, planning content calendars, or when user mentions content creation, brand voice, SEO optimization, social media marketing, or content strategy.",
        "license": "MIT",
        "metadata": {
          "version": "1.0.0",
          "author": "Alireza Rezvani",
          "category": "marketing",
          "domain": "content-marketing",
          "updated": "2025-10-20T00:00:00.000Z",
          "python-tools": "brand_voice_analyzer.py, seo_optimizer.py",
          "tech-stack": "SEO, social-media-platforms"
        }
      },
      "content": "\n# Content Creator\n\nProfessional-grade brand voice analysis, SEO optimization, and platform-specific content frameworks.\n\n## Keywords\ncontent creation, blog posts, SEO, brand voice, social media, content calendar, marketing content, content strategy, content marketing, brand consistency, content optimization, social media marketing, content planning, blog writing, content frameworks, brand guidelines, social media strategy\n\n## Quick Start\n\n### For Brand Voice Development\n1. Run `scripts/brand_voice_analyzer.py` on existing content to establish baseline\n2. Review `references/brand_guidelines.md` to select voice attributes\n3. Apply chosen voice consistently across all content\n\n### For Blog Content Creation\n1. Choose template from `references/content_frameworks.md`\n2. Research keywords for topic\n3. Write content following template structure\n4. Run `scripts/seo_optimizer.py [file] [primary-keyword]` to optimize\n5. Apply recommendations before publishing\n\n### For Social Media Content\n1. Review platform best practices in `references/social_media_optimization.md`\n2. Use appropriate template from `references/content_frameworks.md`\n3. Optimize based on platform-specific guidelines\n4. Schedule using `assets/content_calendar_template.md`\n\n## Core Workflows\n\n### Establishing Brand Voice (First Time Setup)\n\nWhen creating content for a new brand or client:\n\n1. **Analyze Existing Content** (if available)\n   ```bash\n   python scripts/brand_voice_analyzer.py existing_content.txt\n   ```\n   \n2. **Define Voice Attributes**\n   - Review brand personality archetypes in `references/brand_guidelines.md`\n   - Select primary and secondary archetypes\n   - Choose 3-5 tone attributes\n   - Document in brand guidelines\n\n3. **Create Voice Sample**\n   - Write 3 sample pieces in chosen voice\n   - Test consistency using analyzer\n   - Refine based on results\n\n### Creating SEO-Optimized Blog Posts\n\n1. **Keyword Research**\n   - Identify primary keyword (search volume 500-5000/month)\n   - Find 3-5 secondary keywords\n   - List 10-15 LSI keywords\n\n2. **Content Structure**\n   - Use blog template from `references/content_frameworks.md`\n   - Include keyword in title, first paragraph, and 2-3 H2s\n   - Aim for 1,500-2,500 words for comprehensive coverage\n\n3. **Optimization Check**\n   ```bash\n   python scripts/seo_optimizer.py blog_post.md \"primary keyword\" \"secondary,keywords,list\"\n   ```\n\n4. **Apply SEO Recommendations**\n   - Adjust keyword density to 1-3%\n   - Ensure proper heading structure\n   - Add internal and external links\n   - Optimize meta description\n\n### Social Media Content Creation\n\n1. **Platform Selection**\n   - Identify primary platforms based on audience\n   - Review platform-specific guidelines in `references/social_media_optimization.md`\n\n2. **Content Adaptation**\n   - Start with blog post or core message\n   - Use repurposing matrix from `references/content_frameworks.md`\n   - Adapt for each platform following templates\n\n3. **Optimization Checklist**\n   - Platform-appropriate length\n   - Optimal posting time\n   - Correct image dimensions\n   - Platform-specific hashtags\n   - Engagement elements (polls, questions)\n\n### Content Calendar Planning\n\n1. **Monthly Planning**\n   - Copy `assets/content_calendar_template.md`\n   - Set monthly goals and KPIs\n   - Identify key campaigns/themes\n\n2. **Weekly Distribution**\n   - Follow 40/25/25/10 content pillar ratio\n   - Balance platforms throughout week\n   - Align with optimal posting times\n\n3. **Batch Creation**\n   - Create all weekly content in one session\n   - Maintain consistent voice across pieces\n   - Prepare all visual assets together\n\n## Key Scripts\n\n### brand_voice_analyzer.py\nAnalyzes text content for voice characteristics, readability, and consistency.\n\n**Usage**: `python scripts/brand_voice_analyzer.py <file> [json|text]`\n\n**Returns**:\n- Voice profile (formality, tone, perspective)\n- Readability score\n- Sentence structure analysis\n- Improvement recommendations\n\n### seo_optimizer.py\nAnalyzes content for SEO optimization and provides actionable recommendations.\n\n**Usage**: `python scripts/seo_optimizer.py <file> [primary_keyword] [secondary_keywords]`\n\n**Returns**:\n- SEO score (0-100)\n- Keyword density analysis\n- Structure assessment\n- Meta tag suggestions\n- Specific optimization recommendations\n\n## Reference Guides\n\n### When to Use Each Reference\n\n**references/brand_guidelines.md**\n- Setting up new brand voice\n- Ensuring consistency across content\n- Training new team members\n- Resolving voice/tone questions\n\n**references/content_frameworks.md**\n- Starting any new content piece\n- Structuring different content types\n- Creating content templates\n- Planning content repurposing\n\n**references/social_media_optimization.md**\n- Platform-specific optimization\n- Hashtag strategy development\n- Understanding algorithm factors\n- Setting up analytics tracking\n\n## Best Practices\n\n### Content Creation Process\n1. Always start with audience need/pain point\n2. Research before writing\n3. Create outline using templates\n4. Write first draft without editing\n5. Optimize for SEO\n6. Edit for brand voice\n7. Proofread and fact-check\n8. Optimize for platform\n9. Schedule strategically\n\n### Quality Indicators\n- SEO score above 75/100\n- Readability appropriate for audience\n- Consistent brand voice throughout\n- Clear value proposition\n- Actionable takeaways\n- Proper visual formatting\n- Platform-optimized\n\n### Common Pitfalls to Avoid\n- Writing before researching keywords\n- Ignoring platform-specific requirements\n- Inconsistent brand voice\n- Over-optimizing for SEO (keyword stuffing)\n- Missing clear CTAs\n- Publishing without proofreading\n- Ignoring analytics feedback\n\n## Performance Metrics\n\nTrack these KPIs for content success:\n\n### Content Metrics\n- Organic traffic growth\n- Average time on page\n- Bounce rate\n- Social shares\n- Backlinks earned\n\n### Engagement Metrics\n- Comments and discussions\n- Email click-through rates\n- Social media engagement rate\n- Content downloads\n- Form submissions\n\n### Business Metrics\n- Leads generated\n- Conversion rate\n- Customer acquisition cost\n- Revenue attribution\n- ROI per content piece\n\n## Integration Points\n\nThis skill works best with:\n- Analytics platforms (Google Analytics, social media insights)\n- SEO tools (for keyword research)\n- Design tools (for visual content)\n- Scheduling platforms (for content distribution)\n- Email marketing systems (for newsletter content)\n\n## Quick Commands\n\n```bash\n# Analyze brand voice\npython scripts/brand_voice_analyzer.py content.txt\n\n# Optimize for SEO\npython scripts/seo_optimizer.py article.md \"main keyword\"\n\n# Check content against brand guidelines\ngrep -f references/brand_guidelines.md content.txt\n\n# Create monthly calendar\ncp assets/content_calendar_template.md this_month_calendar.md\n```\n"
    }
  },
  "alirezarezvani-claude-skills-marketing-demand-acquisition": {
    "id": "alirezarezvani-claude-skills-marketing-demand-acquisition",
    "name": "marketing-demand-acquisition",
    "description": "Multi-channel demand generation, paid media optimization, SEO strategy, and partnership programs for Series A+ startups. Includes CAC calculator, channel playbooks, HubSpot integration, and international expansion tactics. Use when planning demand generation campaigns, optimizing paid media, building SEO strategies, establishing partnerships, or when user mentions demand gen, paid ads, LinkedIn ads, Google ads, CAC, acquisition, lead generation, or pipeline generation.",
    "repo": {
      "owner": "alirezarezvani",
      "name": "claude-skills",
      "fullName": "alirezarezvani/claude-skills",
      "url": "https://github.com/alirezarezvani/claude-skills/tree/main/marketing-skill/marketing-demand-acquisition",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 521,
      "forks": 103,
      "language": "Python",
      "topics": [
        "agentic-ai",
        "agentic-coding",
        "anthropic-claude",
        "claude-ai",
        "claude-ai-skills",
        "claude-code",
        "claude-code-skills",
        "claude-skills",
        "claudecode-subagents"
      ],
      "updatedAt": "2026-01-08T12:36:14Z",
      "pushedAt": "2026-01-08T08:39:21Z",
      "createdAt": "2025-10-19T04:04:05Z",
      "license": "MIT License"
    },
    "category": "marketing",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: marketing-demand-acquisition\ndescription: Multi-channel demand generation, paid media optimization, SEO strategy, and partnership programs for Series A+ startups. Includes CAC calculator, channel playbooks, HubSpot integration, and international expansion tactics. Use when planning demand generation campaigns, optimizing paid media, building SEO strategies, establishing partnerships, or when user mentions demand gen, paid ads, LinkedIn ads, Google ads, CAC, acquisition, lead generation, or pipeline generation.\nlicense: MIT\nmetadata:\n  version: 1.0.0\n  author: Alireza Rezvani\n  category: marketing\n  domain: demand-generation\n  updated: 2025-10-20\n  python-tools: calculate_cac.py\n  tech-stack: HubSpot, LinkedIn-Ads, Google-Ads, Meta-Ads, SEO-tools\n  target-market: B2B-SaaS, Series-A+\n---\n\n# Marketing Demand & Acquisition\n\nExpert acquisition playbook for Series A+ startups scaling internationally (EU/US/Canada) with hybrid PLG/Sales-Led motion.\n\n## Keywords\ndemand generation, paid media, paid ads, LinkedIn ads, Google ads, Meta ads, CAC, customer acquisition cost, lead generation, MQL, SQL, pipeline generation, acquisition strategy, performance marketing, paid social, paid search, partnerships, affiliate marketing, SEO strategy, HubSpot campaigns, marketing automation, B2B marketing, SaaS marketing\n\n## Role Coverage\n\nThis skill serves:\n- **Demand Generation Manager** - Multi-channel campaigns, pipeline generation\n- **Paid Media/Performance Marketer** - Paid search/social/display optimization\n- **SEO Manager** - Organic acquisition and technical SEO\n- **Affiliate/Partnerships Manager** - Co-marketing and channel partnerships\n\n## Core KPIs by Role\n\n**Demand Gen**: MQL/SQL volume, cost per opportunity, marketing-sourced pipeline $, pipeline velocity, MQLâ†’SQL conversion rate\n\n**Paid Media**: CAC, ROAS, CPL, CPA, incrementality lift, channel efficiency ratio\n\n**SEO**: Organic sessions, non-brand traffic %, keyword rankings (P1-P3), organic-assisted conversions, technical health score\n\n**Partnerships**: Partner-sourced pipeline $, partner CAC, net new logos via partners, co-marketing ROI\n\n## Tech Stack Integration\n\n**HubSpot CRM** - Campaign tracking, lead scoring, attribution, workflows\n**Google Analytics** - Traffic analysis, conversion tracking, funnel optimization\n**Search Console** - Keyword performance, technical issues, indexing\n**LinkedIn Campaign Manager** - B2B paid social\n**Google Ads** - Search, Display, YouTube\n**Meta Ads** - Facebook, Instagram\n\n---\n\n## 1. Demand Generation Framework\n\n### 1.1 Full-Funnel Strategy (2025 Best Practice)\n\n**TOFU (Awareness)** â†’ **MOFU (Consideration)** â†’ **BOFU (Decision)** â†’ **Handoff to Sales/Product**\n\n#### TOFU Tactics\n- Paid social (LinkedIn thought leadership, Meta awareness)\n- Display advertising (programmatic, retargeting)\n- Content syndication\n- SEO (informational keywords)\n- Partnerships (co-webinars, guest content)\n- Target: Brand lift, site traffic, early-stage engagement\n\n#### MOFU Tactics\n- Paid search (solution keywords)\n- Retargeting campaigns\n- Gated content (eBooks, templates, webinars)\n- Email nurture sequences\n- Comparison pages (SEO)\n- Target: MQLs, demo requests, trial signups\n\n#### BOFU Tactics\n- Paid search (brand + competitor keywords)\n- Direct outreach campaigns\n- Free trial CTAs\n- Case studies & ROI calculators\n- Intent-based retargeting\n- Target: SQLs, demos booked, pipeline $\n\n### 1.2 Campaign Planning Template\n\n**Campaign Brief** (use this for every campaign):\n\n```\nCampaign Name: [Q2-2025-LinkedIn-ABM-Enterprise]\nObjective: [Generate 50 SQLs from Enterprise accounts ($50k+ ACV)]\nBudget: [$15k/month]\nDuration: [90 days]\nChannels: [LinkedIn Ads, Retargeting, Email]\nAudience: [Director+ at SaaS companies, 500-5000 employees, EU/US]\nOffer: [Gated Industry Benchmark Report]\nSuccess Metrics:\n  - Primary: 50 SQLs, <$300 CPO\n  - Secondary: 500 MQLs, 10% MQLâ†’SQL rate, 40% email open rate\nHubSpot Setup:\n  - Campaign ID: [create in HubSpot]\n  - Lead scoring: +20 for download, +30 for demo request\n  - Attribution: First-touch + Multi-touch\nHandoff Protocol:\n  - SQL criteria: Title + Company size + Budget confirmed\n  - Routing: Enterprise SDR team via HubSpot workflow\n  - SLA: 4-hour response time\n```\n\n### 1.3 HubSpot Campaign Tracking Setup\n\n**Step-by-step**:\n\n1. **Create Campaign in HubSpot**\n   - Marketing â†’ Campaigns â†’ Create Campaign\n   - Name: `Q2-2025-LinkedIn-ABM-Enterprise`\n   - Tag all assets (landing pages, emails, ads) with campaign ID\n\n2. **UTM Parameter Structure** (critical for attribution)\n   ```\n   utm_source={channel}       // linkedin, google, facebook\n   utm_medium={type}          // cpc, display, email, organic\n   utm_campaign={campaign-id} // q2-2025-linkedin-abm-enterprise\n   utm_content={variant}      // ad-variant-a, email-1\n   utm_term={keyword}         // [for paid search only]\n   ```\n\n3. **Lead Scoring Configuration**\n   - Navigate to: Settings â†’ Marketing â†’ Lead Scoring\n   - Campaign engagement: +10-30 points based on action depth\n   - Channel quality: LinkedIn +5, Google Search +10, Organic +15\n\n4. **Attribution Reports**\n   - Use HubSpot's multi-touch attribution (W-shaped for hybrid motion)\n   - First-touch: Awareness credit\n   - Multi-touch: Full journey credit\n   - Build custom report: Marketing â†’ Reports â†’ Attribution\n\n### 1.4 International Expansion Considerations\n\n**EU Market Entry**:\n- GDPR compliance: Double opt-in for email, explicit consent tracking in HubSpot\n- Localization: Translate landing pages, ads, emails (DE, FR, ES priority)\n- Payment: Display prices in EUR\n- Partnerships: Local co-marketing partners for credibility\n- Paid channels: LinkedIn most effective for B2B EU, Google Ads second\n\n**US/Canada Market Entry**:\n- Messaging: Direct, ROI-focused, less formal than EU\n- Paid channels: Google Ads + LinkedIn equal priority\n- Partnerships: Industry associations, review sites (G2, Capterra)\n- Content: Case studies with $ impact, not just features\n- Sales alignment: Faster sales cycles, need immediate lead follow-up\n\n**Budget Allocation** (Series A recommended):\n- EU: 40% LinkedIn, 25% Google, 20% SEO, 15% Partnerships\n- US/CA: 35% Google, 30% LinkedIn, 20% SEO, 15% Partnerships\n\n---\n\n## 2. Paid Media Optimization\n\n### 2.1 Channel Strategy Matrix\n\n| Channel | Best For | CAC Benchmark | Conversion Rate | Series A Priority |\n|---------|----------|---------------|-----------------|-------------------|\n| **LinkedIn Ads** | B2B, Enterprise, ABM | $150-$400 | 0.5-2% | â­â­â­â­â­ |\n| **Google Search** | High-intent, BOFU | $80-$250 | 2-5% | â­â­â­â­â­ |\n| **Google Display** | Retargeting, awareness | $50-$150 | 0.3-1% | â­â­â­ |\n| **Meta (FB/IG)** | SMB, consumer-like products | $60-$200 | 1-3% | â­â­â­ |\n| **YouTube** | Product demos, brand | $100-$300 | 0.5-1.5% | â­â­ |\n| **Reddit/Twitter** | Technical audiences | $40-$180 | 0.5-2% | â­â­ |\n\n### 2.2 LinkedIn Ads Playbook (Primary B2B Channel)\n\n**Campaign Structure**:\n```\nAccount\nâ””â”€ Campaign Group: [Q2-2025-Enterprise-ABM]\n   â”œâ”€ Campaign 1: [Awareness - Thought Leadership]\n   â”‚  â”œâ”€ Ad Set: [CTO/VP Eng, US, Tech Companies]\n   â”‚  â””â”€ Creatives: [3 carousel posts, 2 video ads]\n   â”œâ”€ Campaign 2: [Consideration - Product Education]\n   â”‚  â”œâ”€ Ad Set: [Engaged audience, retargeting]\n   â”‚  â””â”€ Creatives: [2 lead gen forms, 1 landing page]\n   â””â”€ Campaign 3: [Conversion - Demo Requests]\n      â”œâ”€ Ad Set: [Website visitors, content downloaders]\n      â””â”€ Creatives: [Direct demo CTA, case study]\n```\n\n**Targeting Best Practices**:\n- **Company Size**: 50-5000 employees (Series A sweet spot)\n- **Job Titles**: Director+, VP+, C-level (use LinkedIn's precise targeting)\n- **Industries**: Software, SaaS, Tech Services\n- **Matched Audiences**: Website retargeting (install Insight Tag), uploaded email lists\n- **Budget**: Start $50/day per campaign, scale 20% weekly if CAC < target\n\n**Creative Frameworks**:\n1. **Thought Leadership** - Industry insights, no product pitch\n2. **Social Proof** - Customer logos, testimonials, case study snippets\n3. **Problem-Solution** - Pain point + your solution in 3 seconds\n4. **Demo-First** - Show product immediately, skip fluff\n\n**LinkedIn Lead Gen Forms vs. Landing Pages**:\n- **Lead Gen Forms**: Higher conversion (2-3x), lower quality, use for TOFU/MOFU\n- **Landing Pages**: Lower conversion, higher quality, use for BOFU/demo requests\n- **HubSpot Sync**: Connect LinkedIn Lead Gen Forms via native integration\n\n### 2.3 Google Ads Playbook (High-Intent Capture)\n\n**Campaign Types Priority**:\n1. **Search - Brand** (highest priority, protect brand terms)\n2. **Search - Competitor** (steal market share)\n3. **Search - Solution** (problem-aware buyers)\n4. **Search - Product Category** (earlier stage)\n5. **Display - Retargeting** (re-engage warm traffic)\n\n**Search Campaign Structure**:\n```\nCampaign: [Search-Solution-Keywords]\nâ”œâ”€ Ad Group: [project management software]\nâ”‚  â”œâ”€ Keywords:\nâ”‚  â”‚  - \"project management software\" [Phrase]\nâ”‚  â”‚  - \"best project management tool\" [Phrase]\nâ”‚  â”‚  - +project +management +solution [Broad Match Modifier]\nâ”‚  â””â”€ Ads: [3 responsive search ads with 15 headlines, 4 descriptions]\nâ”‚\nâ”œâ”€ Ad Group: [team collaboration tools]\n   â”œâ”€ Keywords: [5-10 tightly themed keywords]\n   â””â”€ Ads: [3 responsive search ads]\n```\n\n**Keyword Strategy**:\n- **Brand Terms**: Exact match, bid high, protect brand\n- **Competitor Terms**: \"[Competitor] alternative\", \"[Competitor] vs [You]\"\n- **Solution Terms**: \"best [category] software\", \"top [category] tools\"\n- **Problem Terms**: \"how to [solve problem]\"\n- **Negative Keywords**: Maintain list of 100+ (free, cheap, jobs, career, reviews)\n\n**Bid Strategy** (2025 best practice):\n- New campaigns: Start Manual CPC for control\n- After 50+ conversions: Switch to Target CPA\n- After 100+ conversions: Test Maximize Conversions with tCPA\n- EU markets: Bid 15-20% higher for same quality\n\n**Ad Copy Framework** (Responsive Search Ads):\n```\nHeadlines (15 required):\n- H1-3: Value props (Save 10 hours/week, Trusted by 500+ teams)\n- H4-6: Features (AI-powered, Real-time sync, Mobile app)\n- H7-9: Social proof (4.8â˜… G2 rating, Used by Microsoft)\n- H10-12: CTAs (Start free trial, Book demo, See pricing)\n- H13-15: Keywords pinned (Dynamic insertion)\n\nDescriptions (4 required):\n- D1: Primary value prop + CTA (30-60 chars)\n- D2: Feature list + differentiator (60-90 chars)\n- D3: Social proof + urgency (45-90 chars)\n- D4: Backup generic (60-90 chars)\n```\n\n### 2.4 Meta Ads Playbook (SMB/Lower ACV)\n\n**When to Use Meta**:\n- âœ… Product ACV <$10k\n- âœ… Visual product (UI, consumer-facing)\n- âœ… SMB/prosumer audience\n- âœ… Broader awareness campaigns\n- âŒ Enterprise/high ACV (use LinkedIn)\n\n**Campaign Setup**:\n```\nCampaign Objective: [Conversions]\nâ”œâ”€ Ad Set 1: [Lookalike - 1% of converters]\nâ”‚  â””â”€ Placement: [Feed + Stories, Auto]\nâ”œâ”€ Ad Set 2: [Interest - Business Software]\nâ”‚  â””â”€ Placement: [Feed only]\nâ””â”€ Ad Set 3: [Retargeting - Website 30d]\n   â””â”€ Placement: [All placements]\n```\n\n**Audience Strategy**:\n1. **Core Audiences**: Interests (business tools, productivity, startups)\n2. **Lookalike**: 1% of purchasers/high-value leads\n3. **Retargeting**: 30-day website visitors, video viewers (75%+)\n\n**Creative Best Practices**:\n- Use video (1:1 or 9:16 for Stories)\n- First 3 seconds = hook (problem or result)\n- Show product UI in action\n- Add captions (85% watch muted)\n- Test 3-5 creative variants per campaign\n\n### 2.5 Budget Allocation & Scaling\n\n**Initial Budget** (Series A, $30k-50k/month total):\n```\nChannel            Budget    Expected Results\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nLinkedIn Ads       $15k      50 MQLs, 10 SQLs, $1.5k CAC\nGoogle Search      $12k      80 MQLs, 20 SQLs, $600 CAC\nGoogle Display     $5k       120 MQLs, 5 SQLs, $1k CAC\nMeta Ads           $5k       100 MQLs, 8 SQLs, $625 CAC\nPartnerships       $3k       20 MQLs, 5 SQLs, $600 CAC\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nTOTAL              $40k      370 MQLs, 48 SQLs, $833 avg CAC\n```\n\n**Scaling Rules**:\n1. If CAC <target â†’ Increase budget 20% weekly\n2. If CAC >target â†’ Pause, optimize, relaunch\n3. If conversion rate drops >20% â†’ Check landing page, offer fatigue\n4. Scale winners, kill losers fast (2-week test minimum)\n\n**HubSpot ROI Dashboard**:\n- Marketing â†’ Reports â†’ Create Custom Report\n- Metrics: Spend, Leads, MQLs, SQLs, CAC, ROAS, Pipeline $\n- Dimensions: Campaign, Channel, Region\n- Frequency: Daily review, weekly optimization\n\n---\n\n## 3. SEO Strategy\n\n### 3.1 Technical SEO Foundation (Must-Have)\n\n**Pre-Launch Checklist**:\n- [ ] XML sitemap submitted to Search Console\n- [ ] Robots.txt configured (allow crawling)\n- [ ] HTTPS enabled (SSL certificate)\n- [ ] Page speed >90 mobile (Google PageSpeed Insights)\n- [ ] Core Web Vitals passing (LCP, FID, CLS)\n- [ ] Structured data (Organization, Product, FAQ schema)\n- [ ] Canonical tags on all pages\n- [ ] Hreflang tags for international (en-US, en-GB, de-DE, etc.)\n\n**Technical Audit** (quarterly):\n```\n1. Crawl site with Screaming Frog\n2. Check for:\n   - 404 errors (fix or redirect)\n   - Redirect chains (consolidate)\n   - Duplicate content (canonicalize)\n   - Missing meta descriptions\n   - Slow pages (>3s load time)\n   - Mobile usability issues\n3. Fix issues in priority order: Critical â†’ High â†’ Medium\n```\n\n### 3.2 Keyword Strategy Framework\n\n**Keyword Research Process**:\n1. **Seed Keywords** - Your product category (e.g., \"project management software\")\n2. **Use Tools** - Ahrefs, SEMrush, or free: Google Keyword Planner + Search Console\n3. **Analyze** - Volume, difficulty, intent, SERP features\n4. **Prioritize** - Quick wins (low difficulty, high intent)\n\n**Keyword Tiers**:\n\n**Tier 1: High-Intent BOFU** (target first)\n- \"best [product category]\"\n- \"[product category] for [use case]\"\n- \"[competitor] alternative\"\n- Volume: 100-1k/mo, Difficulty: Medium, Intent: Commercial\n\n**Tier 2: Solution-Aware MOFU**\n- \"how to [solve problem]\"\n- \"[problem] solution\"\n- \"[use case] tools\"\n- Volume: 500-5k/mo, Difficulty: Medium-High, Intent: Informational-Commercial\n\n**Tier 3: Problem-Aware TOFU**\n- \"what is [concept]\"\n- \"[problem] examples\"\n- \"[industry] challenges\"\n- Volume: 1k-10k/mo, Difficulty: High, Intent: Informational\n\n**International Keyword Research**:\n- Use Ahrefs/SEMrush with language filters\n- Translate keywords, don't just localize (cultural nuances matter)\n- EU: Higher trust in localized content (domain.de > domain.com/de)\n- UK: Use British spelling (optimise vs. optimize)\n\n### 3.3 On-Page SEO Template\n\n**Page Optimization Checklist**:\n```\nURL: [/best-project-management-software]\nTitle Tag (60 chars): [Best Project Management Software 2025 | YourBrand]\nMeta Description (155 chars): [Compare top 10 PM tools. Features, pricing, reviews. Find the perfect fit for your team. Free trials available.]\n\nH1 (60 chars): [Best Project Management Software in 2025]\nH2s (structure):\n  - What is Project Management Software?\n  - Top 10 PM Tools Compared\n  - Key Features to Look For\n  - Pricing & Plans\n  - How to Choose\n  - FAQ\n\nContent:\n  - Length: 2000-3000 words (comprehensive)\n  - Keyword density: 1-2% (natural)\n  - Internal links: 3-5 relevant pages\n  - External links: 2-3 authoritative sources\n  - Images: 3-5 with alt text\n  - Schema: Product, FAQ, HowTo\n\nCTA:\n  - Above fold: [Start Free Trial]\n  - Mid-content: [Compare Plans]\n  - End: [Book Demo]\n```\n\n**Content Refresh Schedule**:\n- Tier 1 pages: Update quarterly (rankings, pricing, features)\n- Tier 2 pages: Update semi-annually\n- Tier 3 pages: Update annually\n- All pages: Monitor Search Console for ranking drops, refresh immediately\n\n### 3.4 Link Building Strategy (2025 Best Practices)\n\n**Link Acquisition Tactics** (in priority order):\n\n**1. Digital PR** (highest ROI)\n- Publish original research/data\n- Create industry reports\n- Pitch journalists (use HARO, Terkel, Featured)\n- Target: Industry blogs, tech publications\n\n**2. Guest Posting** (quality over quantity)\n- Target: Domain Authority (DA) 40+ sites\n- Avoid: Link farms, PBNs, paid links (Google penalty risk)\n- Anchor text: Branded (70%), topical (20%), exact match (10%)\n\n**3. Partnerships & Co-Marketing**\n- Partner with complementary SaaS tools\n- Create co-branded content\n- Exchange homepage links (footer or partner section)\n\n**4. Community Engagement**\n- Answer questions on Reddit, Quora\n- Participate in industry forums\n- Create tools/calculators â†’ natural backlinks\n\n**5. Broken Link Building**\n- Find broken links on competitor sites\n- Offer your content as replacement\n- Tools: Ahrefs' Broken Backlinks report\n\n**Link Velocity** (avoid penalties):\n- Natural: 5-10 links/month for new sites\n- Aggressive: 20-30 links/month after 6 months\n- Monitor: Google Search Console for manual actions\n\n### 3.5 Content Strategy for SEO\n\n**Content Types by Funnel Stage**:\n\n**TOFU (Awareness)**:\n- Blog posts: \"Ultimate Guide to [Topic]\"\n- Listicles: \"Top 10 [Category]\"\n- Industry reports: \"[Industry] State of 2025\"\n- Target: Broad keywords, thought leadership\n\n**MOFU (Consideration)**:\n- Comparison pages: \"[Your Product] vs [Competitor]\"\n- Best of lists: \"Best [Category] for [Use Case]\"\n- How-to guides: \"How to [Solve Problem] with [Product]\"\n- Target: Solution keywords, product education\n\n**BOFU (Decision)**:\n- Product pages: \"[Product] Features & Pricing\"\n- Case studies: \"How [Customer] Achieved [Result]\"\n- Landing pages: \"Start Free Trial\"\n- Target: Brand keywords, high-intent searches\n\n**Content Calendar** (Series A minimum):\n- TOFU: 4 posts/month (1 per week)\n- MOFU: 2 posts/month\n- BOFU: 1 post/month\n- Refresh: 2 existing posts/month\n\n### 3.6 Local SEO (For Regional Offices)\n\n**Google Business Profile Setup** (per location):\n- Complete all fields: Name, address, phone, hours, category\n- Upload photos: Office, team, product (10+ images)\n- Collect reviews: Ask customers, automate via HubSpot workflow\n- Post updates: Weekly posts about company news, events\n\n**Local Citations** (US/Canada/EU):\n- Submit to: Yelp, Yellow Pages, local directories\n- NAP consistency: Name, Address, Phone identical everywhere\n- Industry directories: Software review sites (G2, Capterra)\n\n---\n\n## 4. Partnerships & Affiliate Programs\n\n### 4.1 Partnership Types & Strategy\n\n**Partnership Tiers**:\n\n**Tier 1: Strategic Partnerships** (high impact, low volume)\n- Target: Complementary SaaS tools with overlapping ICPs\n- Structure: Co-marketing, product integrations, revenue share\n- Examples: Slack â†” Asana, Shopify â†” Klaviyo\n- Effort: High (6-12 months to establish)\n- ROI: Very high (100+ leads/month after ramp)\n\n**Tier 2: Affiliate Partners** (scalable)\n- Target: Bloggers, review sites, industry influencers\n- Structure: Commission per sale (10-30% first year)\n- Platform: Use PartnerStack, Impact, or Rewardful\n- Effort: Medium (setup once, ongoing management)\n- ROI: Medium-High (depends on partner quality)\n\n**Tier 3: Referral Partners** (customer-driven)\n- Target: Your existing customers\n- Structure: Referral bonus ($500-$1k per SQL)\n- Platform: Built into HubSpot or standalone (Friendbuy)\n- Effort: Low (automate via workflows)\n- ROI: Medium (5-10% of customers refer)\n\n**Tier 4: Marketplace Listings** (distribution)\n- Target: Shopify App Store, Salesforce AppExchange, HubSpot Marketplace\n- Structure: Free listing + revenue share\n- Effort: Medium (initial listing, ongoing updates)\n- ROI: Low-Medium (brand visibility + discovery)\n\n### 4.2 Partnership Playbook\n\n**Step 1: Identify Partners**\n```\nCriteria:\n- Similar ICP (overlapping audience, no direct competition)\n- Product fit (complementary, not substitute)\n- Scale (similar company size, funding stage)\n- Values alignment (culture, brand positioning)\n\nResearch:\n- Tools: BuiltWith, SimilarWeb, LinkedIn Sales Nav\n- Look for: Integration pages, partner pages, co-marketing history\n```\n\n**Step 2: Outreach Template**\n```\nSubject: [YourBrand] â†” [TheirBrand] Partnership Idea\n\nHi [Name],\n\nI'm [Your Name] at [YourBrand] - we help [ICP] with [value prop].\n\nI noticed [TheirBrand] serves a similar audience, and I think our customers would benefit from an integration between [YourProduct] and [TheirProduct].\n\nWould you be open to exploring a partnership? I'm thinking:\n- Product integration (bi-directional sync)\n- Co-marketing (joint webinar, case study)\n- Revenue share (referral fees)\n\nLet me know if you'd like to chat. Happy to send more details.\n\nBest,\n[Your Name]\n```\n\n**Step 3: Partnership Agreement**\n- Define scope (integration depth, marketing commitment)\n- Revenue model (rev share %, referral fees, co-selling)\n- Success metrics (leads, pipeline, revenue)\n- Term (12-24 months, with renewal)\n- Exit clause (90-day notice)\n\n**Step 4: Activation & Enablement**\n- Create co-branded assets (landing page, webinar deck, one-pager)\n- Train partner sales team (product demo, pitch deck, objection handling)\n- Set up tracking (UTM parameters, partner portal in HubSpot)\n\n**Step 5: Ongoing Management**\n- Quarterly business reviews (QBRs)\n- Monthly check-ins (pipeline, blockers)\n- Co-marketing calendar (1-2 activities/quarter)\n- Reporting (HubSpot dashboard for partner-sourced pipeline)\n\n### 4.3 Affiliate Program Setup\n\n**Platform Selection**:\n- **PartnerStack** - Best for B2B SaaS, native integrations\n- **Impact** - Enterprise-grade, high control\n- **Rewardful** - Lightweight, Stripe integration\n- **FirstPromoter** - Budget-friendly, good analytics\n\n**Commission Structure** (Series A typical):\n```\nTier 1: Influencers/Publishers\n- 30% recurring for 12 months\n- Or: $500 flat per SQL\n- Bonus: $1k for 10+ referrals/quarter\n\nTier 2: Bloggers/Content Creators\n- 20% recurring for 12 months\n- Or: $300 flat per SQL\n\nTier 3: Customers (Referral Program)\n- $500 per closed deal\n- Or: 1 month free for both referrer + referee\n```\n\n**Recruitment Strategy**:\n1. **Outbound**: Find industry bloggers, YouTubers, newsletter writers\n2. **Inbound**: \"Become an Affiliate\" page, promote in product\n3. **Events**: Recruit at conferences, meetups\n4. **Communities**: Reddit, LinkedIn groups, Slack communities\n\n**Affiliate Enablement Kit**:\n- Brand assets (logos, product screenshots)\n- Pre-written content (blog post templates, social posts)\n- Tracking links (unique UTM codes per affiliate)\n- Sales collateral (one-pagers, case studies, demo videos)\n\n### 4.4 Co-Marketing Campaigns\n\n**Joint Webinar Playbook**:\n```\nPlanning (6 weeks out):\n- Define topic (audience pain point, not product pitch)\n- Assign roles (host, co-host, Q&A moderator)\n- Create landing page (co-branded, dual logos)\n- Design promo assets (social graphics, email templates)\n\nPromotion (4 weeks out):\n- Email: 3 sends (announcement, reminder, last chance)\n- Social: 8-10 posts per partner (LinkedIn, Twitter)\n- Paid: $2k budget for LinkedIn ads â†’ landing page\n- Partners: Cross-promote to each other's audiences\n\nExecution (day of):\n- 60-min format: 5min intro, 40min content, 15min Q&A\n- Record for on-demand\n- Polls/CTAs: Mid-webinar poll, end with demo CTA\n\nFollow-up (1 week after):\n- Send recording to all registrants\n- Nurture sequence: 3 emails over 2 weeks\n- Split leads: Each partner owns their referred leads\n- Report: Attendees, pipeline generated, next steps\n```\n\n**Other Co-Marketing Tactics**:\n- **Co-branded Content**: eBook, report, guide\n- **Case Study**: Joint customer success story\n- **Bundle Offer**: \"Buy [YourProduct] + [TheirProduct], save 20%\"\n- **Cross-promotion**: Feature each other in newsletters\n- **Social Media Takeover**: Guest post on each other's channels\n\n### 4.5 HubSpot Partner Tracking\n\n**Setup**:\n1. **Create Partner Property**\n   - Settings â†’ Properties â†’ Create \"Partner Source\" dropdown\n   - Values: Partner A, Partner B, Affiliate Network, etc.\n\n2. **UTM Tracking**\n   - Partner links: `?utm_source=partner-name&utm_medium=referral`\n   - HubSpot auto-captures UTM parameters\n\n3. **Lead Assignment**\n   - Workflow: If \"Partner Source\" is set â†’ Assign to Partner Manager\n   - Notification: Slack alert when partner lead arrives\n\n4. **Reporting**\n   - Dashboard: Partner-sourced leads, pipeline, revenue\n   - Report to partners: Monthly performance summary\n\n---\n\n## 5. Attribution & Reporting\n\n### 5.1 Attribution Models (HubSpot Native)\n\n**Model Selection** (use multi-touch for hybrid motion):\n\n**First-Touch** - Credit to first interaction\n- Use case: Awareness campaigns, brand building\n- Pro: Shows what drives discovery\n- Con: Ignores nurturing influence\n\n**Last-Touch** - Credit to last interaction before conversion\n- Use case: Direct response, BOFU campaigns\n- Pro: Shows what closes deals\n- Con: Ignores earlier touchpoints\n\n**Multi-Touch (W-Shaped)** - Credit to first, last, and middle (40-20-40 split)\n- Use case: Hybrid PLG/Sales-Led (recommended for Series A)\n- Pro: Full-funnel view\n- Con: More complex to explain to stakeholders\n\n**HubSpot Setup**:\n- Marketing â†’ Reports â†’ Attribution â†’ Select Model\n- Default: Use Multi-Touch for holistic view\n- Compare: Run reports side-by-side to see differences\n\n### 5.2 Reporting Dashboard (HubSpot)\n\n**Weekly Performance Dashboard**:\n```\nMetrics to Track:\n1. Traffic: Visits, unique visitors, bounce rate\n2. Leads: MQLs, SQLs, conversion rates\n3. Pipeline: Opportunities created, value, velocity\n4. CAC: Spend Ã· customers acquired\n5. Channel Mix: % of leads by source\n\nDimensions:\n- By Channel: Organic, Paid, Email, Social, Referral\n- By Campaign: Individual campaign performance\n- By Region: US, CA, EU breakdown\n- By Stage: TOFU, MOFU, BOFU metrics\n```\n\n**Monthly Executive Dashboard**:\n```\nKPIs:\n1. Marketing-Sourced Pipeline: $[X]M (target: $[Y]M)\n2. Marketing-Sourced Revenue: $[X]k (target: $[Y]k)\n3. Blended CAC: $[X] (target: $[Y])\n4. MQLâ†’SQL Rate: [X]% (target: [Y]%)\n5. Pipeline Velocity: [X] days (target: [Y] days)\n6. ROMI: [X]:1 (target: 3:1+)\n\nInsights:\n- Top performing campaigns\n- Underperforming channels (kill or optimize)\n- New experiments to test next month\n- Budget reallocation recommendations\n```\n\n### 5.3 Google Analytics Setup\n\n**Events to Track** (GA4):\n```\nEngagement:\n- page_view (auto-tracked)\n- scroll (75% depth)\n- video_play (product demos)\n- file_download (whitepapers, eBooks)\n\nConversions:\n- sign_up (free trial, account created)\n- demo_request (calendar booking)\n- contact_form (inbound interest)\n- pricing_view (pricing page visit)\n\nE-commerce (if applicable):\n- add_to_cart\n- begin_checkout\n- purchase\n```\n\n**Custom Dimensions**:\n- User Type: Free vs. Paid\n- Plan Type: Starter, Pro, Enterprise\n- HubSpot Lead Status: MQL, SQL, Customer\n- Campaign: HubSpot Campaign ID\n\n**Integration with HubSpot**:\n- Use HubSpot tracking code (includes GA4 by default)\n- Or: Google Tag Manager for advanced tracking\n- Sync: GA4 audiences â†’ HubSpot lists for retargeting\n\n---\n\n## 6. Experimentation Framework\n\n### 6.1 A/B Testing Prioritization (ICE Score)\n\n**Formula**: ICE = (Impact Ã— Confidence Ã— Ease) Ã· 3\n\nRate each factor 1-10:\n- **Impact**: How much will this move the needle?\n- **Confidence**: How sure are you it will work?\n- **Ease**: How easy is it to implement?\n\n**Example Tests** (sorted by ICE score):\n\n| Test | Impact | Confidence | Ease | ICE | Priority |\n|------|--------|------------|------|-----|----------|\n| CTA button color (red vs. green) | 3 | 8 | 10 | 7.0 | Low |\n| Landing page headline rewrite | 8 | 7 | 8 | 7.7 | Medium |\n| Pricing page redesign | 9 | 6 | 4 | 6.3 | Medium |\n| New lead magnet offer | 9 | 8 | 7 | 8.0 | High |\n| Add live chat to pricing page | 7 | 9 | 8 | 8.0 | High |\n\n### 6.2 Test Design & Execution\n\n**Test Template**:\n```\nHypothesis: [Adding a case study carousel to the pricing page will increase demo requests by 20% because users need social proof before committing]\n\nMetric: [Demo requests from /pricing page]\nSample Size: [1000 visitors per variant]\nDuration: [2 weeks or until significance]\nSuccess Criteria: [20% lift, 95% confidence]\n\nVariant A (Control): [Current pricing page]\nVariant B (Treatment): [Pricing page + case study carousel]\n\nTools: [HubSpot A/B test, or Google Optimize]\n```\n\n**Statistical Significance**:\n- Minimum: 95% confidence, 1000 visitors/variant\n- Use calculator: Optimizely Sample Size Calculator\n- Don't stop tests early (false positives)\n\n**Test Velocity** (Series A target):\n- 4-6 tests/month across channels\n- 70% win rate not realistic (aim for 30-40%)\n- Document losers (learnings matter)\n\n### 6.3 Common Experiments\n\n**Landing Page Tests**:\n- Headline variations (problem-focused vs. solution-focused)\n- CTA copy (\"Start Free Trial\" vs. \"Get Started\" vs. \"Try Now\")\n- Form length (5 fields vs. 2 fields)\n- Social proof placement (above vs. below fold)\n- Hero image (product screenshot vs. people vs. abstract)\n\n**Ad Tests**:\n- Creative format (static vs. video vs. carousel)\n- Messaging angle (feature-led vs. benefit-led vs. outcome-led)\n- Audience targeting (broad vs. narrow)\n- Landing page destination (homepage vs. dedicated LP)\n\n**Email Tests**:\n- Subject line length (short vs. long)\n- Personalization (generic vs. first name vs. company name)\n- Send time (morning vs. afternoon vs. evening)\n- CTA placement (top vs. middle vs. bottom)\n\n---\n\n## 7. Handoff Protocols\n\n### 7.1 MQL â†’ SQL Handoff (Marketing â†’ Sales)\n\n**SQL Definition Criteria** (customize for your ICP):\n```\nRequired:\nâœ… Job title: Director+ (or Budget Authority confirmed)\nâœ… Company size: 50-5000 employees\nâœ… Budget: $10k+ annual (or Qualified Need confirmed)\nâœ… Timeline: Buying within 90 days\nâœ… Engagement: Demo requested OR High intent action\n\nOptional:\nâœ… Industry: Target verticals\nâœ… Geography: US/CA/EU\nâœ… Use case: Matches product capabilities\n```\n\n**HubSpot Workflow**:\n1. Lead reaches MQL threshold (lead score >75)\n2. Trigger: Automated email to SDR\n3. SDR qualification call (BANT: Budget, Authority, Need, Timeline)\n4. If qualified â†’ Mark as SQL, assign to AE\n5. If not qualified â†’ Recycle to nurture, adjust lead score\n\n**SLA** (Service Level Agreement):\n- SDR responds to MQL: 4 hours\n- AE books demo with SQL: 24 hours\n- First demo: Within 3 business days of SQL status\n\n### 7.2 SQL â†’ Opportunity Handoff (Sales â†’ RevOps)\n\n**Opportunity Creation**:\n- AE creates opportunity in HubSpot after first demo\n- Required fields: Company, Deal value, Close date, Stage\n- Pipeline stages: Discovery â†’ Demo â†’ Proposal â†’ Negotiation â†’ Closed Won/Lost\n\n**Marketing Support Post-SQL**:\n- Retargeting ads to target accounts (ABM)\n- Send case studies, ROI calculator\n- Invite to customer webinar\n- Executive briefing (for Enterprise deals)\n\n### 7.3 Lost Opportunity Handoff (Sales â†’ Marketing)\n\n**Recycle to Nurture**:\n- Reason: No budget, bad timing, wrong fit\n- Action: Move to \"Nurture\" list in HubSpot\n- Sequence: Quarterly check-in emails, invite to webinars\n- Re-engage: After 6-12 months, SDR re-qualification\n\n**Closed Lost Reasons** (track in HubSpot):\n- Price too high\n- Missing features\n- Chose competitor\n- No budget\n- Bad timing\n- Champion left company\n\n**Use lost reasons to inform**:\n- Product roadmap\n- Pricing changes\n- Competitive positioning\n- Messaging adjustments\n\n---\n\n## 8. Quick Reference\n\n### 8.1 Channel-Specific Benchmarks (B2B SaaS Series A)\n\n| Metric | LinkedIn | Google Search | SEO | Email | Partnerships |\n|--------|----------|---------------|-----|-------|--------------|\n| CTR | 0.4-0.9% | 2-5% | 1-3% | 15-25% | N/A |\n| CVR | 1-3% | 3-7% | 2-5% | 2-5% | 5-10% |\n| CAC | $150-400 | $80-250 | $50-150 | $20-80 | $100-300 |\n| MQLâ†’SQL | 10-20% | 15-25% | 12-22% | 8-15% | 20-35% |\n\n### 8.2 Budget Allocation (Recommended)\n\n**Series A ($40k-60k/month)**:\n- 40% Paid Acquisition (LinkedIn + Google)\n- 25% Content/SEO\n- 20% Partnerships\n- 10% Tools/Automation\n- 5% Experiments/Testing\n\n### 8.3 Team Handoff Quick Guide\n\n**Demand Gen â†’ Sales**:\n- Deliver: SQLs with BANT qualification\n- Frequency: Real-time via HubSpot\n- SLA: 4-hour response time\n\n**Demand Gen â†’ Product Marketing**:\n- Request: Product positioning, competitive intel, case studies\n- Frequency: Monthly sync\n- Deliverables: Updated messaging, new collateral\n\n**Demand Gen â†’ Marketing Ops**:\n- Request: Campaign tracking setup, attribution reports, data cleaning\n- Frequency: Weekly check-in\n- SLA: 48-hour turnaround for new campaigns\n\n**Paid Media â†’ Creative/Brand**:\n- Request: Ad creative (10-20 variants/month)\n- Format: Specs sheet with dimensions, copy length, brand guidelines\n- SLA: 5 business days per request\n\n**SEO â†’ Content**:\n- Request: Content based on keyword research\n- Deliverables: Content brief with target keywords, structure, length\n- Frequency: Monthly editorial calendar\n\n**Partnerships â†’ Sales**:\n- Deliver: Partner-sourced leads with partner context\n- Co-selling: Joint calls for strategic deals\n- Frequency: Weekly partner pipeline review\n\n---\n\n## Resources\n\n### references/\n\n- **hubspot-workflows.md** - Pre-built HubSpot workflow templates for lead scoring, nurture, assignment\n- **campaign-templates.md** - Ready-to-use campaign briefs for LinkedIn, Google, SEO\n- **international-playbooks.md** - Market-specific tactics for EU, US, Canada expansion\n- **attribution-guide.md** - Deep dive on multi-touch attribution setup and analysis\n\n### scripts/\n\n- **calculate_cac.py** - Calculate blended and channel-specific CAC\n- **experiment_calculator.py** - A/B test sample size and significance calculator\n\n### assets/\n\n- **campaign-brief-template.docx** - Editable campaign planning document\n- **dashboard-template.xlsx** - Pre-configured performance dashboard\n\n---\n\n**Last Updated**: October 2025 | **Version**: 1.0\n",
      "frontmatter": {
        "name": "marketing-demand-acquisition",
        "description": "Multi-channel demand generation, paid media optimization, SEO strategy, and partnership programs for Series A+ startups. Includes CAC calculator, channel playbooks, HubSpot integration, and international expansion tactics. Use when planning demand generation campaigns, optimizing paid media, building SEO strategies, establishing partnerships, or when user mentions demand gen, paid ads, LinkedIn ads, Google ads, CAC, acquisition, lead generation, or pipeline generation.",
        "license": "MIT",
        "metadata": {
          "version": "1.0.0",
          "author": "Alireza Rezvani",
          "category": "marketing",
          "domain": "demand-generation",
          "updated": "2025-10-20T00:00:00.000Z",
          "python-tools": "calculate_cac.py",
          "tech-stack": "HubSpot, LinkedIn-Ads, Google-Ads, Meta-Ads, SEO-tools",
          "target-market": "B2B-SaaS, Series-A+"
        }
      },
      "content": "\n# Marketing Demand & Acquisition\n\nExpert acquisition playbook for Series A+ startups scaling internationally (EU/US/Canada) with hybrid PLG/Sales-Led motion.\n\n## Keywords\ndemand generation, paid media, paid ads, LinkedIn ads, Google ads, Meta ads, CAC, customer acquisition cost, lead generation, MQL, SQL, pipeline generation, acquisition strategy, performance marketing, paid social, paid search, partnerships, affiliate marketing, SEO strategy, HubSpot campaigns, marketing automation, B2B marketing, SaaS marketing\n\n## Role Coverage\n\nThis skill serves:\n- **Demand Generation Manager** - Multi-channel campaigns, pipeline generation\n- **Paid Media/Performance Marketer** - Paid search/social/display optimization\n- **SEO Manager** - Organic acquisition and technical SEO\n- **Affiliate/Partnerships Manager** - Co-marketing and channel partnerships\n\n## Core KPIs by Role\n\n**Demand Gen**: MQL/SQL volume, cost per opportunity, marketing-sourced pipeline $, pipeline velocity, MQLâ†’SQL conversion rate\n\n**Paid Media**: CAC, ROAS, CPL, CPA, incrementality lift, channel efficiency ratio\n\n**SEO**: Organic sessions, non-brand traffic %, keyword rankings (P1-P3), organic-assisted conversions, technical health score\n\n**Partnerships**: Partner-sourced pipeline $, partner CAC, net new logos via partners, co-marketing ROI\n\n## Tech Stack Integration\n\n**HubSpot CRM** - Campaign tracking, lead scoring, attribution, workflows\n**Google Analytics** - Traffic analysis, conversion tracking, funnel optimization\n**Search Console** - Keyword performance, technical issues, indexing\n**LinkedIn Campaign Manager** - B2B paid social\n**Google Ads** - Search, Display, YouTube\n**Meta Ads** - Facebook, Instagram\n\n---\n\n## 1. Demand Generation Framework\n\n### 1.1 Full-Funnel Strategy (2025 Best Practice)\n\n**TOFU (Awareness)** â†’ **MOFU (Consideration)** â†’ **BOFU (Decision)** â†’ **Handoff to Sales/Product**\n\n#### TOFU Tactics\n- Paid social (LinkedIn thought leadership, Meta awareness)\n- Display advertising (programmatic, retargeting)\n- Content syndication\n- SEO (informational keywords)\n- Partnerships (co-webinars, guest content)\n- Target: Brand lift, site traffic, early-stage engagement\n\n#### MOFU Tactics\n- Paid search (solution keywords)\n- Retargeting campaigns\n- Gated content (eBooks, templates, webinars)\n- Email nurture sequences\n- Comparison pages (SEO)\n- Target: MQLs, demo requests, trial signups\n\n#### BOFU Tactics\n- Paid search (brand + competitor keywords)\n- Direct outreach campaigns\n- Free trial CTAs\n- Case studies & ROI calculators\n- Intent-based retargeting\n- Target: SQLs, demos booked, pipeline $\n\n### 1.2 Campaign Planning Template\n\n**Campaign Brief** (use this for every campaign):\n\n```\nCampaign Name: [Q2-2025-LinkedIn-ABM-Enterprise]\nObjective: [Generate 50 SQLs from Enterprise accounts ($50k+ ACV)]\nBudget: [$15k/month]\nDuration: [90 days]\nChannels: [LinkedIn Ads, Retargeting, Email]\nAudience: [Director+ at SaaS companies, 500-5000 employees, EU/US]\nOffer: [Gated Industry Benchmark Report]\nSuccess Metrics:\n  - Primary: 50 SQLs, <$300 CPO\n  - Secondary: 500 MQLs, 10% MQLâ†’SQL rate, 40% email open rate\nHubSpot Setup:\n  - Campaign ID: [create in HubSpot]\n  - Lead scoring: +20 for download, +30 for demo request\n  - Attribution: First-touch + Multi-touch\nHandoff Protocol:\n  - SQL criteria: Title + Company size + Budget confirmed\n  - Routing: Enterprise SDR team via HubSpot workflow\n  - SLA: 4-hour response time\n```\n\n### 1.3 HubSpot Campaign Tracking Setup\n\n**Step-by-step**:\n\n1. **Create Campaign in HubSpot**\n   - Marketing â†’ Campaigns â†’ Create Campaign\n   - Name: `Q2-2025-LinkedIn-ABM-Enterprise`\n   - Tag all assets (landing pages, emails, ads) with campaign ID\n\n2. **UTM Parameter Structure** (critical for attribution)\n   ```\n   utm_source={channel}       // linkedin, google, facebook\n   utm_medium={type}          // cpc, display, email, organic\n   utm_campaign={campaign-id} // q2-2025-linkedin-abm-enterprise\n   utm_content={variant}      // ad-variant-a, email-1\n   utm_term={keyword}         // [for paid search only]\n   ```\n\n3. **Lead Scoring Configuration**\n   - Navigate to: Settings â†’ Marketing â†’ Lead Scoring\n   - Campaign engagement: +10-30 points based on action depth\n   - Channel quality: LinkedIn +5, Google Search +10, Organic +15\n\n4. **Attribution Reports**\n   - Use HubSpot's multi-touch attribution (W-shaped for hybrid motion)\n   - First-touch: Awareness credit\n   - Multi-touch: Full journey credit\n   - Build custom report: Marketing â†’ Reports â†’ Attribution\n\n### 1.4 International Expansion Considerations\n\n**EU Market Entry**:\n- GDPR compliance: Double opt-in for email, explicit consent tracking in HubSpot\n- Localization: Translate landing pages, ads, emails (DE, FR, ES priority)\n- Payment: Display prices in EUR\n- Partnerships: Local co-marketing partners for credibility\n- Paid channels: LinkedIn most effective for B2B EU, Google Ads second\n\n**US/Canada Market Entry**:\n- Messaging: Direct, ROI-focused, less formal than EU\n- Paid channels: Google Ads + LinkedIn equal priority\n- Partnerships: Industry associations, review sites (G2, Capterra)\n- Content: Case studies with $ impact, not just features\n- Sales alignment: Faster sales cycles, need immediate lead follow-up\n\n**Budget Allocation** (Series A recommended):\n- EU: 40% LinkedIn, 25% Google, 20% SEO, 15% Partnerships\n- US/CA: 35% Google, 30% LinkedIn, 20% SEO, 15% Partnerships\n\n---\n\n## 2. Paid Media Optimization\n\n### 2.1 Channel Strategy Matrix\n\n| Channel | Best For | CAC Benchmark | Conversion Rate | Series A Priority |\n|---------|----------|---------------|-----------------|-------------------|\n| **LinkedIn Ads** | B2B, Enterprise, ABM | $150-$400 | 0.5-2% | â­â­â­â­â­ |\n| **Google Search** | High-intent, BOFU | $80-$250 | 2-5% | â­â­â­â­â­ |\n| **Google Display** | Retargeting, awareness | $50-$150 | 0.3-1% | â­â­â­ |\n| **Meta (FB/IG)** | SMB, consumer-like products | $60-$200 | 1-3% | â­â­â­ |\n| **YouTube** | Product demos, brand | $100-$300 | 0.5-1.5% | â­â­ |\n| **Reddit/Twitter** | Technical audiences | $40-$180 | 0.5-2% | â­â­ |\n\n### 2.2 LinkedIn Ads Playbook (Primary B2B Channel)\n\n**Campaign Structure**:\n```\nAccount\nâ””â”€ Campaign Group: [Q2-2025-Enterprise-ABM]\n   â”œâ”€ Campaign 1: [Awareness - Thought Leadership]\n   â”‚  â”œâ”€ Ad Set: [CTO/VP Eng, US, Tech Companies]\n   â”‚  â””â”€ Creatives: [3 carousel posts, 2 video ads]\n   â”œâ”€ Campaign 2: [Consideration - Product Education]\n   â”‚  â”œâ”€ Ad Set: [Engaged audience, retargeting]\n   â”‚  â””â”€ Creatives: [2 lead gen forms, 1 landing page]\n   â””â”€ Campaign 3: [Conversion - Demo Requests]\n      â”œâ”€ Ad Set: [Website visitors, content downloaders]\n      â””â”€ Creatives: [Direct demo CTA, case study]\n```\n\n**Targeting Best Practices**:\n- **Company Size**: 50-5000 employees (Series A sweet spot)\n- **Job Titles**: Director+, VP+, C-level (use LinkedIn's precise targeting)\n- **Industries**: Software, SaaS, Tech Services\n- **Matched Audiences**: Website retargeting (install Insight Tag), uploaded email lists\n- **Budget**: Start $50/day per campaign, scale 20% weekly if CAC < target\n\n**Creative Frameworks**:\n1. **Thought Leadership** - Industry insights, no product pitch\n2. **Social Proof** - Customer logos, testimonials, case study snippets\n3. **Problem-Solution** - Pain point + your solution in 3 seconds\n4. **Demo-First** - Show product immediately, skip fluff\n\n**LinkedIn Lead Gen Forms vs. Landing Pages**:\n- **Lead Gen Forms**: Higher conversion (2-3x), lower quality, use for TOFU/MOFU\n- **Landing Pages**: Lower conversion, higher quality, use for BOFU/demo requests\n- **HubSpot Sync**: Connect LinkedIn Lead Gen Forms via native integration\n\n### 2.3 Google Ads Playbook (High-Intent Capture)\n\n**Campaign Types Priority**:\n1. **Search - Brand** (highest priority, protect brand terms)\n2. **Search - Competitor** (steal market share)\n3. **Search - Solution** (problem-aware buyers)\n4. **Search - Product Category** (earlier stage)\n5. **Display - Retargeting** (re-engage warm traffic)\n\n**Search Campaign Structure**:\n```\nCampaign: [Search-Solution-Keywords]\nâ”œâ”€ Ad Group: [project management software]\nâ”‚  â”œâ”€ Keywords:\nâ”‚  â”‚  - \"project management software\" [Phrase]\nâ”‚  â”‚  - \"best project management tool\" [Phrase]\nâ”‚  â”‚  - +project +management +solution [Broad Match Modifier]\nâ”‚  â””â”€ Ads: [3 responsive search ads with 15 headlines, 4 descriptions]\nâ”‚\nâ”œâ”€ Ad Group: [team collaboration tools]\n   â”œâ”€ Keywords: [5-10 tightly themed keywords]\n   â””â”€ Ads: [3 responsive search ads]\n```\n\n**Keyword Strategy**:\n- **Brand Terms**: Exact match, bid high, protect brand\n- **Competitor Terms**: \"[Competitor] alternative\", \"[Competitor] vs [You]\"\n- **Solution Terms**: \"best [category] software\", \"top [category] tools\"\n- **Problem Terms**: \"how to [solve problem]\"\n- **Negative Keywords**: Maintain list of 100+ (free, cheap, jobs, career, reviews)\n\n**Bid Strategy** (2025 best practice):\n- New campaigns: Start Manual CPC for control\n- After 50+ conversions: Switch to Target CPA\n- After 100+ conversions: Test Maximize Conversions with tCPA\n- EU markets: Bid 15-20% higher for same quality\n\n**Ad Copy Framework** (Responsive Search Ads):\n```\nHeadlines (15 required):\n- H1-3: Value props (Save 10 hours/week, Trusted by 500+ teams)\n- H4-6: Features (AI-powered, Real-time sync, Mobile app)\n- H7-9: Social proof (4.8â˜… G2 rating, Used by Microsoft)\n- H10-12: CTAs (Start free trial, Book demo, See pricing)\n- H13-15: Keywords pinned (Dynamic insertion)\n\nDescriptions (4 required):\n- D1: Primary value prop + CTA (30-60 chars)\n- D2: Feature list + differentiator (60-90 chars)\n- D3: Social proof + urgency (45-90 chars)\n- D4: Backup generic (60-90 chars)\n```\n\n### 2.4 Meta Ads Playbook (SMB/Lower ACV)\n\n**When to Use Meta**:\n- âœ… Product ACV <$10k\n- âœ… Visual product (UI, consumer-facing)\n- âœ… SMB/prosumer audience\n- âœ… Broader awareness campaigns\n- âŒ Enterprise/high ACV (use LinkedIn)\n\n**Campaign Setup**:\n```\nCampaign Objective: [Conversions]\nâ”œâ”€ Ad Set 1: [Lookalike - 1% of converters]\nâ”‚  â””â”€ Placement: [Feed + Stories, Auto]\nâ”œâ”€ Ad Set 2: [Interest - Business Software]\nâ”‚  â””â”€ Placement: [Feed only]\nâ””â”€ Ad Set 3: [Retargeting - Website 30d]\n   â””â”€ Placement: [All placements]\n```\n\n**Audience Strategy**:\n1. **Core Audiences**: Interests (business tools, productivity, startups)\n2. **Lookalike**: 1% of purchasers/high-value leads\n3. **Retargeting**: 30-day website visitors, video viewers (75%+)\n\n**Creative Best Practices**:\n- Use video (1:1 or 9:16 for Stories)\n- First 3 seconds = hook (problem or result)\n- Show product UI in action\n- Add captions (85% watch muted)\n- Test 3-5 creative variants per campaign\n\n### 2.5 Budget Allocation & Scaling\n\n**Initial Budget** (Series A, $30k-50k/month total):\n```\nChannel            Budget    Expected Results\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nLinkedIn Ads       $15k      50 MQLs, 10 SQLs, $1.5k CAC\nGoogle Search      $12k      80 MQLs, 20 SQLs, $600 CAC\nGoogle Display     $5k       120 MQLs, 5 SQLs, $1k CAC\nMeta Ads           $5k       100 MQLs, 8 SQLs, $625 CAC\nPartnerships       $3k       20 MQLs, 5 SQLs, $600 CAC\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nTOTAL              $40k      370 MQLs, 48 SQLs, $833 avg CAC\n```\n\n**Scaling Rules**:\n1. If CAC <target â†’ Increase budget 20% weekly\n2. If CAC >target â†’ Pause, optimize, relaunch\n3. If conversion rate drops >20% â†’ Check landing page, offer fatigue\n4. Scale winners, kill losers fast (2-week test minimum)\n\n**HubSpot ROI Dashboard**:\n- Marketing â†’ Reports â†’ Create Custom Report\n- Metrics: Spend, Leads, MQLs, SQLs, CAC, ROAS, Pipeline $\n- Dimensions: Campaign, Channel, Region\n- Frequency: Daily review, weekly optimization\n\n---\n\n## 3. SEO Strategy\n\n### 3.1 Technical SEO Foundation (Must-Have)\n\n**Pre-Launch Checklist**:\n- [ ] XML sitemap submitted to Search Console\n- [ ] Robots.txt configured (allow crawling)\n- [ ] HTTPS enabled (SSL certificate)\n- [ ] Page speed >90 mobile (Google PageSpeed Insights)\n- [ ] Core Web Vitals passing (LCP, FID, CLS)\n- [ ] Structured data (Organization, Product, FAQ schema)\n- [ ] Canonical tags on all pages\n- [ ] Hreflang tags for international (en-US, en-GB, de-DE, etc.)\n\n**Technical Audit** (quarterly):\n```\n1. Crawl site with Screaming Frog\n2. Check for:\n   - 404 errors (fix or redirect)\n   - Redirect chains (consolidate)\n   - Duplicate content (canonicalize)\n   - Missing meta descriptions\n   - Slow pages (>3s load time)\n   - Mobile usability issues\n3. Fix issues in priority order: Critical â†’ High â†’ Medium\n```\n\n### 3.2 Keyword Strategy Framework\n\n**Keyword Research Process**:\n1. **Seed Keywords** - Your product category (e.g., \"project management software\")\n2. **Use Tools** - Ahrefs, SEMrush, or free: Google Keyword Planner + Search Console\n3. **Analyze** - Volume, difficulty, intent, SERP features\n4. **Prioritize** - Quick wins (low difficulty, high intent)\n\n**Keyword Tiers**:\n\n**Tier 1: High-Intent BOFU** (target first)\n- \"best [product category]\"\n- \"[product category] for [use case]\"\n- \"[competitor] alternative\"\n- Volume: 100-1k/mo, Difficulty: Medium, Intent: Commercial\n\n**Tier 2: Solution-Aware MOFU**\n- \"how to [solve problem]\"\n- \"[problem] solution\"\n- \"[use case] tools\"\n- Volume: 500-5k/mo, Difficulty: Medium-High, Intent: Informational-Commercial\n\n**Tier 3: Problem-Aware TOFU**\n- \"what is [concept]\"\n- \"[problem] examples\"\n- \"[industry] challenges\"\n- Volume: 1k-10k/mo, Difficulty: High, Intent: Informational\n\n**International Keyword Research**:\n- Use Ahrefs/SEMrush with language filters\n- Translate keywords, don't just localize (cultural nuances matter)\n- EU: Higher trust in localized content (domain.de > domain.com/de)\n- UK: Use British spelling (optimise vs. optimize)\n\n### 3.3 On-Page SEO Template\n\n**Page Optimization Checklist**:\n```\nURL: [/best-project-management-software]\nTitle Tag (60 chars): [Best Project Management Software 2025 | YourBrand]\nMeta Description (155 chars): [Compare top 10 PM tools. Features, pricing, reviews. Find the perfect fit for your team. Free trials available.]\n\nH1 (60 chars): [Best Project Management Software in 2025]\nH2s (structure):\n  - What is Project Management Software?\n  - Top 10 PM Tools Compared\n  - Key Features to Look For\n  - Pricing & Plans\n  - How to Choose\n  - FAQ\n\nContent:\n  - Length: 2000-3000 words (comprehensive)\n  - Keyword density: 1-2% (natural)\n  - Internal links: 3-5 relevant pages\n  - External links: 2-3 authoritative sources\n  - Images: 3-5 with alt text\n  - Schema: Product, FAQ, HowTo\n\nCTA:\n  - Above fold: [Start Free Trial]\n  - Mid-content: [Compare Plans]\n  - End: [Book Demo]\n```\n\n**Content Refresh Schedule**:\n- Tier 1 pages: Update quarterly (rankings, pricing, features)\n- Tier 2 pages: Update semi-annually\n- Tier 3 pages: Update annually\n- All pages: Monitor Search Console for ranking drops, refresh immediately\n\n### 3.4 Link Building Strategy (2025 Best Practices)\n\n**Link Acquisition Tactics** (in priority order):\n\n**1. Digital PR** (highest ROI)\n- Publish original research/data\n- Create industry reports\n- Pitch journalists (use HARO, Terkel, Featured)\n- Target: Industry blogs, tech publications\n\n**2. Guest Posting** (quality over quantity)\n- Target: Domain Authority (DA) 40+ sites\n- Avoid: Link farms, PBNs, paid links (Google penalty risk)\n- Anchor text: Branded (70%), topical (20%), exact match (10%)\n\n**3. Partnerships & Co-Marketing**\n- Partner with complementary SaaS tools\n- Create co-branded content\n- Exchange homepage links (footer or partner section)\n\n**4. Community Engagement**\n- Answer questions on Reddit, Quora\n- Participate in industry forums\n- Create tools/calculators â†’ natural backlinks\n\n**5. Broken Link Building**\n- Find broken links on competitor sites\n- Offer your content as replacement\n- Tools: Ahrefs' Broken Backlinks report\n\n**Link Velocity** (avoid penalties):\n- Natural: 5-10 links/month for new sites\n- Aggressive: 20-30 links/month after 6 months\n- Monitor: Google Search Console for manual actions\n\n### 3.5 Content Strategy for SEO\n\n**Content Types by Funnel Stage**:\n\n**TOFU (Awareness)**:\n- Blog posts: \"Ultimate Guide to [Topic]\"\n- Listicles: \"Top 10 [Category]\"\n- Industry reports: \"[Industry] State of 2025\"\n- Target: Broad keywords, thought leadership\n\n**MOFU (Consideration)**:\n- Comparison pages: \"[Your Product] vs [Competitor]\"\n- Best of lists: \"Best [Category] for [Use Case]\"\n- How-to guides: \"How to [Solve Problem] with [Product]\"\n- Target: Solution keywords, product education\n\n**BOFU (Decision)**:\n- Product pages: \"[Product] Features & Pricing\"\n- Case studies: \"How [Customer] Achieved [Result]\"\n- Landing pages: \"Start Free Trial\"\n- Target: Brand keywords, high-intent searches\n\n**Content Calendar** (Series A minimum):\n- TOFU: 4 posts/month (1 per week)\n- MOFU: 2 posts/month\n- BOFU: 1 post/month\n- Refresh: 2 existing posts/month\n\n### 3.6 Local SEO (For Regional Offices)\n\n**Google Business Profile Setup** (per location):\n- Complete all fields: Name, address, phone, hours, category\n- Upload photos: Office, team, product (10+ images)\n- Collect reviews: Ask customers, automate via HubSpot workflow\n- Post updates: Weekly posts about company news, events\n\n**Local Citations** (US/Canada/EU):\n- Submit to: Yelp, Yellow Pages, local directories\n- NAP consistency: Name, Address, Phone identical everywhere\n- Industry directories: Software review sites (G2, Capterra)\n\n---\n\n## 4. Partnerships & Affiliate Programs\n\n### 4.1 Partnership Types & Strategy\n\n**Partnership Tiers**:\n\n**Tier 1: Strategic Partnerships** (high impact, low volume)\n- Target: Complementary SaaS tools with overlapping ICPs\n- Structure: Co-marketing, product integrations, revenue share\n- Examples: Slack â†” Asana, Shopify â†” Klaviyo\n- Effort: High (6-12 months to establish)\n- ROI: Very high (100+ leads/month after ramp)\n\n**Tier 2: Affiliate Partners** (scalable)\n- Target: Bloggers, review sites, industry influencers\n- Structure: Commission per sale (10-30% first year)\n- Platform: Use PartnerStack, Impact, or Rewardful\n- Effort: Medium (setup once, ongoing management)\n- ROI: Medium-High (depends on partner quality)\n\n**Tier 3: Referral Partners** (customer-driven)\n- Target: Your existing customers\n- Structure: Referral bonus ($500-$1k per SQL)\n- Platform: Built into HubSpot or standalone (Friendbuy)\n- Effort: Low (automate via workflows)\n- ROI: Medium (5-10% of customers refer)\n\n**Tier 4: Marketplace Listings** (distribution)\n- Target: Shopify App Store, Salesforce AppExchange, HubSpot Marketplace\n- Structure: Free listing + revenue share\n- Effort: Medium (initial listing, ongoing updates)\n- ROI: Low-Medium (brand visibility + discovery)\n\n### 4.2 Partnership Playbook\n\n**Step 1: Identify Partners**\n```\nCriteria:\n- Similar ICP (overlapping audience, no direct competition)\n- Product fit (complementary, not substitute)\n- Scale (similar company size, funding stage)\n- Values alignment (culture, brand positioning)\n\nResearch:\n- Tools: BuiltWith, SimilarWeb, LinkedIn Sales Nav\n- Look for: Integration pages, partner pages, co-marketing history\n```\n\n**Step 2: Outreach Template**\n```\nSubject: [YourBrand] â†” [TheirBrand] Partnership Idea\n\nHi [Name],\n\nI'm [Your Name] at [YourBrand] - we help [ICP] with [value prop].\n\nI noticed [TheirBrand] serves a similar audience, and I think our customers would benefit from an integration between [YourProduct] and [TheirProduct].\n\nWould you be open to exploring a partnership? I'm thinking:\n- Product integration (bi-directional sync)\n- Co-marketing (joint webinar, case study)\n- Revenue share (referral fees)\n\nLet me know if you'd like to chat. Happy to send more details.\n\nBest,\n[Your Name]\n```\n\n**Step 3: Partnership Agreement**\n- Define scope (integration depth, marketing commitment)\n- Revenue model (rev share %, referral fees, co-selling)\n- Success metrics (leads, pipeline, revenue)\n- Term (12-24 months, with renewal)\n- Exit clause (90-day notice)\n\n**Step 4: Activation & Enablement**\n- Create co-branded assets (landing page, webinar deck, one-pager)\n- Train partner sales team (product demo, pitch deck, objection handling)\n- Set up tracking (UTM parameters, partner portal in HubSpot)\n\n**Step 5: Ongoing Management**\n- Quarterly business reviews (QBRs)\n- Monthly check-ins (pipeline, blockers)\n- Co-marketing calendar (1-2 activities/quarter)\n- Reporting (HubSpot dashboard for partner-sourced pipeline)\n\n### 4.3 Affiliate Program Setup\n\n**Platform Selection**:\n- **PartnerStack** - Best for B2B SaaS, native integrations\n- **Impact** - Enterprise-grade, high control\n- **Rewardful** - Lightweight, Stripe integration\n- **FirstPromoter** - Budget-friendly, good analytics\n\n**Commission Structure** (Series A typical):\n```\nTier 1: Influencers/Publishers\n- 30% recurring for 12 months\n- Or: $500 flat per SQL\n- Bonus: $1k for 10+ referrals/quarter\n\nTier 2: Bloggers/Content Creators\n- 20% recurring for 12 months\n- Or: $300 flat per SQL\n\nTier 3: Customers (Referral Program)\n- $500 per closed deal\n- Or: 1 month free for both referrer + referee\n```\n\n**Recruitment Strategy**:\n1. **Outbound**: Find industry bloggers, YouTubers, newsletter writers\n2. **Inbound**: \"Become an Affiliate\" page, promote in product\n3. **Events**: Recruit at conferences, meetups\n4. **Communities**: Reddit, LinkedIn groups, Slack communities\n\n**Affiliate Enablement Kit**:\n- Brand assets (logos, product screenshots)\n- Pre-written content (blog post templates, social posts)\n- Tracking links (unique UTM codes per affiliate)\n- Sales collateral (one-pagers, case studies, demo videos)\n\n### 4.4 Co-Marketing Campaigns\n\n**Joint Webinar Playbook**:\n```\nPlanning (6 weeks out):\n- Define topic (audience pain point, not product pitch)\n- Assign roles (host, co-host, Q&A moderator)\n- Create landing page (co-branded, dual logos)\n- Design promo assets (social graphics, email templates)\n\nPromotion (4 weeks out):\n- Email: 3 sends (announcement, reminder, last chance)\n- Social: 8-10 posts per partner (LinkedIn, Twitter)\n- Paid: $2k budget for LinkedIn ads â†’ landing page\n- Partners: Cross-promote to each other's audiences\n\nExecution (day of):\n- 60-min format: 5min intro, 40min content, 15min Q&A\n- Record for on-demand\n- Polls/CTAs: Mid-webinar poll, end with demo CTA\n\nFollow-up (1 week after):\n- Send recording to all registrants\n- Nurture sequence: 3 emails over 2 weeks\n- Split leads: Each partner owns their referred leads\n- Report: Attendees, pipeline generated, next steps\n```\n\n**Other Co-Marketing Tactics**:\n- **Co-branded Content**: eBook, report, guide\n- **Case Study**: Joint customer success story\n- **Bundle Offer**: \"Buy [YourProduct] + [TheirProduct], save 20%\"\n- **Cross-promotion**: Feature each other in newsletters\n- **Social Media Takeover**: Guest post on each other's channels\n\n### 4.5 HubSpot Partner Tracking\n\n**Setup**:\n1. **Create Partner Property**\n   - Settings â†’ Properties â†’ Create \"Partner Source\" dropdown\n   - Values: Partner A, Partner B, Affiliate Network, etc.\n\n2. **UTM Tracking**\n   - Partner links: `?utm_source=partner-name&utm_medium=referral`\n   - HubSpot auto-captures UTM parameters\n\n3. **Lead Assignment**\n   - Workflow: If \"Partner Source\" is set â†’ Assign to Partner Manager\n   - Notification: Slack alert when partner lead arrives\n\n4. **Reporting**\n   - Dashboard: Partner-sourced leads, pipeline, revenue\n   - Report to partners: Monthly performance summary\n\n---\n\n## 5. Attribution & Reporting\n\n### 5.1 Attribution Models (HubSpot Native)\n\n**Model Selection** (use multi-touch for hybrid motion):\n\n**First-Touch** - Credit to first interaction\n- Use case: Awareness campaigns, brand building\n- Pro: Shows what drives discovery\n- Con: Ignores nurturing influence\n\n**Last-Touch** - Credit to last interaction before conversion\n- Use case: Direct response, BOFU campaigns\n- Pro: Shows what closes deals\n- Con: Ignores earlier touchpoints\n\n**Multi-Touch (W-Shaped)** - Credit to first, last, and middle (40-20-40 split)\n- Use case: Hybrid PLG/Sales-Led (recommended for Series A)\n- Pro: Full-funnel view\n- Con: More complex to explain to stakeholders\n\n**HubSpot Setup**:\n- Marketing â†’ Reports â†’ Attribution â†’ Select Model\n- Default: Use Multi-Touch for holistic view\n- Compare: Run reports side-by-side to see differences\n\n### 5.2 Reporting Dashboard (HubSpot)\n\n**Weekly Performance Dashboard**:\n```\nMetrics to Track:\n1. Traffic: Visits, unique visitors, bounce rate\n2. Leads: MQLs, SQLs, conversion rates\n3. Pipeline: Opportunities created, value, velocity\n4. CAC: Spend Ã· customers acquired\n5. Channel Mix: % of leads by source\n\nDimensions:\n- By Channel: Organic, Paid, Email, Social, Referral\n- By Campaign: Individual campaign performance\n- By Region: US, CA, EU breakdown\n- By Stage: TOFU, MOFU, BOFU metrics\n```\n\n**Monthly Executive Dashboard**:\n```\nKPIs:\n1. Marketing-Sourced Pipeline: $[X]M (target: $[Y]M)\n2. Marketing-Sourced Revenue: $[X]k (target: $[Y]k)\n3. Blended CAC: $[X] (target: $[Y])\n4. MQLâ†’SQL Rate: [X]% (target: [Y]%)\n5. Pipeline Velocity: [X] days (target: [Y] days)\n6. ROMI: [X]:1 (target: 3:1+)\n\nInsights:\n- Top performing campaigns\n- Underperforming channels (kill or optimize)\n- New experiments to test next month\n- Budget reallocation recommendations\n```\n\n### 5.3 Google Analytics Setup\n\n**Events to Track** (GA4):\n```\nEngagement:\n- page_view (auto-tracked)\n- scroll (75% depth)\n- video_play (product demos)\n- file_download (whitepapers, eBooks)\n\nConversions:\n- sign_up (free trial, account created)\n- demo_request (calendar booking)\n- contact_form (inbound interest)\n- pricing_view (pricing page visit)\n\nE-commerce (if applicable):\n- add_to_cart\n- begin_checkout\n- purchase\n```\n\n**Custom Dimensions**:\n- User Type: Free vs. Paid\n- Plan Type: Starter, Pro, Enterprise\n- HubSpot Lead Status: MQL, SQL, Customer\n- Campaign: HubSpot Campaign ID\n\n**Integration with HubSpot**:\n- Use HubSpot tracking code (includes GA4 by default)\n- Or: Google Tag Manager for advanced tracking\n- Sync: GA4 audiences â†’ HubSpot lists for retargeting\n\n---\n\n## 6. Experimentation Framework\n\n### 6.1 A/B Testing Prioritization (ICE Score)\n\n**Formula**: ICE = (Impact Ã— Confidence Ã— Ease) Ã· 3\n\nRate each factor 1-10:\n- **Impact**: How much will this move the needle?\n- **Confidence**: How sure are you it will work?\n- **Ease**: How easy is it to implement?\n\n**Example Tests** (sorted by ICE score):\n\n| Test | Impact | Confidence | Ease | ICE | Priority |\n|------|--------|------------|------|-----|----------|\n| CTA button color (red vs. green) | 3 | 8 | 10 | 7.0 | Low |\n| Landing page headline rewrite | 8 | 7 | 8 | 7.7 | Medium |\n| Pricing page redesign | 9 | 6 | 4 | 6.3 | Medium |\n| New lead magnet offer | 9 | 8 | 7 | 8.0 | High |\n| Add live chat to pricing page | 7 | 9 | 8 | 8.0 | High |\n\n### 6.2 Test Design & Execution\n\n**Test Template**:\n```\nHypothesis: [Adding a case study carousel to the pricing page will increase demo requests by 20% because users need social proof before committing]\n\nMetric: [Demo requests from /pricing page]\nSample Size: [1000 visitors per variant]\nDuration: [2 weeks or until significance]\nSuccess Criteria: [20% lift, 95% confidence]\n\nVariant A (Control): [Current pricing page]\nVariant B (Treatment): [Pricing page + case study carousel]\n\nTools: [HubSpot A/B test, or Google Optimize]\n```\n\n**Statistical Significance**:\n- Minimum: 95% confidence, 1000 visitors/variant\n- Use calculator: Optimizely Sample Size Calculator\n- Don't stop tests early (false positives)\n\n**Test Velocity** (Series A target):\n- 4-6 tests/month across channels\n- 70% win rate not realistic (aim for 30-40%)\n- Document losers (learnings matter)\n\n### 6.3 Common Experiments\n\n**Landing Page Tests**:\n- Headline variations (problem-focused vs. solution-focused)\n- CTA copy (\"Start Free Trial\" vs. \"Get Started\" vs. \"Try Now\")\n- Form length (5 fields vs. 2 fields)\n- Social proof placement (above vs. below fold)\n- Hero image (product screenshot vs. people vs. abstract)\n\n**Ad Tests**:\n- Creative format (static vs. video vs. carousel)\n- Messaging angle (feature-led vs. benefit-led vs. outcome-led)\n- Audience targeting (broad vs. narrow)\n- Landing page destination (homepage vs. dedicated LP)\n\n**Email Tests**:\n- Subject line length (short vs. long)\n- Personalization (generic vs. first name vs. company name)\n- Send time (morning vs. afternoon vs. evening)\n- CTA placement (top vs. middle vs. bottom)\n\n---\n\n## 7. Handoff Protocols\n\n### 7.1 MQL â†’ SQL Handoff (Marketing â†’ Sales)\n\n**SQL Definition Criteria** (customize for your ICP):\n```\nRequired:\nâœ… Job title: Director+ (or Budget Authority confirmed)\nâœ… Company size: 50-5000 employees\nâœ… Budget: $10k+ annual (or Qualified Need confirmed)\nâœ… Timeline: Buying within 90 days\nâœ… Engagement: Demo requested OR High intent action\n\nOptional:\nâœ… Industry: Target verticals\nâœ… Geography: US/CA/EU\nâœ… Use case: Matches product capabilities\n```\n\n**HubSpot Workflow**:\n1. Lead reaches MQL threshold (lead score >75)\n2. Trigger: Automated email to SDR\n3. SDR qualification call (BANT: Budget, Authority, Need, Timeline)\n4. If qualified â†’ Mark as SQL, assign to AE\n5. If not qualified â†’ Recycle to nurture, adjust lead score\n\n**SLA** (Service Level Agreement):\n- SDR responds to MQL: 4 hours\n- AE books demo with SQL: 24 hours\n- First demo: Within 3 business days of SQL status\n\n### 7.2 SQL â†’ Opportunity Handoff (Sales â†’ RevOps)\n\n**Opportunity Creation**:\n- AE creates opportunity in HubSpot after first demo\n- Required fields: Company, Deal value, Close date, Stage\n- Pipeline stages: Discovery â†’ Demo â†’ Proposal â†’ Negotiation â†’ Closed Won/Lost\n\n**Marketing Support Post-SQL**:\n- Retargeting ads to target accounts (ABM)\n- Send case studies, ROI calculator\n- Invite to customer webinar\n- Executive briefing (for Enterprise deals)\n\n### 7.3 Lost Opportunity Handoff (Sales â†’ Marketing)\n\n**Recycle to Nurture**:\n- Reason: No budget, bad timing, wrong fit\n- Action: Move to \"Nurture\" list in HubSpot\n- Sequence: Quarterly check-in emails, invite to webinars\n- Re-engage: After 6-12 months, SDR re-qualification\n\n**Closed Lost Reasons** (track in HubSpot):\n- Price too high\n- Missing features\n- Chose competitor\n- No budget\n- Bad timing\n- Champion left company\n\n**Use lost reasons to inform**:\n- Product roadmap\n- Pricing changes\n- Competitive positioning\n- Messaging adjustments\n\n---\n\n## 8. Quick Reference\n\n### 8.1 Channel-Specific Benchmarks (B2B SaaS Series A)\n\n| Metric | LinkedIn | Google Search | SEO | Email | Partnerships |\n|--------|----------|---------------|-----|-------|--------------|\n| CTR | 0.4-0.9% | 2-5% | 1-3% | 15-25% | N/A |\n| CVR | 1-3% | 3-7% | 2-5% | 2-5% | 5-10% |\n| CAC | $150-400 | $80-250 | $50-150 | $20-80 | $100-300 |\n| MQLâ†’SQL | 10-20% | 15-25% | 12-22% | 8-15% | 20-35% |\n\n### 8.2 Budget Allocation (Recommended)\n\n**Series A ($40k-60k/month)**:\n- 40% Paid Acquisition (LinkedIn + Google)\n- 25% Content/SEO\n- 20% Partnerships\n- 10% Tools/Automation\n- 5% Experiments/Testing\n\n### 8.3 Team Handoff Quick Guide\n\n**Demand Gen â†’ Sales**:\n- Deliver: SQLs with BANT qualification\n- Frequency: Real-time via HubSpot\n- SLA: 4-hour response time\n\n**Demand Gen â†’ Product Marketing**:\n- Request: Product positioning, competitive intel, case studies\n- Frequency: Monthly sync\n- Deliverables: Updated messaging, new collateral\n\n**Demand Gen â†’ Marketing Ops**:\n- Request: Campaign tracking setup, attribution reports, data cleaning\n- Frequency: Weekly check-in\n- SLA: 48-hour turnaround for new campaigns\n\n**Paid Media â†’ Creative/Brand**:\n- Request: Ad creative (10-20 variants/month)\n- Format: Specs sheet with dimensions, copy length, brand guidelines\n- SLA: 5 business days per request\n\n**SEO â†’ Content**:\n- Request: Content based on keyword research\n- Deliverables: Content brief with target keywords, structure, length\n- Frequency: Monthly editorial calendar\n\n**Partnerships â†’ Sales**:\n- Deliver: Partner-sourced leads with partner context\n- Co-selling: Joint calls for strategic deals\n- Frequency: Weekly partner pipeline review\n\n---\n\n## Resources\n\n### references/\n\n- **hubspot-workflows.md** - Pre-built HubSpot workflow templates for lead scoring, nurture, assignment\n- **campaign-templates.md** - Ready-to-use campaign briefs for LinkedIn, Google, SEO\n- **international-playbooks.md** - Market-specific tactics for EU, US, Canada expansion\n- **attribution-guide.md** - Deep dive on multi-touch attribution setup and analysis\n\n### scripts/\n\n- **calculate_cac.py** - Calculate blended and channel-specific CAC\n- **experiment_calculator.py** - A/B test sample size and significance calculator\n\n### assets/\n\n- **campaign-brief-template.docx** - Editable campaign planning document\n- **dashboard-template.xlsx** - Pre-configured performance dashboard\n\n---\n\n**Last Updated**: October 2025 | **Version**: 1.0\n"
    }
  },
  "alirezarezvani-claude-skills-marketing-strategy-pmm": {
    "id": "alirezarezvani-claude-skills-marketing-strategy-pmm",
    "name": "marketing-strategy-pmm",
    "description": "Product marketing, positioning, GTM strategy, and competitive intelligence. Includes ICP definition, April Dunford positioning methodology, launch playbooks, competitive battlecards, and international market entry guides. Use when developing positioning, planning product launches, creating messaging, analyzing competitors, entering new markets, enabling sales, or when user mentions product marketing, positioning, GTM, go-to-market, competitive analysis, market entry, or sales enablement.",
    "repo": {
      "owner": "alirezarezvani",
      "name": "claude-skills",
      "fullName": "alirezarezvani/claude-skills",
      "url": "https://github.com/alirezarezvani/claude-skills/tree/main/marketing-skill/marketing-strategy-pmm",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 521,
      "forks": 103,
      "language": "Python",
      "topics": [
        "agentic-ai",
        "agentic-coding",
        "anthropic-claude",
        "claude-ai",
        "claude-ai-skills",
        "claude-code",
        "claude-code-skills",
        "claude-skills",
        "claudecode-subagents"
      ],
      "updatedAt": "2026-01-08T12:36:14Z",
      "pushedAt": "2026-01-08T08:39:21Z",
      "createdAt": "2025-10-19T04:04:05Z",
      "license": "MIT License"
    },
    "category": "marketing",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: marketing-strategy-pmm\ndescription: Product marketing, positioning, GTM strategy, and competitive intelligence. Includes ICP definition, April Dunford positioning methodology, launch playbooks, competitive battlecards, and international market entry guides. Use when developing positioning, planning product launches, creating messaging, analyzing competitors, entering new markets, enabling sales, or when user mentions product marketing, positioning, GTM, go-to-market, competitive analysis, market entry, or sales enablement.\nlicense: MIT\nmetadata:\n  version: 1.0.0\n  author: Alireza Rezvani\n  category: marketing\n  domain: product-marketing\n  updated: 2025-10-20\n  frameworks: April-Dunford-positioning, ICP-definition, messaging-hierarchy\n  target-market: B2B-SaaS, international-expansion, Series-A+\n---\n\n# Marketing Strategy & Product Marketing\n\nExpert Product Marketing playbook for Series A+ startups expanding internationally with hybrid PLG/Sales-Led motion.\n\n## Keywords\nproduct marketing, positioning, GTM, go-to-market strategy, competitive analysis, competitive intelligence, battlecards, ICP, ideal customer profile, messaging, value proposition, product launch, market entry, international expansion, sales enablement, win loss analysis, PMM, product marketing manager, market positioning, competitive landscape, sales training\n\n## Role Coverage\n\nThis skill serves:\n- **Product Marketing Manager (PMM)** - Positioning, messaging, competitive intel, launches\n- **Head of Marketing** - Strategy, budget, org design, pipeline targets\n- **Head of Growth** - Experimentation, activation, retention, growth loops\n- **CMO/VP Marketing** - Executive strategy, board reporting, team leadership\n\n## Core KPIs by Role\n\n**PMM**: Product adoption rate, win rate vs. competitors, sales velocity, launch impact metrics, competitive win rate, deal size growth\n\n**Head of Marketing**: Marketing-sourced pipeline $, CAC/LTV ratio, ROMI (3:1+ target), brand awareness lift, market share growth\n\n**Head of Growth**: Activation rate, WAU/MAU, conversion rates across funnel, payback period, viral coefficient (PLG)\n\n**CMO**: Revenue growth %, pipeline coverage (3-4x), team productivity, budget efficiency, NPS/brand health\n\n## Tech Stack Integration\n\n**HubSpot** - CRM, deal tracking, competitive loss analysis, sales enablement content\n**Google Analytics** - Product usage, activation funnels, feature adoption\n**Gong/Chorus** - Sales call analysis, competitive intelligence, objection tracking\n**Productboard** - Feature requests, customer feedback, roadmap prioritization\n**Notion/Confluence** - Internal wiki, positioning docs, competitive battlecards\n\n---\n\n## 1. Strategic Foundation\n\n### 1.1 Company Strategy Framework (Series A Context)\n\n**Current State Analysis**:\n```\nStage: Series A\nFunding: $5-15M raised\nTeam Size: 20-50 people\nRevenue: $1-5M ARR\nMarket Position: Challenger/Niche leader\nGrowth Rate Target: 3-5x YoY\n\nKey Challenges:\n- Prove product-market fit at scale\n- Expand from early adopters â†’ mainstream\n- Enter new markets (EU/US/Canada)\n- Compete against incumbents\n- Build repeatable sales motion\n```\n\n**Strategic Priorities** (in order):\n1. **Nail positioning** - Clear, differentiated value prop\n2. **Scale acquisition** - Repeatable, efficient channels\n3. **Prove retention** - Product stickiness, expansion revenue\n4. **Expand markets** - Geographic + vertical expansion\n5. **Build brand** - Awareness, trust, category leadership\n\n### 1.2 ICP (Ideal Customer Profile) Definition\n\n**B2B SaaS ICP Framework**:\n\n**Firmographics**:\n- Company size: 50-5000 employees (Series A sweet spot)\n- Industry: SaaS, Tech, Professional Services\n- Geography: US, Canada, UK, Germany, France (prioritize by TAM)\n- Revenue: $5M-$500M annual\n- Funding stage: Seed to Growth (avoid pre-product)\n\n**Technographics**:\n- Tech stack: Modern (cloud-first, API-driven)\n- Maturity: Growing fast, willing to adopt new tools\n- Existing tools: [List competitors + complementary products]\n- Integration needs: Must integrate with [Salesforce, Slack, etc.]\n\n**Psychographics**:\n- Pain level: 7-10/10 (acute pain, not nice-to-have)\n- Buyer motivation: Efficiency, cost savings, revenue growth\n- Decision process: 2-6 month sales cycle\n- Risk tolerance: Early majority (not bleeding edge)\n\n**Buyer Personas** (3-5 personas max):\n\n**Primary: Economic Buyer** (signs contract)\n- Title: VP, Director, Head of [Department]\n- Goals: ROI, team productivity, cost reduction\n- Fears: Implementation failure, team resistance, budget waste\n- Messaging: Business outcomes, ROI, case studies\n\n**Secondary: Technical Buyer** (evaluates product)\n- Title: Senior Engineer, Architect, Tech Lead\n- Goals: Solves technical problem, easy integration\n- Fears: Technical debt, vendor lock-in, poor support\n- Messaging: Technical capabilities, architecture, security\n\n**User/Champion** (advocates internally)\n- Title: Manager, Team Lead, Power User\n- Goals: Makes their job easier, team loves it\n- Fears: Learning curve, change management\n- Messaging: UX, ease of use, quick wins\n\n**ICP Validation Checklist**:\n- [ ] 5+ paying customers match this profile\n- [ ] Fastest sales cycles (< median time to close)\n- [ ] Highest LTV (> median customer value)\n- [ ] Lowest churn (< 5% annual)\n- [ ] Strong product engagement (daily/weekly usage)\n- [ ] Referenceable (NPS 9-10, willing to do case studies)\n\n**HubSpot ICP Tracking**:\n- Create \"ICP Fit\" property: A (perfect), B (good), C (okay), D (poor)\n- Score based on firmographics, engagement, product usage\n- Report: Win rate by ICP score, pipeline by ICP score\n- Action: Focus acquisition on ICP A/B, nurture C, disqualify D\n\n### 1.3 Market Segmentation Strategy\n\n**Segmentation Dimensions**:\n\n**By Company Size** (recommend starting with one):\n- **SMB** (10-200 employees) - Self-serve PLG, low touch, $100-$2k ACV\n- **Mid-Market** (200-2000 employees) - Hybrid, inside sales, $2k-$50k ACV\n- **Enterprise** (2000+ employees) - Sales-led, field sales, $50k+ ACV\n\n**By Vertical** (choose 2-3 focus verticals):\n- Horizontal: Broad appeal (e.g., project management for any industry)\n- Vertical: Industry-specific (e.g., healthcare CRM, fintech compliance)\n- Approach: Start horizontal, add verticals as you scale\n\n**By Use Case** (messaging varies):\n- Use Case A: [e.g., Team collaboration]\n- Use Case B: [e.g., Client management]\n- Use Case C: [e.g., Project tracking]\n- Each use case = different landing page, messaging, case studies\n\n**By Geography** (Series A focus):\n- **US/Canada**: Largest TAM, fastest sales cycles, highest willingness to pay\n- **UK**: English-speaking, gateway to EU, similar buying behavior to US\n- **Germany**: Largest EU economy, high data privacy standards (GDPR leader)\n- **France**: Second largest EU market, localization critical\n- **Nordics**: High tech adoption, English proficiency, smaller markets\n\n**Segmentation Priority Matrix**:\n```\nSegment: US Mid-Market SaaS Companies (200-2000 employees)\nPriority: 1 (Highest)\nRationale:\n  - Largest TAM ($5B)\n  - Fastest sales cycle (60 days avg)\n  - Highest win rate (35%)\n  - Strong product fit (use cases align)\n  - Existing customer base (50% of customers)\nBudget Allocation: 50% of marketing spend\n```\n\n---\n\n## 2. Positioning & Messaging\n\n### 2.1 Positioning Framework (April Dunford Method)\n\n**Step 1: List Your True Competitive Alternatives**\n\nNot just direct competitors - what would customers do if your product didn't exist?\n\n```\nAlternatives:\n1. Competitor A (direct)\n2. Competitor B (direct)\n3. Spreadsheets + email (status quo)\n4. Build in-house (DIY)\n5. Do nothing (ignore problem)\n```\n\n**Step 2: Isolate Your Unique Attributes**\n\nWhat do you have that alternatives don't?\n\n```\nUnique Attributes:\n1. [Feature X that no one else has]\n2. [Integration Y that's exclusive]\n3. [Approach Z that's differentiated]\n4. [Performance metric better than all]\n```\n\n**Step 3: Map Attributes to Value**\n\nWhat value do these attributes provide to customers?\n\n```\nAttribute: [Real-time collaboration]\nâ†’ Value: Teams can work together simultaneously\nâ†’ Outcome: 50% faster project completion\n\nAttribute: [AI-powered automation]\nâ†’ Value: Eliminates manual data entry\nâ†’ Outcome: Save 10 hours/week per user\n```\n\n**Step 4: Define Your Best-Fit Customers**\n\nWho cares most about this value?\n\n```\nBest-Fit: Mid-market SaaS companies (200-1000 employees)\nWhy: They have distributed teams, need real-time collaboration\nEvidence: Fastest sales cycles, lowest churn, highest NPS\n```\n\n**Step 5: Nail Your Market Category**\n\nWhat market do you dominate?\n\n```\nOptions:\n- Head-to-head: Compete in existing category (e.g., \"CRM\")\n- Big fish, small pond: Own a niche (e.g., \"CRM for agencies\")\n- Create new: Define new category (risky, expensive)\n\nDecision: [Choose based on competitive strength and budget]\n```\n\n**Step 6: Layer on Trends**\n\nWhat trends make this the right time to buy?\n\n```\nTrends:\n- Remote work explosion (2020-2025)\n- AI/ML adoption in enterprise (2024-2025)\n- Data privacy regulations (GDPR, CCPA)\n```\n\n### 2.2 Messaging Architecture\n\n**Value Proposition (One-Liner)**:\n\nTemplate: `[Product] helps [Target Customer] [Achieve Goal] by [Unique Approach]`\n\nExample: \"Acme helps mid-market SaaS teams ship 2x faster by automating project workflows with AI\"\n\n**Messaging Hierarchy**:\n\n```\nLEVEL 1: Value Proposition (one-liner)\n[Your one-liner here]\n\nLEVEL 2: Key Benefits (3-5 bullet points)\n- Benefit 1: [Speed] â†’ Ship products 2x faster\n- Benefit 2: [Quality] â†’ Reduce bugs by 50%\n- Benefit 3: [Collaboration] â†’ Align teams in real-time\n- Benefit 4: [Cost] â†’ Save $100k/year on tools\n\nLEVEL 3: Features (supporting evidence)\n- Feature â†’ Benefit â†’ Outcome\n- AI automation â†’ Eliminates manual work â†’ Save 10 hrs/week\n- Real-time sync â†’ No version conflicts â†’ 50% fewer errors\n- Integrations â†’ Connect existing tools â†’ 80% faster onboarding\n\nLEVEL 4: Proof Points\n- Customer logos: [Microsoft, Shopify, Stripe]\n- Stats: Used by 10,000+ teams, 4.8/5 G2 rating\n- Case studies: How [Customer] achieved [Outcome]\n```\n\n**Messaging by Persona**:\n\n**Economic Buyer** (VP/Director):\n- Primary concern: ROI, business outcomes\n- Tone: Professional, data-driven, results-focused\n- Key message: \"Increase revenue by 25% while reducing costs by $200k/year\"\n- Proof: ROI calculator, case studies with $ impact\n\n**Technical Buyer** (Engineer/Architect):\n- Primary concern: Technical fit, security, scalability\n- Tone: Technical, detailed, objective\n- Key message: \"Enterprise-grade architecture with 99.99% uptime and SOC 2 compliance\"\n- Proof: Technical docs, security whitepaper, architecture diagram\n\n**End User** (Manager/Individual Contributor):\n- Primary concern: Ease of use, daily workflow\n- Tone: Friendly, empathetic, practical\n- Key message: \"Spend less time on busywork, more time on what matters\"\n- Proof: Product demo, free trial, customer testimonials\n\n### 2.3 Messaging Testing & Iteration\n\n**Message Testing Framework**:\n\n1. **Qualitative** (customer interviews):\n   - Ask 10-15 target customers:\n   - \"How would you describe [Product] to a colleague?\"\n   - \"What's the main benefit you get from [Product]?\"\n   - \"Why did you choose us over [Competitor]?\"\n\n2. **Quantitative** (A/B testing):\n   - Test messaging variations on:\n   - Landing page headlines\n   - Ad copy (LinkedIn, Google)\n   - Email subject lines\n   - Measure: CTR, conversion rate, demo requests\n\n3. **Sales Feedback** (win/loss analysis):\n   - Ask sales team monthly:\n   - \"Which message resonates most with prospects?\"\n   - \"What objections are we hearing?\"\n   - \"How do we compare to [Competitor] in customer's eyes?\"\n\n**Iteration Cycle**:\n- Test new messaging: 2-4 weeks\n- Analyze results: 1 week\n- Update messaging docs: 1 week\n- Train sales team: 1 week\n- Repeat quarterly\n\n---\n\n## 3. Competitive Intelligence\n\n### 3.1 Competitive Analysis Framework\n\n**Tier 1: Direct Competitors** (head-to-head, same category)\n- [Competitor A]: Market leader, $100M+ ARR\n- [Competitor B]: Fast-growing challenger, Series B\n- [Competitor C]: Open-source alternative\n\n**Tier 2: Indirect Competitors** (adjacent solutions)\n- [Alt Solution D]: Different approach, overlapping use case\n- [Alt Solution E]: Broader platform, includes your feature\n\n**Tier 3: Status Quo** (what customers do today)\n- Spreadsheets + email\n- Build in-house\n- Do nothing\n\n**Competitive Intelligence Sources**:\n1. **Product trials**: Sign up for competitor products, use actively\n2. **Website monitoring**: Track changes to pricing, messaging, features\n3. **Customer interviews**: Ask \"What alternatives did you consider?\"\n4. **Sales call recordings** (Gong/Chorus): Listen for competitor mentions\n5. **Review sites** (G2, Capterra): Read competitor reviews (pros/cons)\n6. **Job postings**: Competitor hiring = roadmap insights\n7. **Financial filings** (if public): Revenue, growth, strategy\n8. **Social media**: Follow competitor execs, product teams\n9. **Partner channels**: Talk to shared implementation partners\n10. **Industry reports**: Gartner, Forrester, IDC\n\n### 3.2 Competitive Battlecards\n\n**Battlecard Template** (create one per competitor):\n\n```\nCOMPETITOR: [Competitor A]\n\nOVERVIEW:\n- Founded: 2015\n- Funding: Series C, $75M raised\n- HQ: San Francisco\n- Size: 200 employees\n- Customers: 5,000+ companies\n- Pricing: $50-$500/user/month\n\nPOSITIONING:\n- They say: \"All-in-one platform for modern teams\"\n- Reality: Broad but shallow, not deep in any use case\n\nKEY STRENGTHS (What They Do Well):\n1. Strong brand recognition (category leader)\n2. Large feature set (breadth over depth)\n3. Extensive integrations (2,000+ apps)\n\nKEY WEAKNESSES (Where They Fall Short):\n1. Complex UI (steep learning curve)\n2. Expensive (2x our price at scale)\n3. Poor support (low NPS in reviews)\n4. Legacy architecture (slow performance)\n\nOUR ADVANTAGES:\n1. 10x easier to use (time-to-value in minutes vs. days)\n2. 50% lower cost at 100+ users\n3. Superior performance (2x faster load times)\n4. White-glove onboarding (dedicated CSM)\n\nWHEN TO WIN:\n- Customer values ease of use over features\n- Budget-conscious (not enterprise)\n- Need fast time-to-value (<1 week)\n- Poor experience with competitor (switching)\n\nWHEN TO LOSE:\n- Enterprise (>5000 employees) with complex requirements\n- Need feature X that we don't have yet\n- Deep integration with competitor's ecosystem\n- Already invested heavily in competitor (sunk cost)\n\nTALK TRACKS:\n\nObjection: \"We're already using [Competitor A]\"\nResponse: \"That's great - many of our customers came from [Competitor A]. What prompted you to explore alternatives? [Listen for pain points] Typically teams switch to us because [ease of use / cost / performance]. Would it be helpful to see a side-by-side comparison?\"\n\nObjection: \"[Competitor A] has more features\"\nResponse: \"You're right - they've been around longer and have a broader feature set. Here's what we found: most teams only use 20% of those features. Our customers love that we focus on doing [core use case] exceptionally well rather than trying to do everything. What features are most critical for your team?\"\n\nPROOF POINTS:\n- Case study: \"[Customer] switched from [Competitor A], reduced costs by 60%\"\n- Review comparison: \"[4.8 vs. 4.2 G2 rating in 'Ease of Use']\"\n- Win rate: \"35% win rate in competitive deals\"\n\nCOMPETITIVE LANDSCAPE:\n[Link to competitive positioning map]\n[Link to feature comparison matrix]\n```\n\n**Battlecard Distribution**:\n- Store in: Notion, Confluence, or sales enablement platform\n- Update frequency: Monthly (or when competitor launches major feature)\n- Access: Sales, CS, Product, Marketing teams\n- Training: Monthly competitive update calls with sales\n\n### 3.3 Win/Loss Analysis\n\n**Win/Loss Interview Process**:\n\n**Goals**:\n- Understand why you won/lost\n- Validate positioning and messaging\n- Identify product gaps\n- Track competitive trends\n\n**Process**:\n1. **Identify deals** (closed won or lost in last 30 days)\n2. **Request interview** (email or HubSpot workflow)\n3. **Conduct interview** (30-45 min, record with permission)\n4. **Analyze data** (themes, patterns, trends)\n5. **Share insights** (monthly report to product, sales, marketing)\n\n**Interview Questions** (pick 8-10):\n\n**For Wins**:\n- What problem were you trying to solve?\n- What alternatives did you evaluate?\n- Why did you choose us over [Competitor]?\n- What almost made you choose someone else?\n- What could we improve?\n\n**For Losses**:\n- What problem were you trying to solve?\n- Who did you choose instead? Why?\n- What did we do well in the sales process?\n- What could we have done differently?\n- Would you consider us in the future? When?\n\n**Data Tracking** (in HubSpot or spreadsheet):\n\n| Deal | Outcome | Reason | Competitor | Price Factor | Product Gap | Messaging Issue |\n|------|---------|--------|------------|--------------|-------------|-----------------|\n| Acme Corp | Won | Best product fit | Competitor A | No | No | No |\n| Beta Inc | Lost | Price | Competitor B | Yes | No | No |\n| Gamma LLC | Lost | Missing feature X | Built in-house | No | Yes | No |\n\n**Monthly Insights Report**:\n```\nWin/Loss Summary (March 2025):\n- Total deals analyzed: 20 (12 wins, 8 losses)\n- Win rate: 60%\n- Top win reasons:\n  1. Ease of use (8 mentions)\n  2. Better support (6 mentions)\n  3. Price (4 mentions)\n- Top loss reasons:\n  1. Missing feature X (4 mentions)\n  2. Price (3 mentions)\n  3. Competitor relationship (2 mentions)\n\nAction Items:\n- Product: Prioritize feature X (lost 4 deals)\n- Sales: Update battlecard for Competitor A (won 5 competitive deals)\n- Marketing: Create case study on \"ease of use\" theme\n```\n\n---\n\n## 4. Go-To-Market (GTM) Strategy\n\n### 4.1 GTM Motion Types\n\n**PLG (Product-Led Growth)**:\n- Entry: Free trial or freemium\n- Buyer: End user â†’ Manager â†’ VP\n- Sales: Low touch or self-serve\n- ACV: <$10k\n- Example: Slack, Notion, Figma\n\n**Sales-Led Growth**:\n- Entry: Demo request â†’ Sales qualification\n- Buyer: VP â†’ C-level\n- Sales: High touch, consultative\n- ACV: $25k+\n- Example: Salesforce, Workday, SAP\n\n**Hybrid (PLG + Sales)**:\n- Entry: Free trial for SMB, demo for Enterprise\n- Buyer: End user (PLG) or Executive (Sales-Led)\n- Sales: Self-serve â†’ Assisted â†’ Enterprise\n- ACV: $5k-$100k\n- Example: HubSpot, Atlassian, Zoom\n\n**Series A Recommendation**: Start with **Hybrid**\n- Reason: Faster learning, broader TAM, efficient scaling\n- Approach:\n  - Bottom-up (PLG): Free trial â†’ Paid team plan â†’ Upgrade to Enterprise\n  - Top-down (Sales): Outbound to Enterprise â†’ Demo â†’ POC â†’ Close\n\n### 4.2 GTM Launch Playbook (90-Day Plan)\n\n**Pre-Launch (Days -90 to -30)**:\n\nWeek 1-4: Foundation\n- [ ] Define ICP and buyer personas\n- [ ] Develop positioning and messaging\n- [ ] Create competitive battlecards\n- [ ] Set success metrics (pipeline $, MQLs, win rate)\n\nWeek 5-8: Content & Enablement\n- [ ] Build website pages (homepage, product, pricing)\n- [ ] Create sales deck and demo script\n- [ ] Produce launch assets (one-pager, case studies, FAQs)\n- [ ] Develop email nurture sequences\n- [ ] Train sales team on positioning and talk tracks\n\nWeek 9-12: Channel Setup\n- [ ] Launch paid campaigns (LinkedIn, Google)\n- [ ] Set up HubSpot tracking and attribution\n- [ ] Publish SEO content (blog posts, guides)\n- [ ] Activate partnerships (co-marketing plans)\n- [ ] Test conversion funnels (landing page â†’ signup)\n\n**Launch (Days 1-30)**:\n\nWeek 1: Awareness\n- [ ] Press release distribution\n- [ ] Email announcement to existing database\n- [ ] Social media campaign (LinkedIn, Twitter)\n- [ ] Paid ads go live (awareness campaigns)\n- [ ] Outbound sales blitz (top 100 accounts)\n\nWeek 2-4: Activation\n- [ ] Monitor conversion rates (daily)\n- [ ] A/B test landing pages and ad copy\n- [ ] Sales follow-up on inbound leads (<4 hour SLA)\n- [ ] Customer interviews (feedback on positioning)\n- [ ] Adjust messaging based on early signals\n\n**Post-Launch (Days 31-90)**:\n\nWeek 5-8: Optimization\n- [ ] Analyze win/loss data (why did we win/lose?)\n- [ ] Optimize underperforming channels (pause or pivot)\n- [ ] Scale winning channels (20% weekly budget increase)\n- [ ] Publish post-launch case studies\n- [ ] Expand content (SEO, demand gen)\n\nWeek 9-12: Scale\n- [ ] Enter new market segments (vertical or geo)\n- [ ] Launch partnerships (co-marketing campaigns)\n- [ ] Build PLG loops (referral program, viral features)\n- [ ] Sales team expansion (hire based on pipeline)\n- [ ] Iterate positioning (quarterly messaging refresh)\n\n### 4.3 International Market Entry (EU/US/Canada)\n\n**Market Entry Priority** (Series A recommended order):\n\n**Phase 1: US Market** (Months 1-6)\n- Why: Largest TAM, fastest sales cycles, highest ACV\n- Entry strategy:\n  - Hire US-based SDRs/AEs (or partner with US sales agency)\n  - Localize website (USD pricing, US phone number)\n  - Paid ads (Google + LinkedIn) targeting US companies\n  - Partnerships with US-based tech companies\n- Budget: 50% of total marketing spend\n- Target: $1M ARR from US by Month 6\n\n**Phase 2: UK Market** (Months 4-9)\n- Why: English-speaking, gateway to EU, similar to US\n- Entry strategy:\n  - Hire UK sales rep or partner with UK agency\n  - Localize pricing (GBP), GDPR compliance\n  - Content localization (British spelling, cultural nuances)\n  - UK partnerships (local SaaS companies)\n- Budget: 20% of marketing spend\n- Target: $500k ARR from UK by Month 9\n\n**Phase 3: DACH (Germany/Austria/Switzerland)** (Months 7-12)\n- Why: Largest EU economy, high data privacy standards\n- Entry strategy:\n  - Translate website and product (German)\n  - Hire German-speaking sales rep\n  - GDPR compliance (critical for German market)\n  - Partnerships with German tech companies\n  - Local case studies and testimonials\n- Budget: 15% of marketing spend\n- Target: $300k ARR from DACH by Month 12\n\n**Phase 4: France** (Months 10-15)\n- Why: Second largest EU market, localization critical\n- Entry strategy:\n  - Full French translation (website, product, support)\n  - Hire French-speaking sales and support\n  - French partnerships and case studies\n  - Comply with French data regulations\n- Budget: 10% of marketing spend\n- Target: $200k ARR from France by Month 15\n\n**Phase 5: Canada** (Months 7-12)\n- Why: Similar to US, easier entry, smaller market\n- Entry strategy:\n  - Minimal localization (CAD pricing)\n  - Leverage US sales team (similar buying behavior)\n  - Canadian partnerships\n- Budget: 5% of marketing spend\n- Target: $100k ARR from Canada by Month 12\n\n**Localization Checklist (per market)**:\n\n- [ ] **Website**: Translate, localize currency, phone number\n- [ ] **Product**: UI translation (if needed for that market)\n- [ ] **Pricing**: Local currency, VAT/taxes displayed\n- [ ] **Support**: Local business hours, language support\n- [ ] **Legal**: Data privacy compliance (GDPR, CCPA)\n- [ ] **Sales**: Hire local reps or partner with local agency\n- [ ] **Marketing**: Localized ads, content, case studies\n- [ ] **Payments**: Local payment methods (SEPA, iDEAL, etc.)\n\n**Budget Allocation** (international expansion):\n```\nYear 1 (Series A):\n- US: 50% ($200k)\n- UK: 20% ($80k)\n- DACH: 15% ($60k)\n- France: 10% ($40k)\n- Canada: 5% ($20k)\n\nTotal: $400k marketing spend (international)\nExpected ROI: 3:1 (marketing-sourced pipeline : spend)\n```\n\n---\n\n## 5. Product Launch Framework\n\n### 5.1 Launch Tiers (Effort vs. Impact)\n\n**Tier 1: Major Launch** (quarterly, high impact)\n- Scope: New product, major feature, platform expansion\n- Audience: Existing customers + new prospects + press\n- Effort: 6-8 weeks prep, full cross-functional launch\n- Budget: $50k-$100k (Series A)\n- Activities: Press release, webinar, email series, paid ads, sales blitz\n\n**Tier 2: Standard Launch** (monthly, medium impact)\n- Scope: Significant feature, integration, improvement\n- Audience: Existing customers + select prospects\n- Effort: 3-4 weeks prep, core team involvement\n- Budget: $10k-$25k\n- Activities: Blog post, email announcement, product update, sales enablement\n\n**Tier 3: Minor Launch** (weekly, low impact)\n- Scope: Small feature, bug fix, optimization\n- Audience: Existing customers only\n- Effort: 1 week prep, product + marketing only\n- Budget: <$5k\n- Activities: In-app notification, changelog, support docs\n\n### 5.2 Major Launch Playbook (Tier 1)\n\n**8 Weeks Before Launch**:\n\nWeek -8:\n- [ ] Kickoff meeting (Product, Marketing, Sales, CS)\n- [ ] Define launch goals (pipeline $, MQLs, press coverage)\n- [ ] Identify target audience (ICP, personas)\n- [ ] Create positioning and messaging\n- [ ] Assign roles and responsibilities\n\nWeek -7:\n- [ ] Develop GTM strategy (channels, tactics, budget)\n- [ ] Create sales enablement (deck, demo script, FAQs)\n- [ ] Plan content (blog posts, case studies, videos)\n- [ ] Design creative assets (ads, social graphics, emails)\n\nWeek -6:\n- [ ] Build landing pages (product page, demo request)\n- [ ] Set up HubSpot campaigns and tracking\n- [ ] Write press release and pitch media\n- [ ] Create email nurture sequences\n- [ ] Produce demo video\n\nWeek -5:\n- [ ] Beta test with select customers (feedback)\n- [ ] Train sales team (positioning, demo, objection handling)\n- [ ] Train CS team (onboarding, support docs)\n- [ ] Finalize launch timeline and channel mix\n- [ ] Prepare customer case studies\n\n**4 Weeks Before Launch**:\n\nWeek -4:\n- [ ] Launch paid ad campaigns (LinkedIn, Google)\n- [ ] Publish teaser content (blog, social)\n- [ ] Send pre-launch email to customer base\n- [ ] Pitch press and influencers\n- [ ] Set up webinar registration\n\nWeek -3:\n- [ ] A/B test landing pages and ad copy\n- [ ] Ramp up content production (blog posts, videos)\n- [ ] Sales prospecting (outbound to target accounts)\n- [ ] Finalize webinar content and speakers\n- [ ] Prepare launch day checklist\n\nWeek -2:\n- [ ] Send reminder emails (webinar, launch countdown)\n- [ ] Increase paid ad spend (ramp up)\n- [ ] Sales follow-up on warmed leads\n- [ ] Dry run: Test all systems (website, forms, CRM)\n- [ ] Prepare launch day assets (social posts, emails)\n\nWeek -1:\n- [ ] Final review: All assets approved\n- [ ] Pre-launch email to VIP customers and partners\n- [ ] Sales team ready (trained, motivated, quotas set)\n- [ ] CS team ready (docs updated, chat support staffed)\n- [ ] Press embargo lifts (if applicable)\n\n**Launch Week**:\n\nDay 1 (Launch Day):\n- [ ] Press release goes live (distribute to media)\n- [ ] Email announcement to full database\n- [ ] Social media blitz (LinkedIn, Twitter, Facebook)\n- [ ] Paid ads at full budget\n- [ ] Sales outbound campaign (top 500 accounts)\n- [ ] Product update in-app (notify existing users)\n- [ ] Monitor metrics (signups, demos, press pickup)\n\nDays 2-5:\n- [ ] Daily monitoring (conversion rates, funnel drop-offs)\n- [ ] A/B test optimizations (headlines, CTAs)\n- [ ] Sales follow-up (4-hour SLA on inbound leads)\n- [ ] Respond to press inquiries\n- [ ] Post customer testimonials and early wins\n- [ ] Webinar (Day 3 or 4)\n\nWeek 2:\n- [ ] Analyze launch results (vs. goals)\n- [ ] Publish post-launch content (case studies, how-to guides)\n- [ ] Sales continue outbound (sustained momentum)\n- [ ] Optimize underperforming channels\n- [ ] Scale winning channels (increase budget)\n\nWeek 3-4:\n- [ ] Post-launch report (metrics, learnings, next steps)\n- [ ] Customer feedback interviews (product improvements)\n- [ ] Win/loss analysis (why did we win/lose deals?)\n- [ ] Adjust messaging and positioning (based on feedback)\n- [ ] Plan next launch (apply learnings)\n\n### 5.3 Launch Metrics Dashboard\n\n**Leading Indicators** (track daily):\n- Landing page visitors\n- Demo requests\n- Free trial signups\n- MQLs generated\n- Sales pipeline created ($)\n\n**Lagging Indicators** (track weekly/monthly):\n- SQLs generated\n- Deals closed (count + $)\n- Win rate (vs. pre-launch)\n- Customer adoption rate (% of customers using feature)\n- NPS score (feature-specific)\n\n**HubSpot Dashboard**:\n```\nLaunch Campaign: [Q2-2025-Product-X-Launch]\n\nWEEK 1 RESULTS:\nTraffic: 10,000 visitors (goal: 8,000) âœ…\nMQLs: 250 (goal: 200) âœ…\nSQLs: 40 (goal: 50) âš ï¸\nPipeline: $800k (goal: $1M) âš ï¸\nDemos: 80 (goal: 100) âš ï¸\n\nTOP CHANNELS:\n1. LinkedIn Ads: 120 MQLs, $150 CPL\n2. Email: 80 MQLs, $25 CPL\n3. Organic: 40 MQLs, $0 CPL\n\nUNDERPERFORMING:\n- Google Search: 10 MQLs, $400 CPL (pause and optimize)\n- Webinar: 50 registrants, 20% show rate (improve email reminders)\n\nNEXT ACTIONS:\n- Increase LinkedIn Ads budget by 30%\n- A/B test new landing page headline\n- Sales follow-up blitz on 40 SQLs\n```\n\n---\n\n## 6. Sales Enablement & Collaboration\n\n### 6.1 Sales Enablement Assets (Must-Have)\n\n**Core Assets**:\n\n**1. Sales Deck** (15-20 slides)\n```\nSlide 1: Title slide (logo, tagline)\nSlide 2: Agenda\nSlide 3: Company intro (mission, vision, traction)\nSlide 4: Problem statement (customer pain points)\nSlide 5: Solution overview (your product)\nSlide 6: Key benefits (3-5 bullets)\nSlide 7: Product demo (screenshots or video)\nSlide 8: Differentiation (vs. competitors)\nSlide 9: Customer logos (social proof)\nSlide 10: Case study (results-focused)\nSlide 11: Pricing and plans\nSlide 12: Implementation timeline\nSlide 13: Support and success\nSlide 14: Next steps (CTA)\nSlide 15: Q&A\n\nGuidelines:\n- Visual-first (minimal text, large images)\n- Customer-centric (benefits > features)\n- Modular (easy to skip/reorder slides)\n- Updated quarterly (or after major product changes)\n```\n\n**2. One-Pagers** (1-page PDF)\n- Product overview (what it is, who it's for, key features)\n- Competitive comparison (vs. Competitor A, B, C)\n- Case study (customer story with metrics)\n- Pricing sheet (plans, features, add-ons)\n\n**3. Battlecards** (per competitor)\n- See Section 3.2 for detailed battlecard template\n\n**4. Demo Script** (30-45 min)\n```\nDemo Flow:\n1. Intro (2 min) - Who we are, what we'll cover\n2. Discovery (5 min) - Ask about their needs, pain points\n3. Demo (20 min) - Show product (focus on their use case)\n4. Q&A (10 min) - Address objections, questions\n5. Next steps (3 min) - Define trial or POC plan\n\nDemo Tips:\n- Show, don't tell (product in action > slides)\n- Use customer data (not \"Company XYZ\" examples)\n- Focus on outcomes (not features)\n- Address objections proactively (price, competition)\n- Always drive to next step (trial, POC, proposal)\n```\n\n**5. Email Templates** (HubSpot sequences)\n- Cold outreach (prospecting)\n- Demo follow-up\n- Trial conversion\n- Proposal sent\n- Closing sequence\n\n**6. ROI Calculator** (spreadsheet or web tool)\n- Input: Customer's current costs, time spent, team size\n- Output: Savings with your product, payback period, 3-year ROI\n- Example: \"Save $150k/year, 6-month payback, 500% ROI\"\n\n### 6.2 Sales Training Program\n\n**Monthly Sales Enablement Call** (60 min):\n- Product updates (new features, roadmap)\n- Competitive landscape (new competitors, battlecard updates)\n- Win/loss insights (why we're winning/losing)\n- Best practices (top performer shares tips)\n- Q&A (open forum for questions)\n\n**Quarterly Sales Training** (half-day workshop):\n- Deep dive: Positioning and messaging refresh\n- Role-playing: Objection handling, competitive demos\n- Product training: New features, advanced use cases\n- Customer panel: Hear directly from customers (why they bought)\n\n**Sales Onboarding** (new hires):\n- Week 1: Company, product, market overview\n- Week 2: ICP, personas, messaging\n- Week 3: Competitive intelligence, battlecards\n- Week 4: Demo certification (must pass to sell)\n\n### 6.3 Marketing â†” Sales Handoffs\n\n**MQL â†’ SQL Handoff** (see marketing-demand-acquisition skill for details)\n\n**Product Marketing â†’ Sales**:\n\n**Weekly Sync** (30 min):\n- Review: Win/loss insights, competitive updates\n- Share: New assets (battlecards, case studies, one-pagers)\n- Feedback: What's working, what's not\n- Request: Sales asks for specific assets (e.g., \"Need competitor X battlecard\")\n\n**Quarterly Business Review** (QBR):\n- Results: Pipeline, win rate, deal size, sales velocity\n- Insights: Top win/loss reasons, competitive trends\n- Action items: Product gaps, messaging updates, enablement needs\n\n**Communication Channels**:\n- Slack: #sales-enablement (daily questions, quick updates)\n- HubSpot: Centralized asset library (decks, one-pagers, videos)\n- Notion: Internal wiki (positioning, messaging, competitive intel)\n\n---\n\n## 7. Metrics & Analytics\n\n### 7.1 PMM KPIs (Track Monthly)\n\n**Product Adoption**:\n- % of customers using new feature (within 30 days of launch)\n- Target: >40% adoption within 90 days\n\n**Sales Velocity**:\n- Days from SQL to closed won\n- Target: Decrease by 20% YoY\n\n**Win Rate**:\n- % of opportunities won (vs. competitors)\n- Target: >30% win rate (competitive deals)\n\n**Deal Size**:\n- Average contract value (ACV)\n- Target: Increase by 25% YoY\n\n**Launch Impact**:\n- Pipeline $ generated from launch campaigns\n- Target: 3:1 ROMI (pipeline $ : marketing spend)\n\n**Competitive Win Rate**:\n- % of deals won against Competitor A, B, C\n- Target: >35% win rate vs. top competitor\n\n### 7.2 HubSpot Reporting\n\n**Custom Reports**:\n\n**1. Product Launch Impact**\n```\nMetrics: Leads, MQLs, SQLs, Pipeline $, Closed Won $\nDimensions: Campaign, Channel, Region\nFilters: Campaign = \"Q2-2025-Product-X-Launch\"\nTime period: 90 days post-launch\n```\n\n**2. Competitive Win Rate**\n```\nMetrics: Opportunities, Closed Won, Win Rate %\nDimensions: Competitor (property)\nFilters: Deal stage = Closed Won or Closed Lost\nSegment by: Competitor A, B, C, Other\n```\n\n**3. Sales Enablement Usage**\n```\nMetrics: Asset downloads, views, shares\nDimensions: Asset type (deck, battlecard, case study)\nFilters: User = Sales team\nInsight: Which assets are most used by sales\n```\n\n### 7.3 Quarterly Business Review (QBR)\n\n**QBR Template** (present to executive team):\n\n**Slide 1: Executive Summary**\n```\nQ2 2025 Highlights:\n- Launched Product X (pipeline: $2M, 500 MQLs)\n- Entered UK market (20 new customers, $400k ARR)\n- Improved win rate by 15% (competitive positioning)\n- Published 3 case studies (2x sales usage vs. Q1)\n```\n\n**Slide 2: Metrics Dashboard**\n```\nKPI             Q2 Target   Q2 Actual   Status\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nMQLs            800         950         âœ… +19%\nSQLs            150         140         âš ï¸ -7%\nPipeline $      $4M         $3.8M       âš ï¸ -5%\nWin Rate        30%         35%         âœ… +17%\nDeal Size       $45k        $52k        âœ… +16%\nSales Velocity  75 days     68 days     âœ… -9%\n```\n\n**Slide 3: Key Insights**\n```\nWhat Worked:\n1. Product X launch exceeded MQL target by 19%\n2. Improved competitive positioning â†’ 35% win rate\n3. UK market entry on track ($400k ARR in 3 months)\n\nWhat Didn't Work:\n1. SQL conversion rate dropped from 20% to 15%\n2. Google Ads underperformed (paused and optimizing)\n3. Competitor A launched aggressive pricing (5 lost deals)\n\nAction Items:\n1. Improve SQL qualification criteria (work with sales)\n2. Update battlecard for Competitor A (new pricing)\n3. Double down on UK market (hire local AE)\n```\n\n**Slide 4: Next Quarter Plan**\n```\nQ3 2025 Priorities:\n1. Launch Product Y (pipeline target: $3M)\n2. Enter DACH market (Germany, Austria, Switzerland)\n3. Refresh messaging and website (new positioning)\n4. Scale partnerships (3 new strategic partners)\n5. Build customer advocacy program (10 case studies)\n\nBudget: $150k (up from $120k in Q2)\nHeadcount: +1 PMM, +1 Content Marketer\n```\n\n---\n\n## 8. Quick Reference\n\n### 8.1 PMM Monthly Checklist\n\n**Week 1** (Strategy & Planning):\n- [ ] Review previous month metrics (win rate, deal size, pipeline)\n- [ ] Analyze win/loss interviews (competitive trends)\n- [ ] Update competitive battlecards (if needed)\n- [ ] Plan next month campaigns and content\n\n**Week 2** (Content & Enablement):\n- [ ] Create new sales assets (1-pager, case study, deck update)\n- [ ] Publish content (blog post, video, webinar)\n- [ ] Train sales on new positioning or product updates\n- [ ] Review sales asset usage (what's working?)\n\n**Week 3** (Launches & Campaigns):\n- [ ] Support product launches (if any)\n- [ ] Monitor campaign performance (MQLs, SQLs, pipeline)\n- [ ] Optimize underperforming channels\n- [ ] Customer interviews (feedback on positioning)\n\n**Week 4** (Reporting & Iteration):\n- [ ] Monthly metrics report (for exec team)\n- [ ] Sales enablement call (updates, Q&A)\n- [ ] Win/loss analysis (themes, trends)\n- [ ] Plan next quarter launches and strategy\n\n### 8.2 Positioning Development Timeline\n\n**Week 1**: Research\n- Customer interviews (10-15)\n- Competitive analysis\n- Market trends\n\n**Week 2**: Framework\n- April Dunford positioning exercise\n- Define unique value\n- Identify best-fit customers\n\n**Week 3**: Messaging\n- Craft value proposition\n- Build messaging hierarchy\n- Create persona-specific messaging\n\n**Week 4**: Validation\n- Test with sales team\n- A/B test on landing pages\n- Customer feedback\n\n**Week 5-6**: Rollout\n- Update website, sales decks\n- Train sales and CS teams\n- Launch campaigns with new messaging\n\n### 8.3 Team Handoff Protocols\n\n**PMM â†’ Demand Gen**:\n- Deliver: Positioning, messaging, competitive intel, launch plans\n- Frequency: Monthly sync + ad-hoc for launches\n- SLA: 2-week lead time for major campaigns\n\n**PMM â†’ Sales**:\n- Deliver: Battlecards, sales decks, demo scripts, objection handling\n- Frequency: Monthly enablement call + weekly Slack updates\n- SLA: 48 hours for urgent competitive questions\n\n**PMM â†’ Product**:\n- Deliver: Customer feedback, competitive feature gaps, win/loss insights\n- Frequency: Weekly product sync\n- SLA: Quarterly roadmap input (feature prioritization)\n\n**PMM â†’ Customer Success**:\n- Deliver: Product positioning, adoption tactics, customer education content\n- Frequency: Monthly sync\n- SLA: 1 week for new product launch enablement\n\n---\n\n## Resources\n\n### references/\n\n- **positioning-frameworks.md** - Detailed guide on April Dunford, Geoffrey Moore positioning methods\n- **launch-checklists.md** - Tier 1/2/3 launch checklists and templates\n- **international-gtm.md** - Market-by-market expansion playbooks (US, UK, DACH, France, Canada)\n- **messaging-templates.md** - Ready-to-use messaging frameworks for different personas\n\n### scripts/\n\n- **competitor_tracker.py** - Track competitor website/pricing changes\n- **win_loss_analyzer.py** - Analyze win/loss interview data for trends\n\n### assets/\n\n- **sales-deck-template.pptx** - Editable master sales deck\n- **battlecard-template.docx** - Competitive battlecard template\n- **one-pager-template.pptx** - Product one-pager design template\n- **roi-calculator.xlsx** - ROI calculator spreadsheet\n\n---\n\n**Last Updated**: October 2025 | **Version**: 1.0\n",
      "frontmatter": {
        "name": "marketing-strategy-pmm",
        "description": "Product marketing, positioning, GTM strategy, and competitive intelligence. Includes ICP definition, April Dunford positioning methodology, launch playbooks, competitive battlecards, and international market entry guides. Use when developing positioning, planning product launches, creating messaging, analyzing competitors, entering new markets, enabling sales, or when user mentions product marketing, positioning, GTM, go-to-market, competitive analysis, market entry, or sales enablement.",
        "license": "MIT",
        "metadata": {
          "version": "1.0.0",
          "author": "Alireza Rezvani",
          "category": "marketing",
          "domain": "product-marketing",
          "updated": "2025-10-20T00:00:00.000Z",
          "frameworks": "April-Dunford-positioning, ICP-definition, messaging-hierarchy",
          "target-market": "B2B-SaaS, international-expansion, Series-A+"
        }
      },
      "content": "\n# Marketing Strategy & Product Marketing\n\nExpert Product Marketing playbook for Series A+ startups expanding internationally with hybrid PLG/Sales-Led motion.\n\n## Keywords\nproduct marketing, positioning, GTM, go-to-market strategy, competitive analysis, competitive intelligence, battlecards, ICP, ideal customer profile, messaging, value proposition, product launch, market entry, international expansion, sales enablement, win loss analysis, PMM, product marketing manager, market positioning, competitive landscape, sales training\n\n## Role Coverage\n\nThis skill serves:\n- **Product Marketing Manager (PMM)** - Positioning, messaging, competitive intel, launches\n- **Head of Marketing** - Strategy, budget, org design, pipeline targets\n- **Head of Growth** - Experimentation, activation, retention, growth loops\n- **CMO/VP Marketing** - Executive strategy, board reporting, team leadership\n\n## Core KPIs by Role\n\n**PMM**: Product adoption rate, win rate vs. competitors, sales velocity, launch impact metrics, competitive win rate, deal size growth\n\n**Head of Marketing**: Marketing-sourced pipeline $, CAC/LTV ratio, ROMI (3:1+ target), brand awareness lift, market share growth\n\n**Head of Growth**: Activation rate, WAU/MAU, conversion rates across funnel, payback period, viral coefficient (PLG)\n\n**CMO**: Revenue growth %, pipeline coverage (3-4x), team productivity, budget efficiency, NPS/brand health\n\n## Tech Stack Integration\n\n**HubSpot** - CRM, deal tracking, competitive loss analysis, sales enablement content\n**Google Analytics** - Product usage, activation funnels, feature adoption\n**Gong/Chorus** - Sales call analysis, competitive intelligence, objection tracking\n**Productboard** - Feature requests, customer feedback, roadmap prioritization\n**Notion/Confluence** - Internal wiki, positioning docs, competitive battlecards\n\n---\n\n## 1. Strategic Foundation\n\n### 1.1 Company Strategy Framework (Series A Context)\n\n**Current State Analysis**:\n```\nStage: Series A\nFunding: $5-15M raised\nTeam Size: 20-50 people\nRevenue: $1-5M ARR\nMarket Position: Challenger/Niche leader\nGrowth Rate Target: 3-5x YoY\n\nKey Challenges:\n- Prove product-market fit at scale\n- Expand from early adopters â†’ mainstream\n- Enter new markets (EU/US/Canada)\n- Compete against incumbents\n- Build repeatable sales motion\n```\n\n**Strategic Priorities** (in order):\n1. **Nail positioning** - Clear, differentiated value prop\n2. **Scale acquisition** - Repeatable, efficient channels\n3. **Prove retention** - Product stickiness, expansion revenue\n4. **Expand markets** - Geographic + vertical expansion\n5. **Build brand** - Awareness, trust, category leadership\n\n### 1.2 ICP (Ideal Customer Profile) Definition\n\n**B2B SaaS ICP Framework**:\n\n**Firmographics**:\n- Company size: 50-5000 employees (Series A sweet spot)\n- Industry: SaaS, Tech, Professional Services\n- Geography: US, Canada, UK, Germany, France (prioritize by TAM)\n- Revenue: $5M-$500M annual\n- Funding stage: Seed to Growth (avoid pre-product)\n\n**Technographics**:\n- Tech stack: Modern (cloud-first, API-driven)\n- Maturity: Growing fast, willing to adopt new tools\n- Existing tools: [List competitors + complementary products]\n- Integration needs: Must integrate with [Salesforce, Slack, etc.]\n\n**Psychographics**:\n- Pain level: 7-10/10 (acute pain, not nice-to-have)\n- Buyer motivation: Efficiency, cost savings, revenue growth\n- Decision process: 2-6 month sales cycle\n- Risk tolerance: Early majority (not bleeding edge)\n\n**Buyer Personas** (3-5 personas max):\n\n**Primary: Economic Buyer** (signs contract)\n- Title: VP, Director, Head of [Department]\n- Goals: ROI, team productivity, cost reduction\n- Fears: Implementation failure, team resistance, budget waste\n- Messaging: Business outcomes, ROI, case studies\n\n**Secondary: Technical Buyer** (evaluates product)\n- Title: Senior Engineer, Architect, Tech Lead\n- Goals: Solves technical problem, easy integration\n- Fears: Technical debt, vendor lock-in, poor support\n- Messaging: Technical capabilities, architecture, security\n\n**User/Champion** (advocates internally)\n- Title: Manager, Team Lead, Power User\n- Goals: Makes their job easier, team loves it\n- Fears: Learning curve, change management\n- Messaging: UX, ease of use, quick wins\n\n**ICP Validation Checklist**:\n- [ ] 5+ paying customers match this profile\n- [ ] Fastest sales cycles (< median time to close)\n- [ ] Highest LTV (> median customer value)\n- [ ] Lowest churn (< 5% annual)\n- [ ] Strong product engagement (daily/weekly usage)\n- [ ] Referenceable (NPS 9-10, willing to do case studies)\n\n**HubSpot ICP Tracking**:\n- Create \"ICP Fit\" property: A (perfect), B (good), C (okay), D (poor)\n- Score based on firmographics, engagement, product usage\n- Report: Win rate by ICP score, pipeline by ICP score\n- Action: Focus acquisition on ICP A/B, nurture C, disqualify D\n\n### 1.3 Market Segmentation Strategy\n\n**Segmentation Dimensions**:\n\n**By Company Size** (recommend starting with one):\n- **SMB** (10-200 employees) - Self-serve PLG, low touch, $100-$2k ACV\n- **Mid-Market** (200-2000 employees) - Hybrid, inside sales, $2k-$50k ACV\n- **Enterprise** (2000+ employees) - Sales-led, field sales, $50k+ ACV\n\n**By Vertical** (choose 2-3 focus verticals):\n- Horizontal: Broad appeal (e.g., project management for any industry)\n- Vertical: Industry-specific (e.g., healthcare CRM, fintech compliance)\n- Approach: Start horizontal, add verticals as you scale\n\n**By Use Case** (messaging varies):\n- Use Case A: [e.g., Team collaboration]\n- Use Case B: [e.g., Client management]\n- Use Case C: [e.g., Project tracking]\n- Each use case = different landing page, messaging, case studies\n\n**By Geography** (Series A focus):\n- **US/Canada**: Largest TAM, fastest sales cycles, highest willingness to pay\n- **UK**: English-speaking, gateway to EU, similar buying behavior to US\n- **Germany**: Largest EU economy, high data privacy standards (GDPR leader)\n- **France**: Second largest EU market, localization critical\n- **Nordics**: High tech adoption, English proficiency, smaller markets\n\n**Segmentation Priority Matrix**:\n```\nSegment: US Mid-Market SaaS Companies (200-2000 employees)\nPriority: 1 (Highest)\nRationale:\n  - Largest TAM ($5B)\n  - Fastest sales cycle (60 days avg)\n  - Highest win rate (35%)\n  - Strong product fit (use cases align)\n  - Existing customer base (50% of customers)\nBudget Allocation: 50% of marketing spend\n```\n\n---\n\n## 2. Positioning & Messaging\n\n### 2.1 Positioning Framework (April Dunford Method)\n\n**Step 1: List Your True Competitive Alternatives**\n\nNot just direct competitors - what would customers do if your product didn't exist?\n\n```\nAlternatives:\n1. Competitor A (direct)\n2. Competitor B (direct)\n3. Spreadsheets + email (status quo)\n4. Build in-house (DIY)\n5. Do nothing (ignore problem)\n```\n\n**Step 2: Isolate Your Unique Attributes**\n\nWhat do you have that alternatives don't?\n\n```\nUnique Attributes:\n1. [Feature X that no one else has]\n2. [Integration Y that's exclusive]\n3. [Approach Z that's differentiated]\n4. [Performance metric better than all]\n```\n\n**Step 3: Map Attributes to Value**\n\nWhat value do these attributes provide to customers?\n\n```\nAttribute: [Real-time collaboration]\nâ†’ Value: Teams can work together simultaneously\nâ†’ Outcome: 50% faster project completion\n\nAttribute: [AI-powered automation]\nâ†’ Value: Eliminates manual data entry\nâ†’ Outcome: Save 10 hours/week per user\n```\n\n**Step 4: Define Your Best-Fit Customers**\n\nWho cares most about this value?\n\n```\nBest-Fit: Mid-market SaaS companies (200-1000 employees)\nWhy: They have distributed teams, need real-time collaboration\nEvidence: Fastest sales cycles, lowest churn, highest NPS\n```\n\n**Step 5: Nail Your Market Category**\n\nWhat market do you dominate?\n\n```\nOptions:\n- Head-to-head: Compete in existing category (e.g., \"CRM\")\n- Big fish, small pond: Own a niche (e.g., \"CRM for agencies\")\n- Create new: Define new category (risky, expensive)\n\nDecision: [Choose based on competitive strength and budget]\n```\n\n**Step 6: Layer on Trends**\n\nWhat trends make this the right time to buy?\n\n```\nTrends:\n- Remote work explosion (2020-2025)\n- AI/ML adoption in enterprise (2024-2025)\n- Data privacy regulations (GDPR, CCPA)\n```\n\n### 2.2 Messaging Architecture\n\n**Value Proposition (One-Liner)**:\n\nTemplate: `[Product] helps [Target Customer] [Achieve Goal] by [Unique Approach]`\n\nExample: \"Acme helps mid-market SaaS teams ship 2x faster by automating project workflows with AI\"\n\n**Messaging Hierarchy**:\n\n```\nLEVEL 1: Value Proposition (one-liner)\n[Your one-liner here]\n\nLEVEL 2: Key Benefits (3-5 bullet points)\n- Benefit 1: [Speed] â†’ Ship products 2x faster\n- Benefit 2: [Quality] â†’ Reduce bugs by 50%\n- Benefit 3: [Collaboration] â†’ Align teams in real-time\n- Benefit 4: [Cost] â†’ Save $100k/year on tools\n\nLEVEL 3: Features (supporting evidence)\n- Feature â†’ Benefit â†’ Outcome\n- AI automation â†’ Eliminates manual work â†’ Save 10 hrs/week\n- Real-time sync â†’ No version conflicts â†’ 50% fewer errors\n- Integrations â†’ Connect existing tools â†’ 80% faster onboarding\n\nLEVEL 4: Proof Points\n- Customer logos: [Microsoft, Shopify, Stripe]\n- Stats: Used by 10,000+ teams, 4.8/5 G2 rating\n- Case studies: How [Customer] achieved [Outcome]\n```\n\n**Messaging by Persona**:\n\n**Economic Buyer** (VP/Director):\n- Primary concern: ROI, business outcomes\n- Tone: Professional, data-driven, results-focused\n- Key message: \"Increase revenue by 25% while reducing costs by $200k/year\"\n- Proof: ROI calculator, case studies with $ impact\n\n**Technical Buyer** (Engineer/Architect):\n- Primary concern: Technical fit, security, scalability\n- Tone: Technical, detailed, objective\n- Key message: \"Enterprise-grade architecture with 99.99% uptime and SOC 2 compliance\"\n- Proof: Technical docs, security whitepaper, architecture diagram\n\n**End User** (Manager/Individual Contributor):\n- Primary concern: Ease of use, daily workflow\n- Tone: Friendly, empathetic, practical\n- Key message: \"Spend less time on busywork, more time on what matters\"\n- Proof: Product demo, free trial, customer testimonials\n\n### 2.3 Messaging Testing & Iteration\n\n**Message Testing Framework**:\n\n1. **Qualitative** (customer interviews):\n   - Ask 10-15 target customers:\n   - \"How would you describe [Product] to a colleague?\"\n   - \"What's the main benefit you get from [Product]?\"\n   - \"Why did you choose us over [Competitor]?\"\n\n2. **Quantitative** (A/B testing):\n   - Test messaging variations on:\n   - Landing page headlines\n   - Ad copy (LinkedIn, Google)\n   - Email subject lines\n   - Measure: CTR, conversion rate, demo requests\n\n3. **Sales Feedback** (win/loss analysis):\n   - Ask sales team monthly:\n   - \"Which message resonates most with prospects?\"\n   - \"What objections are we hearing?\"\n   - \"How do we compare to [Competitor] in customer's eyes?\"\n\n**Iteration Cycle**:\n- Test new messaging: 2-4 weeks\n- Analyze results: 1 week\n- Update messaging docs: 1 week\n- Train sales team: 1 week\n- Repeat quarterly\n\n---\n\n## 3. Competitive Intelligence\n\n### 3.1 Competitive Analysis Framework\n\n**Tier 1: Direct Competitors** (head-to-head, same category)\n- [Competitor A]: Market leader, $100M+ ARR\n- [Competitor B]: Fast-growing challenger, Series B\n- [Competitor C]: Open-source alternative\n\n**Tier 2: Indirect Competitors** (adjacent solutions)\n- [Alt Solution D]: Different approach, overlapping use case\n- [Alt Solution E]: Broader platform, includes your feature\n\n**Tier 3: Status Quo** (what customers do today)\n- Spreadsheets + email\n- Build in-house\n- Do nothing\n\n**Competitive Intelligence Sources**:\n1. **Product trials**: Sign up for competitor products, use actively\n2. **Website monitoring**: Track changes to pricing, messaging, features\n3. **Customer interviews**: Ask \"What alternatives did you consider?\"\n4. **Sales call recordings** (Gong/Chorus): Listen for competitor mentions\n5. **Review sites** (G2, Capterra): Read competitor reviews (pros/cons)\n6. **Job postings**: Competitor hiring = roadmap insights\n7. **Financial filings** (if public): Revenue, growth, strategy\n8. **Social media**: Follow competitor execs, product teams\n9. **Partner channels**: Talk to shared implementation partners\n10. **Industry reports**: Gartner, Forrester, IDC\n\n### 3.2 Competitive Battlecards\n\n**Battlecard Template** (create one per competitor):\n\n```\nCOMPETITOR: [Competitor A]\n\nOVERVIEW:\n- Founded: 2015\n- Funding: Series C, $75M raised\n- HQ: San Francisco\n- Size: 200 employees\n- Customers: 5,000+ companies\n- Pricing: $50-$500/user/month\n\nPOSITIONING:\n- They say: \"All-in-one platform for modern teams\"\n- Reality: Broad but shallow, not deep in any use case\n\nKEY STRENGTHS (What They Do Well):\n1. Strong brand recognition (category leader)\n2. Large feature set (breadth over depth)\n3. Extensive integrations (2,000+ apps)\n\nKEY WEAKNESSES (Where They Fall Short):\n1. Complex UI (steep learning curve)\n2. Expensive (2x our price at scale)\n3. Poor support (low NPS in reviews)\n4. Legacy architecture (slow performance)\n\nOUR ADVANTAGES:\n1. 10x easier to use (time-to-value in minutes vs. days)\n2. 50% lower cost at 100+ users\n3. Superior performance (2x faster load times)\n4. White-glove onboarding (dedicated CSM)\n\nWHEN TO WIN:\n- Customer values ease of use over features\n- Budget-conscious (not enterprise)\n- Need fast time-to-value (<1 week)\n- Poor experience with competitor (switching)\n\nWHEN TO LOSE:\n- Enterprise (>5000 employees) with complex requirements\n- Need feature X that we don't have yet\n- Deep integration with competitor's ecosystem\n- Already invested heavily in competitor (sunk cost)\n\nTALK TRACKS:\n\nObjection: \"We're already using [Competitor A]\"\nResponse: \"That's great - many of our customers came from [Competitor A]. What prompted you to explore alternatives? [Listen for pain points] Typically teams switch to us because [ease of use / cost / performance]. Would it be helpful to see a side-by-side comparison?\"\n\nObjection: \"[Competitor A] has more features\"\nResponse: \"You're right - they've been around longer and have a broader feature set. Here's what we found: most teams only use 20% of those features. Our customers love that we focus on doing [core use case] exceptionally well rather than trying to do everything. What features are most critical for your team?\"\n\nPROOF POINTS:\n- Case study: \"[Customer] switched from [Competitor A], reduced costs by 60%\"\n- Review comparison: \"[4.8 vs. 4.2 G2 rating in 'Ease of Use']\"\n- Win rate: \"35% win rate in competitive deals\"\n\nCOMPETITIVE LANDSCAPE:\n[Link to competitive positioning map]\n[Link to feature comparison matrix]\n```\n\n**Battlecard Distribution**:\n- Store in: Notion, Confluence, or sales enablement platform\n- Update frequency: Monthly (or when competitor launches major feature)\n- Access: Sales, CS, Product, Marketing teams\n- Training: Monthly competitive update calls with sales\n\n### 3.3 Win/Loss Analysis\n\n**Win/Loss Interview Process**:\n\n**Goals**:\n- Understand why you won/lost\n- Validate positioning and messaging\n- Identify product gaps\n- Track competitive trends\n\n**Process**:\n1. **Identify deals** (closed won or lost in last 30 days)\n2. **Request interview** (email or HubSpot workflow)\n3. **Conduct interview** (30-45 min, record with permission)\n4. **Analyze data** (themes, patterns, trends)\n5. **Share insights** (monthly report to product, sales, marketing)\n\n**Interview Questions** (pick 8-10):\n\n**For Wins**:\n- What problem were you trying to solve?\n- What alternatives did you evaluate?\n- Why did you choose us over [Competitor]?\n- What almost made you choose someone else?\n- What could we improve?\n\n**For Losses**:\n- What problem were you trying to solve?\n- Who did you choose instead? Why?\n- What did we do well in the sales process?\n- What could we have done differently?\n- Would you consider us in the future? When?\n\n**Data Tracking** (in HubSpot or spreadsheet):\n\n| Deal | Outcome | Reason | Competitor | Price Factor | Product Gap | Messaging Issue |\n|------|---------|--------|------------|--------------|-------------|-----------------|\n| Acme Corp | Won | Best product fit | Competitor A | No | No | No |\n| Beta Inc | Lost | Price | Competitor B | Yes | No | No |\n| Gamma LLC | Lost | Missing feature X | Built in-house | No | Yes | No |\n\n**Monthly Insights Report**:\n```\nWin/Loss Summary (March 2025):\n- Total deals analyzed: 20 (12 wins, 8 losses)\n- Win rate: 60%\n- Top win reasons:\n  1. Ease of use (8 mentions)\n  2. Better support (6 mentions)\n  3. Price (4 mentions)\n- Top loss reasons:\n  1. Missing feature X (4 mentions)\n  2. Price (3 mentions)\n  3. Competitor relationship (2 mentions)\n\nAction Items:\n- Product: Prioritize feature X (lost 4 deals)\n- Sales: Update battlecard for Competitor A (won 5 competitive deals)\n- Marketing: Create case study on \"ease of use\" theme\n```\n\n---\n\n## 4. Go-To-Market (GTM) Strategy\n\n### 4.1 GTM Motion Types\n\n**PLG (Product-Led Growth)**:\n- Entry: Free trial or freemium\n- Buyer: End user â†’ Manager â†’ VP\n- Sales: Low touch or self-serve\n- ACV: <$10k\n- Example: Slack, Notion, Figma\n\n**Sales-Led Growth**:\n- Entry: Demo request â†’ Sales qualification\n- Buyer: VP â†’ C-level\n- Sales: High touch, consultative\n- ACV: $25k+\n- Example: Salesforce, Workday, SAP\n\n**Hybrid (PLG + Sales)**:\n- Entry: Free trial for SMB, demo for Enterprise\n- Buyer: End user (PLG) or Executive (Sales-Led)\n- Sales: Self-serve â†’ Assisted â†’ Enterprise\n- ACV: $5k-$100k\n- Example: HubSpot, Atlassian, Zoom\n\n**Series A Recommendation**: Start with **Hybrid**\n- Reason: Faster learning, broader TAM, efficient scaling\n- Approach:\n  - Bottom-up (PLG): Free trial â†’ Paid team plan â†’ Upgrade to Enterprise\n  - Top-down (Sales): Outbound to Enterprise â†’ Demo â†’ POC â†’ Close\n\n### 4.2 GTM Launch Playbook (90-Day Plan)\n\n**Pre-Launch (Days -90 to -30)**:\n\nWeek 1-4: Foundation\n- [ ] Define ICP and buyer personas\n- [ ] Develop positioning and messaging\n- [ ] Create competitive battlecards\n- [ ] Set success metrics (pipeline $, MQLs, win rate)\n\nWeek 5-8: Content & Enablement\n- [ ] Build website pages (homepage, product, pricing)\n- [ ] Create sales deck and demo script\n- [ ] Produce launch assets (one-pager, case studies, FAQs)\n- [ ] Develop email nurture sequences\n- [ ] Train sales team on positioning and talk tracks\n\nWeek 9-12: Channel Setup\n- [ ] Launch paid campaigns (LinkedIn, Google)\n- [ ] Set up HubSpot tracking and attribution\n- [ ] Publish SEO content (blog posts, guides)\n- [ ] Activate partnerships (co-marketing plans)\n- [ ] Test conversion funnels (landing page â†’ signup)\n\n**Launch (Days 1-30)**:\n\nWeek 1: Awareness\n- [ ] Press release distribution\n- [ ] Email announcement to existing database\n- [ ] Social media campaign (LinkedIn, Twitter)\n- [ ] Paid ads go live (awareness campaigns)\n- [ ] Outbound sales blitz (top 100 accounts)\n\nWeek 2-4: Activation\n- [ ] Monitor conversion rates (daily)\n- [ ] A/B test landing pages and ad copy\n- [ ] Sales follow-up on inbound leads (<4 hour SLA)\n- [ ] Customer interviews (feedback on positioning)\n- [ ] Adjust messaging based on early signals\n\n**Post-Launch (Days 31-90)**:\n\nWeek 5-8: Optimization\n- [ ] Analyze win/loss data (why did we win/lose?)\n- [ ] Optimize underperforming channels (pause or pivot)\n- [ ] Scale winning channels (20% weekly budget increase)\n- [ ] Publish post-launch case studies\n- [ ] Expand content (SEO, demand gen)\n\nWeek 9-12: Scale\n- [ ] Enter new market segments (vertical or geo)\n- [ ] Launch partnerships (co-marketing campaigns)\n- [ ] Build PLG loops (referral program, viral features)\n- [ ] Sales team expansion (hire based on pipeline)\n- [ ] Iterate positioning (quarterly messaging refresh)\n\n### 4.3 International Market Entry (EU/US/Canada)\n\n**Market Entry Priority** (Series A recommended order):\n\n**Phase 1: US Market** (Months 1-6)\n- Why: Largest TAM, fastest sales cycles, highest ACV\n- Entry strategy:\n  - Hire US-based SDRs/AEs (or partner with US sales agency)\n  - Localize website (USD pricing, US phone number)\n  - Paid ads (Google + LinkedIn) targeting US companies\n  - Partnerships with US-based tech companies\n- Budget: 50% of total marketing spend\n- Target: $1M ARR from US by Month 6\n\n**Phase 2: UK Market** (Months 4-9)\n- Why: English-speaking, gateway to EU, similar to US\n- Entry strategy:\n  - Hire UK sales rep or partner with UK agency\n  - Localize pricing (GBP), GDPR compliance\n  - Content localization (British spelling, cultural nuances)\n  - UK partnerships (local SaaS companies)\n- Budget: 20% of marketing spend\n- Target: $500k ARR from UK by Month 9\n\n**Phase 3: DACH (Germany/Austria/Switzerland)** (Months 7-12)\n- Why: Largest EU economy, high data privacy standards\n- Entry strategy:\n  - Translate website and product (German)\n  - Hire German-speaking sales rep\n  - GDPR compliance (critical for German market)\n  - Partnerships with German tech companies\n  - Local case studies and testimonials\n- Budget: 15% of marketing spend\n- Target: $300k ARR from DACH by Month 12\n\n**Phase 4: France** (Months 10-15)\n- Why: Second largest EU market, localization critical\n- Entry strategy:\n  - Full French translation (website, product, support)\n  - Hire French-speaking sales and support\n  - French partnerships and case studies\n  - Comply with French data regulations\n- Budget: 10% of marketing spend\n- Target: $200k ARR from France by Month 15\n\n**Phase 5: Canada** (Months 7-12)\n- Why: Similar to US, easier entry, smaller market\n- Entry strategy:\n  - Minimal localization (CAD pricing)\n  - Leverage US sales team (similar buying behavior)\n  - Canadian partnerships\n- Budget: 5% of marketing spend\n- Target: $100k ARR from Canada by Month 12\n\n**Localization Checklist (per market)**:\n\n- [ ] **Website**: Translate, localize currency, phone number\n- [ ] **Product**: UI translation (if needed for that market)\n- [ ] **Pricing**: Local currency, VAT/taxes displayed\n- [ ] **Support**: Local business hours, language support\n- [ ] **Legal**: Data privacy compliance (GDPR, CCPA)\n- [ ] **Sales**: Hire local reps or partner with local agency\n- [ ] **Marketing**: Localized ads, content, case studies\n- [ ] **Payments**: Local payment methods (SEPA, iDEAL, etc.)\n\n**Budget Allocation** (international expansion):\n```\nYear 1 (Series A):\n- US: 50% ($200k)\n- UK: 20% ($80k)\n- DACH: 15% ($60k)\n- France: 10% ($40k)\n- Canada: 5% ($20k)\n\nTotal: $400k marketing spend (international)\nExpected ROI: 3:1 (marketing-sourced pipeline : spend)\n```\n\n---\n\n## 5. Product Launch Framework\n\n### 5.1 Launch Tiers (Effort vs. Impact)\n\n**Tier 1: Major Launch** (quarterly, high impact)\n- Scope: New product, major feature, platform expansion\n- Audience: Existing customers + new prospects + press\n- Effort: 6-8 weeks prep, full cross-functional launch\n- Budget: $50k-$100k (Series A)\n- Activities: Press release, webinar, email series, paid ads, sales blitz\n\n**Tier 2: Standard Launch** (monthly, medium impact)\n- Scope: Significant feature, integration, improvement\n- Audience: Existing customers + select prospects\n- Effort: 3-4 weeks prep, core team involvement\n- Budget: $10k-$25k\n- Activities: Blog post, email announcement, product update, sales enablement\n\n**Tier 3: Minor Launch** (weekly, low impact)\n- Scope: Small feature, bug fix, optimization\n- Audience: Existing customers only\n- Effort: 1 week prep, product + marketing only\n- Budget: <$5k\n- Activities: In-app notification, changelog, support docs\n\n### 5.2 Major Launch Playbook (Tier 1)\n\n**8 Weeks Before Launch**:\n\nWeek -8:\n- [ ] Kickoff meeting (Product, Marketing, Sales, CS)\n- [ ] Define launch goals (pipeline $, MQLs, press coverage)\n- [ ] Identify target audience (ICP, personas)\n- [ ] Create positioning and messaging\n- [ ] Assign roles and responsibilities\n\nWeek -7:\n- [ ] Develop GTM strategy (channels, tactics, budget)\n- [ ] Create sales enablement (deck, demo script, FAQs)\n- [ ] Plan content (blog posts, case studies, videos)\n- [ ] Design creative assets (ads, social graphics, emails)\n\nWeek -6:\n- [ ] Build landing pages (product page, demo request)\n- [ ] Set up HubSpot campaigns and tracking\n- [ ] Write press release and pitch media\n- [ ] Create email nurture sequences\n- [ ] Produce demo video\n\nWeek -5:\n- [ ] Beta test with select customers (feedback)\n- [ ] Train sales team (positioning, demo, objection handling)\n- [ ] Train CS team (onboarding, support docs)\n- [ ] Finalize launch timeline and channel mix\n- [ ] Prepare customer case studies\n\n**4 Weeks Before Launch**:\n\nWeek -4:\n- [ ] Launch paid ad campaigns (LinkedIn, Google)\n- [ ] Publish teaser content (blog, social)\n- [ ] Send pre-launch email to customer base\n- [ ] Pitch press and influencers\n- [ ] Set up webinar registration\n\nWeek -3:\n- [ ] A/B test landing pages and ad copy\n- [ ] Ramp up content production (blog posts, videos)\n- [ ] Sales prospecting (outbound to target accounts)\n- [ ] Finalize webinar content and speakers\n- [ ] Prepare launch day checklist\n\nWeek -2:\n- [ ] Send reminder emails (webinar, launch countdown)\n- [ ] Increase paid ad spend (ramp up)\n- [ ] Sales follow-up on warmed leads\n- [ ] Dry run: Test all systems (website, forms, CRM)\n- [ ] Prepare launch day assets (social posts, emails)\n\nWeek -1:\n- [ ] Final review: All assets approved\n- [ ] Pre-launch email to VIP customers and partners\n- [ ] Sales team ready (trained, motivated, quotas set)\n- [ ] CS team ready (docs updated, chat support staffed)\n- [ ] Press embargo lifts (if applicable)\n\n**Launch Week**:\n\nDay 1 (Launch Day):\n- [ ] Press release goes live (distribute to media)\n- [ ] Email announcement to full database\n- [ ] Social media blitz (LinkedIn, Twitter, Facebook)\n- [ ] Paid ads at full budget\n- [ ] Sales outbound campaign (top 500 accounts)\n- [ ] Product update in-app (notify existing users)\n- [ ] Monitor metrics (signups, demos, press pickup)\n\nDays 2-5:\n- [ ] Daily monitoring (conversion rates, funnel drop-offs)\n- [ ] A/B test optimizations (headlines, CTAs)\n- [ ] Sales follow-up (4-hour SLA on inbound leads)\n- [ ] Respond to press inquiries\n- [ ] Post customer testimonials and early wins\n- [ ] Webinar (Day 3 or 4)\n\nWeek 2:\n- [ ] Analyze launch results (vs. goals)\n- [ ] Publish post-launch content (case studies, how-to guides)\n- [ ] Sales continue outbound (sustained momentum)\n- [ ] Optimize underperforming channels\n- [ ] Scale winning channels (increase budget)\n\nWeek 3-4:\n- [ ] Post-launch report (metrics, learnings, next steps)\n- [ ] Customer feedback interviews (product improvements)\n- [ ] Win/loss analysis (why did we win/lose deals?)\n- [ ] Adjust messaging and positioning (based on feedback)\n- [ ] Plan next launch (apply learnings)\n\n### 5.3 Launch Metrics Dashboard\n\n**Leading Indicators** (track daily):\n- Landing page visitors\n- Demo requests\n- Free trial signups\n- MQLs generated\n- Sales pipeline created ($)\n\n**Lagging Indicators** (track weekly/monthly):\n- SQLs generated\n- Deals closed (count + $)\n- Win rate (vs. pre-launch)\n- Customer adoption rate (% of customers using feature)\n- NPS score (feature-specific)\n\n**HubSpot Dashboard**:\n```\nLaunch Campaign: [Q2-2025-Product-X-Launch]\n\nWEEK 1 RESULTS:\nTraffic: 10,000 visitors (goal: 8,000) âœ…\nMQLs: 250 (goal: 200) âœ…\nSQLs: 40 (goal: 50) âš ï¸\nPipeline: $800k (goal: $1M) âš ï¸\nDemos: 80 (goal: 100) âš ï¸\n\nTOP CHANNELS:\n1. LinkedIn Ads: 120 MQLs, $150 CPL\n2. Email: 80 MQLs, $25 CPL\n3. Organic: 40 MQLs, $0 CPL\n\nUNDERPERFORMING:\n- Google Search: 10 MQLs, $400 CPL (pause and optimize)\n- Webinar: 50 registrants, 20% show rate (improve email reminders)\n\nNEXT ACTIONS:\n- Increase LinkedIn Ads budget by 30%\n- A/B test new landing page headline\n- Sales follow-up blitz on 40 SQLs\n```\n\n---\n\n## 6. Sales Enablement & Collaboration\n\n### 6.1 Sales Enablement Assets (Must-Have)\n\n**Core Assets**:\n\n**1. Sales Deck** (15-20 slides)\n```\nSlide 1: Title slide (logo, tagline)\nSlide 2: Agenda\nSlide 3: Company intro (mission, vision, traction)\nSlide 4: Problem statement (customer pain points)\nSlide 5: Solution overview (your product)\nSlide 6: Key benefits (3-5 bullets)\nSlide 7: Product demo (screenshots or video)\nSlide 8: Differentiation (vs. competitors)\nSlide 9: Customer logos (social proof)\nSlide 10: Case study (results-focused)\nSlide 11: Pricing and plans\nSlide 12: Implementation timeline\nSlide 13: Support and success\nSlide 14: Next steps (CTA)\nSlide 15: Q&A\n\nGuidelines:\n- Visual-first (minimal text, large images)\n- Customer-centric (benefits > features)\n- Modular (easy to skip/reorder slides)\n- Updated quarterly (or after major product changes)\n```\n\n**2. One-Pagers** (1-page PDF)\n- Product overview (what it is, who it's for, key features)\n- Competitive comparison (vs. Competitor A, B, C)\n- Case study (customer story with metrics)\n- Pricing sheet (plans, features, add-ons)\n\n**3. Battlecards** (per competitor)\n- See Section 3.2 for detailed battlecard template\n\n**4. Demo Script** (30-45 min)\n```\nDemo Flow:\n1. Intro (2 min) - Who we are, what we'll cover\n2. Discovery (5 min) - Ask about their needs, pain points\n3. Demo (20 min) - Show product (focus on their use case)\n4. Q&A (10 min) - Address objections, questions\n5. Next steps (3 min) - Define trial or POC plan\n\nDemo Tips:\n- Show, don't tell (product in action > slides)\n- Use customer data (not \"Company XYZ\" examples)\n- Focus on outcomes (not features)\n- Address objections proactively (price, competition)\n- Always drive to next step (trial, POC, proposal)\n```\n\n**5. Email Templates** (HubSpot sequences)\n- Cold outreach (prospecting)\n- Demo follow-up\n- Trial conversion\n- Proposal sent\n- Closing sequence\n\n**6. ROI Calculator** (spreadsheet or web tool)\n- Input: Customer's current costs, time spent, team size\n- Output: Savings with your product, payback period, 3-year ROI\n- Example: \"Save $150k/year, 6-month payback, 500% ROI\"\n\n### 6.2 Sales Training Program\n\n**Monthly Sales Enablement Call** (60 min):\n- Product updates (new features, roadmap)\n- Competitive landscape (new competitors, battlecard updates)\n- Win/loss insights (why we're winning/losing)\n- Best practices (top performer shares tips)\n- Q&A (open forum for questions)\n\n**Quarterly Sales Training** (half-day workshop):\n- Deep dive: Positioning and messaging refresh\n- Role-playing: Objection handling, competitive demos\n- Product training: New features, advanced use cases\n- Customer panel: Hear directly from customers (why they bought)\n\n**Sales Onboarding** (new hires):\n- Week 1: Company, product, market overview\n- Week 2: ICP, personas, messaging\n- Week 3: Competitive intelligence, battlecards\n- Week 4: Demo certification (must pass to sell)\n\n### 6.3 Marketing â†” Sales Handoffs\n\n**MQL â†’ SQL Handoff** (see marketing-demand-acquisition skill for details)\n\n**Product Marketing â†’ Sales**:\n\n**Weekly Sync** (30 min):\n- Review: Win/loss insights, competitive updates\n- Share: New assets (battlecards, case studies, one-pagers)\n- Feedback: What's working, what's not\n- Request: Sales asks for specific assets (e.g., \"Need competitor X battlecard\")\n\n**Quarterly Business Review** (QBR):\n- Results: Pipeline, win rate, deal size, sales velocity\n- Insights: Top win/loss reasons, competitive trends\n- Action items: Product gaps, messaging updates, enablement needs\n\n**Communication Channels**:\n- Slack: #sales-enablement (daily questions, quick updates)\n- HubSpot: Centralized asset library (decks, one-pagers, videos)\n- Notion: Internal wiki (positioning, messaging, competitive intel)\n\n---\n\n## 7. Metrics & Analytics\n\n### 7.1 PMM KPIs (Track Monthly)\n\n**Product Adoption**:\n- % of customers using new feature (within 30 days of launch)\n- Target: >40% adoption within 90 days\n\n**Sales Velocity**:\n- Days from SQL to closed won\n- Target: Decrease by 20% YoY\n\n**Win Rate**:\n- % of opportunities won (vs. competitors)\n- Target: >30% win rate (competitive deals)\n\n**Deal Size**:\n- Average contract value (ACV)\n- Target: Increase by 25% YoY\n\n**Launch Impact**:\n- Pipeline $ generated from launch campaigns\n- Target: 3:1 ROMI (pipeline $ : marketing spend)\n\n**Competitive Win Rate**:\n- % of deals won against Competitor A, B, C\n- Target: >35% win rate vs. top competitor\n\n### 7.2 HubSpot Reporting\n\n**Custom Reports**:\n\n**1. Product Launch Impact**\n```\nMetrics: Leads, MQLs, SQLs, Pipeline $, Closed Won $\nDimensions: Campaign, Channel, Region\nFilters: Campaign = \"Q2-2025-Product-X-Launch\"\nTime period: 90 days post-launch\n```\n\n**2. Competitive Win Rate**\n```\nMetrics: Opportunities, Closed Won, Win Rate %\nDimensions: Competitor (property)\nFilters: Deal stage = Closed Won or Closed Lost\nSegment by: Competitor A, B, C, Other\n```\n\n**3. Sales Enablement Usage**\n```\nMetrics: Asset downloads, views, shares\nDimensions: Asset type (deck, battlecard, case study)\nFilters: User = Sales team\nInsight: Which assets are most used by sales\n```\n\n### 7.3 Quarterly Business Review (QBR)\n\n**QBR Template** (present to executive team):\n\n**Slide 1: Executive Summary**\n```\nQ2 2025 Highlights:\n- Launched Product X (pipeline: $2M, 500 MQLs)\n- Entered UK market (20 new customers, $400k ARR)\n- Improved win rate by 15% (competitive positioning)\n- Published 3 case studies (2x sales usage vs. Q1)\n```\n\n**Slide 2: Metrics Dashboard**\n```\nKPI             Q2 Target   Q2 Actual   Status\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nMQLs            800         950         âœ… +19%\nSQLs            150         140         âš ï¸ -7%\nPipeline $      $4M         $3.8M       âš ï¸ -5%\nWin Rate        30%         35%         âœ… +17%\nDeal Size       $45k        $52k        âœ… +16%\nSales Velocity  75 days     68 days     âœ… -9%\n```\n\n**Slide 3: Key Insights**\n```\nWhat Worked:\n1. Product X launch exceeded MQL target by 19%\n2. Improved competitive positioning â†’ 35% win rate\n3. UK market entry on track ($400k ARR in 3 months)\n\nWhat Didn't Work:\n1. SQL conversion rate dropped from 20% to 15%\n2. Google Ads underperformed (paused and optimizing)\n3. Competitor A launched aggressive pricing (5 lost deals)\n\nAction Items:\n1. Improve SQL qualification criteria (work with sales)\n2. Update battlecard for Competitor A (new pricing)\n3. Double down on UK market (hire local AE)\n```\n\n**Slide 4: Next Quarter Plan**\n```\nQ3 2025 Priorities:\n1. Launch Product Y (pipeline target: $3M)\n2. Enter DACH market (Germany, Austria, Switzerland)\n3. Refresh messaging and website (new positioning)\n4. Scale partnerships (3 new strategic partners)\n5. Build customer advocacy program (10 case studies)\n\nBudget: $150k (up from $120k in Q2)\nHeadcount: +1 PMM, +1 Content Marketer\n```\n\n---\n\n## 8. Quick Reference\n\n### 8.1 PMM Monthly Checklist\n\n**Week 1** (Strategy & Planning):\n- [ ] Review previous month metrics (win rate, deal size, pipeline)\n- [ ] Analyze win/loss interviews (competitive trends)\n- [ ] Update competitive battlecards (if needed)\n- [ ] Plan next month campaigns and content\n\n**Week 2** (Content & Enablement):\n- [ ] Create new sales assets (1-pager, case study, deck update)\n- [ ] Publish content (blog post, video, webinar)\n- [ ] Train sales on new positioning or product updates\n- [ ] Review sales asset usage (what's working?)\n\n**Week 3** (Launches & Campaigns):\n- [ ] Support product launches (if any)\n- [ ] Monitor campaign performance (MQLs, SQLs, pipeline)\n- [ ] Optimize underperforming channels\n- [ ] Customer interviews (feedback on positioning)\n\n**Week 4** (Reporting & Iteration):\n- [ ] Monthly metrics report (for exec team)\n- [ ] Sales enablement call (updates, Q&A)\n- [ ] Win/loss analysis (themes, trends)\n- [ ] Plan next quarter launches and strategy\n\n### 8.2 Positioning Development Timeline\n\n**Week 1**: Research\n- Customer interviews (10-15)\n- Competitive analysis\n- Market trends\n\n**Week 2**: Framework\n- April Dunford positioning exercise\n- Define unique value\n- Identify best-fit customers\n\n**Week 3**: Messaging\n- Craft value proposition\n- Build messaging hierarchy\n- Create persona-specific messaging\n\n**Week 4**: Validation\n- Test with sales team\n- A/B test on landing pages\n- Customer feedback\n\n**Week 5-6**: Rollout\n- Update website, sales decks\n- Train sales and CS teams\n- Launch campaigns with new messaging\n\n### 8.3 Team Handoff Protocols\n\n**PMM â†’ Demand Gen**:\n- Deliver: Positioning, messaging, competitive intel, launch plans\n- Frequency: Monthly sync + ad-hoc for launches\n- SLA: 2-week lead time for major campaigns\n\n**PMM â†’ Sales**:\n- Deliver: Battlecards, sales decks, demo scripts, objection handling\n- Frequency: Monthly enablement call + weekly Slack updates\n- SLA: 48 hours for urgent competitive questions\n\n**PMM â†’ Product**:\n- Deliver: Customer feedback, competitive feature gaps, win/loss insights\n- Frequency: Weekly product sync\n- SLA: Quarterly roadmap input (feature prioritization)\n\n**PMM â†’ Customer Success**:\n- Deliver: Product positioning, adoption tactics, customer education content\n- Frequency: Monthly sync\n- SLA: 1 week for new product launch enablement\n\n---\n\n## Resources\n\n### references/\n\n- **positioning-frameworks.md** - Detailed guide on April Dunford, Geoffrey Moore positioning methods\n- **launch-checklists.md** - Tier 1/2/3 launch checklists and templates\n- **international-gtm.md** - Market-by-market expansion playbooks (US, UK, DACH, France, Canada)\n- **messaging-templates.md** - Ready-to-use messaging frameworks for different personas\n\n### scripts/\n\n- **competitor_tracker.py** - Track competitor website/pricing changes\n- **win_loss_analyzer.py** - Analyze win/loss interview data for trends\n\n### assets/\n\n- **sales-deck-template.pptx** - Editable master sales deck\n- **battlecard-template.docx** - Competitive battlecard template\n- **one-pager-template.pptx** - Product one-pager design template\n- **roi-calculator.xlsx** - ROI calculator spreadsheet\n\n---\n\n**Last Updated**: October 2025 | **Version**: 1.0\n"
    }
  },
  "alirezarezvani-claude-skills-app-store-optimization": {
    "id": "alirezarezvani-claude-skills-app-store-optimization",
    "name": "app-store-optimization",
    "description": "Complete App Store Optimization (ASO) toolkit for researching, optimizing, and tracking mobile app performance on Apple App Store and Google Play Store",
    "repo": {
      "owner": "alirezarezvani",
      "name": "claude-skills",
      "fullName": "alirezarezvani/claude-skills",
      "url": "https://github.com/alirezarezvani/claude-skills/tree/main/marketing-skill/app-store-optimization",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 521,
      "forks": 103,
      "language": "Python",
      "topics": [
        "agentic-ai",
        "agentic-coding",
        "anthropic-claude",
        "claude-ai",
        "claude-ai-skills",
        "claude-code",
        "claude-code-skills",
        "claude-skills",
        "claudecode-subagents"
      ],
      "updatedAt": "2026-01-08T12:36:14Z",
      "pushedAt": "2026-01-08T08:39:21Z",
      "createdAt": "2025-10-19T04:04:05Z",
      "license": "MIT License"
    },
    "category": "marketing",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: app-store-optimization\ndescription: Complete App Store Optimization (ASO) toolkit for researching, optimizing, and tracking mobile app performance on Apple App Store and Google Play Store\n---\n\n# App Store Optimization (ASO) Skill\n\nThis comprehensive skill provides complete ASO capabilities for successfully launching and optimizing mobile applications on the Apple App Store and Google Play Store.\n\n## Capabilities\n\n### Research & Analysis\n- **Keyword Research**: Analyze keyword volume, competition, and relevance for app discovery\n- **Competitor Analysis**: Deep-dive into top-performing apps in your category\n- **Market Trend Analysis**: Identify emerging trends and opportunities in your app category\n- **Review Sentiment Analysis**: Extract insights from user reviews to identify strengths and issues\n- **Category Analysis**: Evaluate optimal category and subcategory placement strategies\n\n### Metadata Optimization\n- **Title Optimization**: Create compelling titles with optimal keyword placement (platform-specific character limits)\n- **Description Optimization**: Craft both short and full descriptions that convert and rank\n- **Subtitle/Promotional Text**: Optimize Apple-specific subtitle (30 chars) and promotional text (170 chars)\n- **Keyword Field**: Maximize Apple's 100-character keyword field with strategic selection\n- **Category Selection**: Data-driven recommendations for primary and secondary categories\n- **Icon Best Practices**: Guidelines for designing high-converting app icons\n- **Screenshot Optimization**: Strategies for creating screenshots that drive installs\n- **Preview Video**: Best practices for app preview videos\n- **Localization**: Multi-language optimization strategies for global reach\n\n### Conversion Optimization\n- **A/B Testing Framework**: Plan and track metadata experiments for continuous improvement\n- **Visual Asset Testing**: Test icons, screenshots, and videos for maximum conversion\n- **Store Listing Optimization**: Comprehensive page optimization for impression-to-install conversion\n- **Call-to-Action**: Optimize CTAs in descriptions and promotional materials\n\n### Rating & Review Management\n- **Review Monitoring**: Track and analyze user reviews for actionable insights\n- **Response Strategies**: Templates and best practices for responding to reviews\n- **Rating Improvement**: Tactical approaches to improve app ratings organically\n- **Issue Identification**: Surface common problems and feature requests from reviews\n\n### Launch & Update Strategies\n- **Pre-Launch Checklist**: Complete validation before submitting to stores\n- **Launch Timing**: Optimize release timing for maximum visibility and downloads\n- **Update Cadence**: Plan optimal update frequency and feature rollouts\n- **Feature Announcements**: Craft \"What's New\" sections that re-engage users\n- **Seasonal Optimization**: Leverage seasonal trends and events\n\n### Analytics & Tracking\n- **ASO Score**: Calculate overall ASO health score across multiple factors\n- **Keyword Rankings**: Track keyword position changes over time\n- **Conversion Metrics**: Monitor impression-to-install conversion rates\n- **Download Velocity**: Track download trends and momentum\n- **Performance Benchmarking**: Compare against category averages and competitors\n\n### Platform-Specific Requirements\n- **Apple App Store**:\n  - Title: 30 characters\n  - Subtitle: 30 characters\n  - Promotional Text: 170 characters (editable without app update)\n  - Description: 4,000 characters\n  - Keywords: 100 characters (comma-separated, no spaces)\n  - What's New: 4,000 characters\n- **Google Play Store**:\n  - Title: 50 characters (formerly 30, increased in 2021)\n  - Short Description: 80 characters\n  - Full Description: 4,000 characters\n  - No separate keyword field (keywords extracted from title and description)\n\n## Input Requirements\n\n### Keyword Research\n```json\n{\n  \"app_name\": \"MyApp\",\n  \"category\": \"Productivity\",\n  \"target_keywords\": [\"task manager\", \"productivity\", \"todo list\"],\n  \"competitors\": [\"Todoist\", \"Any.do\", \"Microsoft To Do\"],\n  \"language\": \"en-US\"\n}\n```\n\n### Metadata Optimization\n```json\n{\n  \"platform\": \"apple\" | \"google\",\n  \"app_info\": {\n    \"name\": \"MyApp\",\n    \"category\": \"Productivity\",\n    \"target_audience\": \"Professionals aged 25-45\",\n    \"key_features\": [\"Task management\", \"Team collaboration\", \"AI assistance\"],\n    \"unique_value\": \"AI-powered task prioritization\"\n  },\n  \"current_metadata\": {\n    \"title\": \"Current Title\",\n    \"subtitle\": \"Current Subtitle\",\n    \"description\": \"Current description...\"\n  },\n  \"target_keywords\": [\"productivity\", \"task manager\", \"todo\"]\n}\n```\n\n### Review Analysis\n```json\n{\n  \"app_id\": \"com.myapp.app\",\n  \"platform\": \"apple\" | \"google\",\n  \"date_range\": \"last_30_days\" | \"last_90_days\" | \"all_time\",\n  \"rating_filter\": [1, 2, 3, 4, 5],\n  \"language\": \"en\"\n}\n```\n\n### ASO Score Calculation\n```json\n{\n  \"metadata\": {\n    \"title_quality\": 0.8,\n    \"description_quality\": 0.7,\n    \"keyword_density\": 0.6\n  },\n  \"ratings\": {\n    \"average_rating\": 4.5,\n    \"total_ratings\": 15000\n  },\n  \"conversion\": {\n    \"impression_to_install\": 0.05\n  },\n  \"keyword_rankings\": {\n    \"top_10\": 5,\n    \"top_50\": 12,\n    \"top_100\": 18\n  }\n}\n```\n\n## Output Formats\n\n### Keyword Research Report\n- List of recommended keywords with search volume estimates\n- Competition level analysis (low/medium/high)\n- Relevance scores for each keyword\n- Strategic recommendations for primary vs. secondary keywords\n- Long-tail keyword opportunities\n\n### Optimized Metadata Package\n- Platform-specific title (with character count validation)\n- Subtitle/promotional text (Apple)\n- Short description (Google)\n- Full description (both platforms)\n- Keyword field (Apple - 100 chars)\n- Character count validation for all fields\n- Keyword density analysis\n- Before/after comparison\n\n### Competitor Analysis Report\n- Top 10 competitors in category\n- Their metadata strategies\n- Keyword overlap analysis\n- Visual asset assessment\n- Rating and review volume comparison\n- Identified gaps and opportunities\n\n### ASO Health Score\n- Overall score (0-100)\n- Category breakdown:\n  - Metadata Quality (0-25)\n  - Ratings & Reviews (0-25)\n  - Keyword Performance (0-25)\n  - Conversion Metrics (0-25)\n- Specific improvement recommendations\n- Priority action items\n\n### A/B Test Plan\n- Hypothesis and test variables\n- Test duration recommendations\n- Success metrics definition\n- Sample size calculations\n- Statistical significance thresholds\n\n### Launch Checklist\n- Pre-submission validation (all required assets, metadata)\n- Store compliance verification\n- Testing checklist (devices, OS versions)\n- Marketing preparation items\n- Post-launch monitoring plan\n\n## How to Use\n\n### Keyword Research\n```\nHey Claudeâ€”I just added the \"app-store-optimization\" skill. Can you research the best keywords for a productivity app targeting professionals? Focus on keywords with good search volume but lower competition.\n```\n\n### Optimize App Store Listing\n```\nHey Claudeâ€”I just added the \"app-store-optimization\" skill. Can you optimize my app's metadata for the Apple App Store? Here's my current listing: [provide current metadata]. I want to rank for \"task management\" and \"productivity tools\".\n```\n\n### Analyze Competitor Strategy\n```\nHey Claudeâ€”I just added the \"app-store-optimization\" skill. Can you analyze the ASO strategies of Todoist, Any.do, and Microsoft To Do? I want to understand what they're doing well and where there are opportunities.\n```\n\n### Review Sentiment Analysis\n```\nHey Claudeâ€”I just added the \"app-store-optimization\" skill. Can you analyze recent reviews for my app (com.myapp.ios) and identify the most common user complaints and feature requests?\n```\n\n### Calculate ASO Score\n```\nHey Claudeâ€”I just added the \"app-store-optimization\" skill. Can you calculate my app's overall ASO health score and provide specific recommendations for improvement?\n```\n\n### Plan A/B Test\n```\nHey Claudeâ€”I just added the \"app-store-optimization\" skill. I want to A/B test my app icon and first screenshot. Can you help me design the test and determine how long to run it?\n```\n\n### Pre-Launch Checklist\n```\nHey Claudeâ€”I just added the \"app-store-optimization\" skill. Can you generate a comprehensive pre-launch checklist for submitting my app to both Apple App Store and Google Play Store?\n```\n\n## Scripts\n\n### keyword_analyzer.py\nAnalyzes keywords for search volume, competition, and relevance. Provides strategic recommendations for primary and secondary keywords.\n\n**Key Functions:**\n- `analyze_keyword()`: Analyze single keyword metrics\n- `compare_keywords()`: Compare multiple keywords\n- `find_long_tail()`: Discover long-tail keyword opportunities\n- `calculate_keyword_difficulty()`: Assess competition level\n\n### metadata_optimizer.py\nOptimizes titles, descriptions, and keyword fields with platform-specific character limit validation.\n\n**Key Functions:**\n- `optimize_title()`: Create compelling, keyword-rich titles\n- `optimize_description()`: Generate conversion-focused descriptions\n- `optimize_keyword_field()`: Maximize Apple's 100-char keyword field\n- `validate_character_limits()`: Ensure compliance with platform limits\n- `calculate_keyword_density()`: Analyze keyword usage in metadata\n\n### competitor_analyzer.py\nAnalyzes top competitors' ASO strategies and identifies opportunities.\n\n**Key Functions:**\n- `get_top_competitors()`: Identify category leaders\n- `analyze_competitor_metadata()`: Extract and analyze competitor keywords\n- `compare_visual_assets()`: Evaluate icons and screenshots\n- `identify_gaps()`: Find competitive opportunities\n\n### aso_scorer.py\nCalculates comprehensive ASO health score across multiple dimensions.\n\n**Key Functions:**\n- `calculate_overall_score()`: Compute 0-100 ASO score\n- `score_metadata_quality()`: Evaluate title, description, keywords\n- `score_ratings_reviews()`: Assess rating quality and volume\n- `score_keyword_performance()`: Analyze ranking positions\n- `score_conversion_metrics()`: Evaluate impression-to-install rates\n- `generate_recommendations()`: Provide prioritized action items\n\n### ab_test_planner.py\nPlans and tracks A/B tests for metadata and visual assets.\n\n**Key Functions:**\n- `design_test()`: Create test hypothesis and variables\n- `calculate_sample_size()`: Determine required test duration\n- `calculate_significance()`: Assess statistical significance\n- `track_results()`: Monitor test performance\n- `generate_report()`: Summarize test outcomes\n\n### localization_helper.py\nManages multi-language ASO optimization strategies.\n\n**Key Functions:**\n- `identify_target_markets()`: Recommend localization priorities\n- `translate_metadata()`: Generate localized metadata\n- `adapt_keywords()`: Research locale-specific keywords\n- `validate_translations()`: Check character limits per language\n- `calculate_localization_roi()`: Estimate impact of localization\n\n### review_analyzer.py\nAnalyzes user reviews for sentiment, issues, and feature requests.\n\n**Key Functions:**\n- `analyze_sentiment()`: Calculate positive/negative/neutral ratios\n- `extract_common_themes()`: Identify frequently mentioned topics\n- `identify_issues()`: Surface bugs and user complaints\n- `find_feature_requests()`: Extract desired features\n- `track_sentiment_trends()`: Monitor sentiment over time\n- `generate_response_templates()`: Create review response drafts\n\n### launch_checklist.py\nGenerates comprehensive pre-launch and update checklists.\n\n**Key Functions:**\n- `generate_prelaunch_checklist()`: Complete submission validation\n- `validate_app_store_compliance()`: Check Apple guidelines\n- `validate_play_store_compliance()`: Check Google policies\n- `create_update_plan()`: Plan update cadence and features\n- `optimize_launch_timing()`: Recommend release dates\n- `plan_seasonal_campaigns()`: Identify seasonal opportunities\n\n## Best Practices\n\n### Keyword Research\n1. **Volume vs. Competition**: Balance high-volume keywords with achievable rankings\n2. **Relevance First**: Only target keywords genuinely relevant to your app\n3. **Long-Tail Strategy**: Include 3-4 word phrases with lower competition\n4. **Continuous Research**: Keyword trends changeâ€”research quarterly\n5. **Competitor Keywords**: Don't copy blindly; ensure relevance to your features\n\n### Metadata Optimization\n1. **Front-Load Keywords**: Place most important keywords early in title/description\n2. **Natural Language**: Write for humans first, SEO second\n3. **Feature Benefits**: Focus on user benefits, not just features\n4. **A/B Test Everything**: Test titles, descriptions, screenshots systematically\n5. **Update Regularly**: Refresh metadata every major update\n6. **Character Limits**: Use every characterâ€”don't waste valuable space\n7. **Apple Keyword Field**: No plurals, duplicates, or spaces between commas\n\n### Visual Assets\n1. **Icon**: Must be recognizable at small sizes (60x60px)\n2. **Screenshots**: First 2-3 are criticalâ€”most users don't scroll\n3. **Captions**: Use screenshot captions to tell your value story\n4. **Consistency**: Match visual style to app design\n5. **A/B Test Icons**: Icon is the single most important visual element\n\n### Reviews & Ratings\n1. **Respond Quickly**: Reply to reviews within 24-48 hours\n2. **Professional Tone**: Always courteous, even with negative reviews\n3. **Address Issues**: Show you're actively fixing reported problems\n4. **Thank Supporters**: Acknowledge positive reviews\n5. **Prompt Strategically**: Ask for ratings after positive experiences\n\n### Launch Strategy\n1. **Soft Launch**: Consider launching in smaller markets first\n2. **PR Timing**: Coordinate press coverage with launch\n3. **Update Frequently**: Initial updates signal active development\n4. **Monitor Closely**: Track metrics daily for first 2 weeks\n5. **Iterate Quickly**: Fix critical issues immediately\n\n### Localization\n1. **Prioritize Markets**: Start with English, Spanish, Chinese, French, German\n2. **Native Speakers**: Use professional translators, not machine translation\n3. **Cultural Adaptation**: Some features resonate differently by culture\n4. **Test Locally**: Have native speakers review before publishing\n5. **Measure ROI**: Track downloads by locale to assess impact\n\n## Limitations\n\n### Data Dependencies\n- Keyword search volume estimates are approximate (no official data from Apple/Google)\n- Competitor data may be incomplete for private apps\n- Review analysis limited to public reviews (can't access private feedback)\n- Historical data may not be available for new apps\n\n### Platform Constraints\n- Apple App Store keyword changes require app submission (except Promotional Text)\n- Google Play Store metadata changes take 1-2 hours to index\n- A/B testing requires significant traffic for statistical significance\n- Store algorithms are proprietary and change without notice\n\n### Industry Variability\n- ASO benchmarks vary significantly by category (games vs. utilities)\n- Seasonality affects different categories differently\n- Geographic markets have different competitive landscapes\n- Cultural preferences impact what works in different countries\n\n### Scope Boundaries\n- Does not include paid user acquisition strategies (Apple Search Ads, Google Ads)\n- Does not cover app development or UI/UX optimization\n- Does not include app analytics implementation (use Firebase, Mixpanel, etc.)\n- Does not handle app submission technical issues (provisioning profiles, certificates)\n\n### When NOT to Use This Skill\n- For web apps (different SEO strategies apply)\n- For enterprise apps not in public stores\n- For apps in beta/TestFlight only\n- If you need paid advertising strategies (use marketing skills instead)\n\n## Integration with Other Skills\n\nThis skill works well with:\n- **Content Strategy Skills**: For creating app descriptions and marketing copy\n- **Analytics Skills**: For analyzing download and engagement data\n- **Localization Skills**: For managing multi-language content\n- **Design Skills**: For creating optimized visual assets\n- **Marketing Skills**: For coordinating broader launch campaigns\n\n## Version & Updates\n\nThis skill is based on current Apple App Store and Google Play Store requirements as of November 2025. Store policies and best practices evolveâ€”verify current requirements before major launches.\n\n**Key Updates to Monitor:**\n- Apple App Store Connect updates (apple.com/app-store/review/guidelines)\n- Google Play Console updates (play.google.com/console/about/guides/releasewithconfidence)\n- iOS/Android version adoption rates (affects device testing)\n- Store algorithm changes (follow ASO blogs and communities)\n",
      "frontmatter": {
        "name": "app-store-optimization",
        "description": "Complete App Store Optimization (ASO) toolkit for researching, optimizing, and tracking mobile app performance on Apple App Store and Google Play Store"
      },
      "content": "\n# App Store Optimization (ASO) Skill\n\nThis comprehensive skill provides complete ASO capabilities for successfully launching and optimizing mobile applications on the Apple App Store and Google Play Store.\n\n## Capabilities\n\n### Research & Analysis\n- **Keyword Research**: Analyze keyword volume, competition, and relevance for app discovery\n- **Competitor Analysis**: Deep-dive into top-performing apps in your category\n- **Market Trend Analysis**: Identify emerging trends and opportunities in your app category\n- **Review Sentiment Analysis**: Extract insights from user reviews to identify strengths and issues\n- **Category Analysis**: Evaluate optimal category and subcategory placement strategies\n\n### Metadata Optimization\n- **Title Optimization**: Create compelling titles with optimal keyword placement (platform-specific character limits)\n- **Description Optimization**: Craft both short and full descriptions that convert and rank\n- **Subtitle/Promotional Text**: Optimize Apple-specific subtitle (30 chars) and promotional text (170 chars)\n- **Keyword Field**: Maximize Apple's 100-character keyword field with strategic selection\n- **Category Selection**: Data-driven recommendations for primary and secondary categories\n- **Icon Best Practices**: Guidelines for designing high-converting app icons\n- **Screenshot Optimization**: Strategies for creating screenshots that drive installs\n- **Preview Video**: Best practices for app preview videos\n- **Localization**: Multi-language optimization strategies for global reach\n\n### Conversion Optimization\n- **A/B Testing Framework**: Plan and track metadata experiments for continuous improvement\n- **Visual Asset Testing**: Test icons, screenshots, and videos for maximum conversion\n- **Store Listing Optimization**: Comprehensive page optimization for impression-to-install conversion\n- **Call-to-Action**: Optimize CTAs in descriptions and promotional materials\n\n### Rating & Review Management\n- **Review Monitoring**: Track and analyze user reviews for actionable insights\n- **Response Strategies**: Templates and best practices for responding to reviews\n- **Rating Improvement**: Tactical approaches to improve app ratings organically\n- **Issue Identification**: Surface common problems and feature requests from reviews\n\n### Launch & Update Strategies\n- **Pre-Launch Checklist**: Complete validation before submitting to stores\n- **Launch Timing**: Optimize release timing for maximum visibility and downloads\n- **Update Cadence**: Plan optimal update frequency and feature rollouts\n- **Feature Announcements**: Craft \"What's New\" sections that re-engage users\n- **Seasonal Optimization**: Leverage seasonal trends and events\n\n### Analytics & Tracking\n- **ASO Score**: Calculate overall ASO health score across multiple factors\n- **Keyword Rankings**: Track keyword position changes over time\n- **Conversion Metrics**: Monitor impression-to-install conversion rates\n- **Download Velocity**: Track download trends and momentum\n- **Performance Benchmarking**: Compare against category averages and competitors\n\n### Platform-Specific Requirements\n- **Apple App Store**:\n  - Title: 30 characters\n  - Subtitle: 30 characters\n  - Promotional Text: 170 characters (editable without app update)\n  - Description: 4,000 characters\n  - Keywords: 100 characters (comma-separated, no spaces)\n  - What's New: 4,000 characters\n- **Google Play Store**:\n  - Title: 50 characters (formerly 30, increased in 2021)\n  - Short Description: 80 characters\n  - Full Description: 4,000 characters\n  - No separate keyword field (keywords extracted from title and description)\n\n## Input Requirements\n\n### Keyword Research\n```json\n{\n  \"app_name\": \"MyApp\",\n  \"category\": \"Productivity\",\n  \"target_keywords\": [\"task manager\", \"productivity\", \"todo list\"],\n  \"competitors\": [\"Todoist\", \"Any.do\", \"Microsoft To Do\"],\n  \"language\": \"en-US\"\n}\n```\n\n### Metadata Optimization\n```json\n{\n  \"platform\": \"apple\" | \"google\",\n  \"app_info\": {\n    \"name\": \"MyApp\",\n    \"category\": \"Productivity\",\n    \"target_audience\": \"Professionals aged 25-45\",\n    \"key_features\": [\"Task management\", \"Team collaboration\", \"AI assistance\"],\n    \"unique_value\": \"AI-powered task prioritization\"\n  },\n  \"current_metadata\": {\n    \"title\": \"Current Title\",\n    \"subtitle\": \"Current Subtitle\",\n    \"description\": \"Current description...\"\n  },\n  \"target_keywords\": [\"productivity\", \"task manager\", \"todo\"]\n}\n```\n\n### Review Analysis\n```json\n{\n  \"app_id\": \"com.myapp.app\",\n  \"platform\": \"apple\" | \"google\",\n  \"date_range\": \"last_30_days\" | \"last_90_days\" | \"all_time\",\n  \"rating_filter\": [1, 2, 3, 4, 5],\n  \"language\": \"en\"\n}\n```\n\n### ASO Score Calculation\n```json\n{\n  \"metadata\": {\n    \"title_quality\": 0.8,\n    \"description_quality\": 0.7,\n    \"keyword_density\": 0.6\n  },\n  \"ratings\": {\n    \"average_rating\": 4.5,\n    \"total_ratings\": 15000\n  },\n  \"conversion\": {\n    \"impression_to_install\": 0.05\n  },\n  \"keyword_rankings\": {\n    \"top_10\": 5,\n    \"top_50\": 12,\n    \"top_100\": 18\n  }\n}\n```\n\n## Output Formats\n\n### Keyword Research Report\n- List of recommended keywords with search volume estimates\n- Competition level analysis (low/medium/high)\n- Relevance scores for each keyword\n- Strategic recommendations for primary vs. secondary keywords\n- Long-tail keyword opportunities\n\n### Optimized Metadata Package\n- Platform-specific title (with character count validation)\n- Subtitle/promotional text (Apple)\n- Short description (Google)\n- Full description (both platforms)\n- Keyword field (Apple - 100 chars)\n- Character count validation for all fields\n- Keyword density analysis\n- Before/after comparison\n\n### Competitor Analysis Report\n- Top 10 competitors in category\n- Their metadata strategies\n- Keyword overlap analysis\n- Visual asset assessment\n- Rating and review volume comparison\n- Identified gaps and opportunities\n\n### ASO Health Score\n- Overall score (0-100)\n- Category breakdown:\n  - Metadata Quality (0-25)\n  - Ratings & Reviews (0-25)\n  - Keyword Performance (0-25)\n  - Conversion Metrics (0-25)\n- Specific improvement recommendations\n- Priority action items\n\n### A/B Test Plan\n- Hypothesis and test variables\n- Test duration recommendations\n- Success metrics definition\n- Sample size calculations\n- Statistical significance thresholds\n\n### Launch Checklist\n- Pre-submission validation (all required assets, metadata)\n- Store compliance verification\n- Testing checklist (devices, OS versions)\n- Marketing preparation items\n- Post-launch monitoring plan\n\n## How to Use\n\n### Keyword Research\n```\nHey Claudeâ€”I just added the \"app-store-optimization\" skill. Can you research the best keywords for a productivity app targeting professionals? Focus on keywords with good search volume but lower competition.\n```\n\n### Optimize App Store Listing\n```\nHey Claudeâ€”I just added the \"app-store-optimization\" skill. Can you optimize my app's metadata for the Apple App Store? Here's my current listing: [provide current metadata]. I want to rank for \"task management\" and \"productivity tools\".\n```\n\n### Analyze Competitor Strategy\n```\nHey Claudeâ€”I just added the \"app-store-optimization\" skill. Can you analyze the ASO strategies of Todoist, Any.do, and Microsoft To Do? I want to understand what they're doing well and where there are opportunities.\n```\n\n### Review Sentiment Analysis\n```\nHey Claudeâ€”I just added the \"app-store-optimization\" skill. Can you analyze recent reviews for my app (com.myapp.ios) and identify the most common user complaints and feature requests?\n```\n\n### Calculate ASO Score\n```\nHey Claudeâ€”I just added the \"app-store-optimization\" skill. Can you calculate my app's overall ASO health score and provide specific recommendations for improvement?\n```\n\n### Plan A/B Test\n```\nHey Claudeâ€”I just added the \"app-store-optimization\" skill. I want to A/B test my app icon and first screenshot. Can you help me design the test and determine how long to run it?\n```\n\n### Pre-Launch Checklist\n```\nHey Claudeâ€”I just added the \"app-store-optimization\" skill. Can you generate a comprehensive pre-launch checklist for submitting my app to both Apple App Store and Google Play Store?\n```\n\n## Scripts\n\n### keyword_analyzer.py\nAnalyzes keywords for search volume, competition, and relevance. Provides strategic recommendations for primary and secondary keywords.\n\n**Key Functions:**\n- `analyze_keyword()`: Analyze single keyword metrics\n- `compare_keywords()`: Compare multiple keywords\n- `find_long_tail()`: Discover long-tail keyword opportunities\n- `calculate_keyword_difficulty()`: Assess competition level\n\n### metadata_optimizer.py\nOptimizes titles, descriptions, and keyword fields with platform-specific character limit validation.\n\n**Key Functions:**\n- `optimize_title()`: Create compelling, keyword-rich titles\n- `optimize_description()`: Generate conversion-focused descriptions\n- `optimize_keyword_field()`: Maximize Apple's 100-char keyword field\n- `validate_character_limits()`: Ensure compliance with platform limits\n- `calculate_keyword_density()`: Analyze keyword usage in metadata\n\n### competitor_analyzer.py\nAnalyzes top competitors' ASO strategies and identifies opportunities.\n\n**Key Functions:**\n- `get_top_competitors()`: Identify category leaders\n- `analyze_competitor_metadata()`: Extract and analyze competitor keywords\n- `compare_visual_assets()`: Evaluate icons and screenshots\n- `identify_gaps()`: Find competitive opportunities\n\n### aso_scorer.py\nCalculates comprehensive ASO health score across multiple dimensions.\n\n**Key Functions:**\n- `calculate_overall_score()`: Compute 0-100 ASO score\n- `score_metadata_quality()`: Evaluate title, description, keywords\n- `score_ratings_reviews()`: Assess rating quality and volume\n- `score_keyword_performance()`: Analyze ranking positions\n- `score_conversion_metrics()`: Evaluate impression-to-install rates\n- `generate_recommendations()`: Provide prioritized action items\n\n### ab_test_planner.py\nPlans and tracks A/B tests for metadata and visual assets.\n\n**Key Functions:**\n- `design_test()`: Create test hypothesis and variables\n- `calculate_sample_size()`: Determine required test duration\n- `calculate_significance()`: Assess statistical significance\n- `track_results()`: Monitor test performance\n- `generate_report()`: Summarize test outcomes\n\n### localization_helper.py\nManages multi-language ASO optimization strategies.\n\n**Key Functions:**\n- `identify_target_markets()`: Recommend localization priorities\n- `translate_metadata()`: Generate localized metadata\n- `adapt_keywords()`: Research locale-specific keywords\n- `validate_translations()`: Check character limits per language\n- `calculate_localization_roi()`: Estimate impact of localization\n\n### review_analyzer.py\nAnalyzes user reviews for sentiment, issues, and feature requests.\n\n**Key Functions:**\n- `analyze_sentiment()`: Calculate positive/negative/neutral ratios\n- `extract_common_themes()`: Identify frequently mentioned topics\n- `identify_issues()`: Surface bugs and user complaints\n- `find_feature_requests()`: Extract desired features\n- `track_sentiment_trends()`: Monitor sentiment over time\n- `generate_response_templates()`: Create review response drafts\n\n### launch_checklist.py\nGenerates comprehensive pre-launch and update checklists.\n\n**Key Functions:**\n- `generate_prelaunch_checklist()`: Complete submission validation\n- `validate_app_store_compliance()`: Check Apple guidelines\n- `validate_play_store_compliance()`: Check Google policies\n- `create_update_plan()`: Plan update cadence and features\n- `optimize_launch_timing()`: Recommend release dates\n- `plan_seasonal_campaigns()`: Identify seasonal opportunities\n\n## Best Practices\n\n### Keyword Research\n1. **Volume vs. Competition**: Balance high-volume keywords with achievable rankings\n2. **Relevance First**: Only target keywords genuinely relevant to your app\n3. **Long-Tail Strategy**: Include 3-4 word phrases with lower competition\n4. **Continuous Research**: Keyword trends changeâ€”research quarterly\n5. **Competitor Keywords**: Don't copy blindly; ensure relevance to your features\n\n### Metadata Optimization\n1. **Front-Load Keywords**: Place most important keywords early in title/description\n2. **Natural Language**: Write for humans first, SEO second\n3. **Feature Benefits**: Focus on user benefits, not just features\n4. **A/B Test Everything**: Test titles, descriptions, screenshots systematically\n5. **Update Regularly**: Refresh metadata every major update\n6. **Character Limits**: Use every characterâ€”don't waste valuable space\n7. **Apple Keyword Field**: No plurals, duplicates, or spaces between commas\n\n### Visual Assets\n1. **Icon**: Must be recognizable at small sizes (60x60px)\n2. **Screenshots**: First 2-3 are criticalâ€”most users don't scroll\n3. **Captions**: Use screenshot captions to tell your value story\n4. **Consistency**: Match visual style to app design\n5. **A/B Test Icons**: Icon is the single most important visual element\n\n### Reviews & Ratings\n1. **Respond Quickly**: Reply to reviews within 24-48 hours\n2. **Professional Tone**: Always courteous, even with negative reviews\n3. **Address Issues**: Show you're actively fixing reported problems\n4. **Thank Supporters**: Acknowledge positive reviews\n5. **Prompt Strategically**: Ask for ratings after positive experiences\n\n### Launch Strategy\n1. **Soft Launch**: Consider launching in smaller markets first\n2. **PR Timing**: Coordinate press coverage with launch\n3. **Update Frequently**: Initial updates signal active development\n4. **Monitor Closely**: Track metrics daily for first 2 weeks\n5. **Iterate Quickly**: Fix critical issues immediately\n\n### Localization\n1. **Prioritize Markets**: Start with English, Spanish, Chinese, French, German\n2. **Native Speakers**: Use professional translators, not machine translation\n3. **Cultural Adaptation**: Some features resonate differently by culture\n4. **Test Locally**: Have native speakers review before publishing\n5. **Measure ROI**: Track downloads by locale to assess impact\n\n## Limitations\n\n### Data Dependencies\n- Keyword search volume estimates are approximate (no official data from Apple/Google)\n- Competitor data may be incomplete for private apps\n- Review analysis limited to public reviews (can't access private feedback)\n- Historical data may not be available for new apps\n\n### Platform Constraints\n- Apple App Store keyword changes require app submission (except Promotional Text)\n- Google Play Store metadata changes take 1-2 hours to index\n- A/B testing requires significant traffic for statistical significance\n- Store algorithms are proprietary and change without notice\n\n### Industry Variability\n- ASO benchmarks vary significantly by category (games vs. utilities)\n- Seasonality affects different categories differently\n- Geographic markets have different competitive landscapes\n- Cultural preferences impact what works in different countries\n\n### Scope Boundaries\n- Does not include paid user acquisition strategies (Apple Search Ads, Google Ads)\n- Does not cover app development or UI/UX optimization\n- Does not include app analytics implementation (use Firebase, Mixpanel, etc.)\n- Does not handle app submission technical issues (provisioning profiles, certificates)\n\n### When NOT to Use This Skill\n- For web apps (different SEO strategies apply)\n- For enterprise apps not in public stores\n- For apps in beta/TestFlight only\n- If you need paid advertising strategies (use marketing skills instead)\n\n## Integration with Other Skills\n\nThis skill works well with:\n- **Content Strategy Skills**: For creating app descriptions and marketing copy\n- **Analytics Skills**: For analyzing download and engagement data\n- **Localization Skills**: For managing multi-language content\n- **Design Skills**: For creating optimized visual assets\n- **Marketing Skills**: For coordinating broader launch campaigns\n\n## Version & Updates\n\nThis skill is based on current Apple App Store and Google Play Store requirements as of November 2025. Store policies and best practices evolveâ€”verify current requirements before major launches.\n\n**Key Updates to Monitor:**\n- Apple App Store Connect updates (apple.com/app-store/review/guidelines)\n- Google Play Console updates (play.google.com/console/about/guides/releasewithconfidence)\n- iOS/Android version adoption rates (affects device testing)\n- Store algorithm changes (follow ASO blogs and communities)\n"
    }
  },
  "alirezarezvani-claude-skills-social-media-analyzer": {
    "id": "alirezarezvani-claude-skills-social-media-analyzer",
    "name": "social-media-analyzer",
    "description": "Analyzes social media campaign performance across platforms with engagement metrics, ROI calculations, and audience insights for data-driven marketing decisions",
    "repo": {
      "owner": "alirezarezvani",
      "name": "claude-skills",
      "fullName": "alirezarezvani/claude-skills",
      "url": "https://github.com/alirezarezvani/claude-skills/tree/main/marketing-skill/social-media-analyzer",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 521,
      "forks": 103,
      "language": "Python",
      "topics": [
        "agentic-ai",
        "agentic-coding",
        "anthropic-claude",
        "claude-ai",
        "claude-ai-skills",
        "claude-code",
        "claude-code-skills",
        "claude-skills",
        "claudecode-subagents"
      ],
      "updatedAt": "2026-01-08T12:36:14Z",
      "pushedAt": "2026-01-08T08:39:21Z",
      "createdAt": "2025-10-19T04:04:05Z",
      "license": "MIT License"
    },
    "category": "marketing",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: social-media-analyzer\ndescription: Analyzes social media campaign performance across platforms with engagement metrics, ROI calculations, and audience insights for data-driven marketing decisions\n---\n\n# Social Media Campaign Analyzer\n\nThis skill provides comprehensive analysis of social media campaign performance, helping marketing agencies deliver actionable insights to clients.\n\n## Capabilities\n\n- **Multi-Platform Analysis**: Track performance across Facebook, Instagram, Twitter, LinkedIn, TikTok\n- **Engagement Metrics**: Calculate engagement rate, reach, impressions, click-through rate\n- **ROI Analysis**: Measure cost per engagement, cost per click, return on ad spend\n- **Audience Insights**: Analyze demographics, peak engagement times, content performance\n- **Trend Detection**: Identify high-performing content types and posting patterns\n- **Competitive Benchmarking**: Compare performance against industry standards\n\n## Input Requirements\n\nCampaign data including:\n- **Platform metrics**: Likes, comments, shares, saves, clicks\n- **Reach data**: Impressions, unique reach, follower growth\n- **Cost data**: Ad spend, campaign budget (for ROI calculations)\n- **Content details**: Post type (image, video, carousel), posting time, hashtags\n- **Time period**: Date range for analysis\n\nFormats accepted:\n- JSON with structured campaign data\n- CSV exports from social media platforms\n- Text descriptions of key metrics\n\n## Output Formats\n\nResults include:\n- **Performance dashboard**: Key metrics with trends\n- **Engagement analysis**: Best and worst performing posts\n- **ROI breakdown**: Cost efficiency metrics\n- **Audience insights**: Demographics and behavior patterns\n- **Recommendations**: Data-driven suggestions for optimization\n- **Visual reports**: Charts and graphs (Excel/PDF format)\n\n## How to Use\n\n\"Analyze this Facebook campaign data and calculate engagement metrics\"\n\"What's the ROI on this Instagram ad campaign with $500 spend and 2,000 clicks?\"\n\"Compare performance across all social platforms for the last month\"\n\n## Scripts\n\n- `calculate_metrics.py`: Core calculation engine for all social media metrics\n- `analyze_performance.py`: Performance analysis and recommendation generation\n\n## Best Practices\n\n1. Ensure data completeness before analysis (missing metrics affect accuracy)\n2. Compare metrics within same time periods for fair comparisons\n3. Consider platform-specific benchmarks (Instagram engagement differs from LinkedIn)\n4. Account for organic vs. paid metrics separately\n5. Track metrics over time to identify trends\n6. Include context (seasonality, campaigns, events) when interpreting results\n\n## Limitations\n\n- Requires accurate data from social media platforms\n- Industry benchmarks are general guidelines and vary by niche\n- Historical data doesn't guarantee future performance\n- Organic reach calculations may vary by platform algorithm changes\n- Cannot access data directly from platforms (requires manual export or API integration)\n- Some platforms limit data availability (e.g., TikTok analytics for business accounts only)\n",
      "frontmatter": {
        "name": "social-media-analyzer",
        "description": "Analyzes social media campaign performance across platforms with engagement metrics, ROI calculations, and audience insights for data-driven marketing decisions"
      },
      "content": "\n# Social Media Campaign Analyzer\n\nThis skill provides comprehensive analysis of social media campaign performance, helping marketing agencies deliver actionable insights to clients.\n\n## Capabilities\n\n- **Multi-Platform Analysis**: Track performance across Facebook, Instagram, Twitter, LinkedIn, TikTok\n- **Engagement Metrics**: Calculate engagement rate, reach, impressions, click-through rate\n- **ROI Analysis**: Measure cost per engagement, cost per click, return on ad spend\n- **Audience Insights**: Analyze demographics, peak engagement times, content performance\n- **Trend Detection**: Identify high-performing content types and posting patterns\n- **Competitive Benchmarking**: Compare performance against industry standards\n\n## Input Requirements\n\nCampaign data including:\n- **Platform metrics**: Likes, comments, shares, saves, clicks\n- **Reach data**: Impressions, unique reach, follower growth\n- **Cost data**: Ad spend, campaign budget (for ROI calculations)\n- **Content details**: Post type (image, video, carousel), posting time, hashtags\n- **Time period**: Date range for analysis\n\nFormats accepted:\n- JSON with structured campaign data\n- CSV exports from social media platforms\n- Text descriptions of key metrics\n\n## Output Formats\n\nResults include:\n- **Performance dashboard**: Key metrics with trends\n- **Engagement analysis**: Best and worst performing posts\n- **ROI breakdown**: Cost efficiency metrics\n- **Audience insights**: Demographics and behavior patterns\n- **Recommendations**: Data-driven suggestions for optimization\n- **Visual reports**: Charts and graphs (Excel/PDF format)\n\n## How to Use\n\n\"Analyze this Facebook campaign data and calculate engagement metrics\"\n\"What's the ROI on this Instagram ad campaign with $500 spend and 2,000 clicks?\"\n\"Compare performance across all social platforms for the last month\"\n\n## Scripts\n\n- `calculate_metrics.py`: Core calculation engine for all social media metrics\n- `analyze_performance.py`: Performance analysis and recommendation generation\n\n## Best Practices\n\n1. Ensure data completeness before analysis (missing metrics affect accuracy)\n2. Compare metrics within same time periods for fair comparisons\n3. Consider platform-specific benchmarks (Instagram engagement differs from LinkedIn)\n4. Account for organic vs. paid metrics separately\n5. Track metrics over time to identify trends\n6. Include context (seasonality, campaigns, events) when interpreting results\n\n## Limitations\n\n- Requires accurate data from social media platforms\n- Industry benchmarks are general guidelines and vary by niche\n- Historical data doesn't guarantee future performance\n- Organic reach calculations may vary by platform algorithm changes\n- Cannot access data directly from platforms (requires manual export or API integration)\n- Some platforms limit data availability (e.g., TikTok analytics for business accounts only)\n"
    }
  },
  "alirezarezvani-claude-skills-ceo-advisor": {
    "id": "alirezarezvani-claude-skills-ceo-advisor",
    "name": "ceo-advisor",
    "description": "Executive leadership guidance for strategic decision-making, organizational development, and stakeholder management. Includes strategy analyzer, financial scenario modeling, board governance frameworks, and investor relations playbooks. Use when planning strategy, preparing board presentations, managing investors, developing organizational culture, making executive decisions, or when user mentions CEO, strategic planning, board meetings, investor updates, organizational leadership, or executive strategy.",
    "repo": {
      "owner": "alirezarezvani",
      "name": "claude-skills",
      "fullName": "alirezarezvani/claude-skills",
      "url": "https://github.com/alirezarezvani/claude-skills/tree/main/c-level-advisor/ceo-advisor",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 521,
      "forks": 103,
      "language": "Python",
      "topics": [
        "agentic-ai",
        "agentic-coding",
        "anthropic-claude",
        "claude-ai",
        "claude-ai-skills",
        "claude-code",
        "claude-code-skills",
        "claude-skills",
        "claudecode-subagents"
      ],
      "updatedAt": "2026-01-08T12:36:14Z",
      "pushedAt": "2026-01-08T08:39:21Z",
      "createdAt": "2025-10-19T04:04:05Z",
      "license": "MIT License"
    },
    "category": "business",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: ceo-advisor\ndescription: Executive leadership guidance for strategic decision-making, organizational development, and stakeholder management. Includes strategy analyzer, financial scenario modeling, board governance frameworks, and investor relations playbooks. Use when planning strategy, preparing board presentations, managing investors, developing organizational culture, making executive decisions, or when user mentions CEO, strategic planning, board meetings, investor updates, organizational leadership, or executive strategy.\nlicense: MIT\nmetadata:\n  version: 1.0.0\n  author: Alireza Rezvani\n  category: c-level\n  domain: ceo-leadership\n  updated: 2025-10-20\n  python-tools: strategy_analyzer.py, financial_scenario_analyzer.py\n  frameworks: executive-decision-framework, board-governance, investor-relations\n---\n\n# CEO Advisor\n\nStrategic frameworks and tools for chief executive leadership, organizational transformation, and stakeholder management.\n\n## Keywords\nCEO, chief executive officer, executive leadership, strategic planning, board governance, investor relations, board meetings, board presentations, financial modeling, strategic decisions, organizational culture, company culture, leadership development, stakeholder management, executive strategy, crisis management, organizational transformation, investor updates, strategic initiatives, company vision\n\n## Quick Start\n\n### For Strategic Planning\n```bash\npython scripts/strategy_analyzer.py\n```\nAnalyzes strategic position and generates actionable recommendations.\n\n### For Financial Scenarios\n```bash\npython scripts/financial_scenario_analyzer.py\n```\nModels different business scenarios with risk-adjusted projections.\n\n### For Decision Making\nReview `references/executive_decision_framework.md` for structured decision processes.\n\n### For Board Management\nUse templates in `references/board_governance_investor_relations.md` for board packages.\n\n### For Culture Building\nImplement frameworks from `references/leadership_organizational_culture.md` for transformation.\n\n## Core CEO Responsibilities\n\n### 1. Vision & Strategy\n\n#### Setting Direction\n- **Vision Development**: Define 10-year aspirational future\n- **Mission Articulation**: Clear purpose and why we exist\n- **Strategy Formulation**: 3-5 year competitive positioning\n- **Value Definition**: Core beliefs and principles\n\n#### Strategic Planning Cycle\n```\nQ1: Environmental Scan\n- Market analysis\n- Competitive intelligence\n- Technology trends\n- Regulatory landscape\n\nQ2: Strategy Development\n- Strategic options generation\n- Scenario planning\n- Resource allocation\n- Risk assessment\n\nQ3: Planning & Budgeting\n- Annual operating plan\n- Budget allocation\n- OKR setting\n- Initiative prioritization\n\nQ4: Communication & Launch\n- Board approval\n- Investor communication\n- Employee cascade\n- Partner alignment\n```\n\n### 2. Capital & Resource Management\n\n#### Capital Allocation Framework\n```python\n# Run financial scenario analysis\npython scripts/financial_scenario_analyzer.py\n\n# Allocation priorities:\n1. Core Operations (40-50%)\n2. Growth Investments (25-35%)\n3. Innovation/R&D (10-15%)\n4. Strategic Reserve (10-15%)\n5. Shareholder Returns (varies)\n```\n\n#### Fundraising Strategy\n- **Seed/Series A**: Product-market fit focus\n- **Series B/C**: Growth acceleration\n- **Late Stage**: Market expansion\n- **IPO**: Public market access\n- **Debt**: Non-dilutive growth\n\n### 3. Stakeholder Leadership\n\n#### Stakeholder Priority Matrix\n```\n         Influence â†’\n         Low        High\n    High â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nInterest â”‚ Keep    â”‚ Manage  â”‚\n    â†‘    â”‚Informed â”‚ Closely â”‚\n         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n    Low  â”‚Monitor  â”‚  Keep   â”‚\n         â”‚         â”‚Satisfiedâ”‚\n         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nPrimary Stakeholders:\n- Board of Directors\n- Investors\n- Employees\n- Customers\n\nSecondary Stakeholders:\n- Partners\n- Community\n- Media\n- Regulators\n```\n\n### 4. Organizational Leadership\n\n#### Culture Development\nFrom `references/leadership_organizational_culture.md`:\n\n**Culture Transformation Timeline**:\n- Months 1-2: Assessment\n- Months 2-3: Design\n- Months 4-12: Implementation\n- Months 12+: Embedding\n\n**Key Levers**:\n- Leadership modeling\n- Communication\n- Systems alignment\n- Recognition\n- Accountability\n\n### 5. External Representation\n\n#### CEO Communication Calendar\n\n**Daily**:\n- Customer touchpoint\n- Team check-in\n- Metric review\n\n**Weekly**:\n- Executive team meeting\n- Board member update\n- Key customer/partner call\n- Media opportunity\n\n**Monthly**:\n- All-hands meeting\n- Board report\n- Investor update\n- Industry engagement\n\n**Quarterly**:\n- Board meeting\n- Earnings call\n- Strategy review\n- Town hall\n\n## Executive Routines\n\n### Daily CEO Schedule Template\n\n```\n6:00 AM - Personal development (reading, exercise)\n7:00 AM - Day planning & priority review\n8:00 AM - Metric dashboard review\n8:30 AM - Customer/market intelligence\n9:00 AM - Strategic work block\n10:30 AM - Meetings block\n12:00 PM - Lunch (networking/thinking)\n1:00 PM - External meetings\n3:00 PM - Internal meetings\n4:30 PM - Email/communication\n5:30 PM - Team walk-around\n6:00 PM - Transition/reflection\n```\n\n### Weekly Leadership Rhythm\n\n**Monday**: Strategy & Planning\n- Executive team meeting\n- Metrics review\n- Week planning\n\n**Tuesday**: External Focus\n- Customer meetings\n- Partner discussions\n- Investor relations\n\n**Wednesday**: Operations\n- Deep dives\n- Problem solving\n- Process review\n\n**Thursday**: People & Culture\n- 1-on-1s\n- Talent reviews\n- Culture initiatives\n\n**Friday**: Innovation & Future\n- Strategic projects\n- Learning time\n- Planning ahead\n\n## Critical CEO Decisions\n\n### Go/No-Go Decision Framework\n\nUse framework from `references/executive_decision_framework.md`:\n\n**Major Decisions Requiring Framework**:\n- M&A opportunities\n- Market expansion\n- Major pivots\n- Large investments\n- Restructuring\n- Leadership changes\n\n**Decision Checklist**:\n- [ ] Problem clearly defined\n- [ ] Data/evidence gathered\n- [ ] Options evaluated\n- [ ] Stakeholders consulted\n- [ ] Risks assessed\n- [ ] Implementation planned\n- [ ] Success metrics defined\n- [ ] Communication prepared\n\n### Crisis Management\n\n#### Crisis Leadership Playbook\n\n**Level 1 Crisis** (Department)\n- Monitor situation\n- Support as needed\n- Review afterwards\n\n**Level 2 Crisis** (Company)\n- Activate crisis team\n- Lead response\n- Communicate frequently\n\n**Level 3 Crisis** (Existential)\n- Take direct control\n- Board engagement\n- All-hands focus\n- External communication\n\n## Board Management\n\n### Board Meeting Success\n\nFrom `references/board_governance_investor_relations.md`:\n\n**Preparation Timeline**:\n- T-4 weeks: Agenda development\n- T-2 weeks: Material preparation\n- T-1 week: Package distribution\n- T-0: Meeting execution\n\n**Board Package Components**:\n1. CEO Letter (1-2 pages)\n2. Dashboard (1 page)\n3. Financial review (5 pages)\n4. Strategic updates (10 pages)\n5. Risk register (2 pages)\n6. Appendices\n\n### Managing Board Dynamics\n\n**Building Trust**:\n- Regular communication\n- No surprises\n- Transparency\n- Follow-through\n- Respect expertise\n\n**Difficult Conversations**:\n- Prepare thoroughly\n- Lead with facts\n- Own responsibility\n- Present solutions\n- Seek alignment\n\n## Investor Relations\n\n### Investor Communication\n\n**Earnings Cycle**:\n1. Pre-announcement quiet period\n2. Earnings release\n3. Conference call\n4. Follow-up meetings\n5. Conference participation\n\n**Key Messages**:\n- Growth trajectory\n- Competitive position\n- Financial performance\n- Strategic progress\n- Future outlook\n\n### Fundraising Excellence\n\n**Pitch Deck Structure**:\n1. Problem (1 slide)\n2. Solution (1-2 slides)\n3. Market (1-2 slides)\n4. Product (2-3 slides)\n5. Business Model (1 slide)\n6. Go-to-Market (1-2 slides)\n7. Competition (1 slide)\n8. Team (1 slide)\n9. Financials (2 slides)\n10. Ask (1 slide)\n\n## Performance Management\n\n### Company Scorecard\n\n**Financial Metrics**:\n- Revenue growth\n- Gross margin\n- EBITDA\n- Cash flow\n- Runway\n\n**Customer Metrics**:\n- Acquisition\n- Retention\n- NPS\n- LTV/CAC\n\n**Operational Metrics**:\n- Productivity\n- Quality\n- Efficiency\n- Innovation\n\n**People Metrics**:\n- Engagement\n- Retention\n- Diversity\n- Development\n\n### CEO Self-Assessment\n\n**Quarterly Reflection**:\n- What went well?\n- What could improve?\n- Key learnings?\n- Priority adjustments?\n\n**Annual 360 Review**:\n- Board feedback\n- Executive team input\n- Skip-level insights\n- Self-evaluation\n- Development plan\n\n## Succession Planning\n\n### CEO Succession Timeline\n\n**Ongoing**:\n- Identify internal candidates\n- Develop high potentials\n- External benchmarking\n\n**T-3 Years**:\n- Formal succession planning\n- Candidate assessment\n- Development acceleration\n\n**T-1 Year**:\n- Final selection\n- Transition planning\n- Communication strategy\n\n**Transition**:\n- Knowledge transfer\n- Stakeholder handoff\n- Gradual transition\n\n## Personal Development\n\n### CEO Learning Agenda\n\n**Core Competencies**:\n- Strategic thinking\n- Financial acumen\n- Leadership presence\n- Communication\n- Decision making\n\n**Development Activities**:\n- Executive coaching\n- Peer networking (YPO/EO)\n- Board service\n- Industry involvement\n- Continuous education\n\n### Work-Life Integration\n\n**Sustainability Practices**:\n- Protected family time\n- Exercise routine\n- Mental health support\n- Vacation planning\n- Delegation discipline\n\n**Energy Management**:\n- Know peak hours\n- Block deep work time\n- Batch similar tasks\n- Take breaks\n- Reflect daily\n\n## Tools & Resources\n\n### Essential CEO Tools\n\n**Strategy & Planning**:\n- Strategy frameworks (Porter, BCG, McKinsey)\n- Scenario planning tools\n- OKR management systems\n\n**Financial Management**:\n- Financial modeling\n- Cap table management\n- Investor CRM\n\n**Communication**:\n- Board portal\n- Investor relations platform\n- Employee communication tools\n\n**Personal Productivity**:\n- Calendar management\n- Task management\n- Note-taking system\n\n### Key Resources\n\n**Books**:\n- \"Good to Great\" - Jim Collins\n- \"The Hard Thing About Hard Things\" - Ben Horowitz\n- \"High Output Management\" - Andy Grove\n- \"The Lean Startup\" - Eric Ries\n\n**Frameworks**:\n- Jobs-to-be-Done\n- Blue Ocean Strategy\n- Balanced Scorecard\n- OKRs\n\n**Networks**:\n- YPO (Young Presidents' Organization)\n- EO (Entrepreneurs' Organization)\n- Industry associations\n- CEO peer groups\n\n## Success Metrics\n\n### CEO Effectiveness Indicators\n\nâœ… **Strategic Success**\n- Vision clarity and buy-in\n- Strategy execution on track\n- Market position improving\n- Innovation pipeline strong\n\nâœ… **Financial Success**\n- Revenue growth targets met\n- Profitability improving\n- Cash position strong\n- Valuation increasing\n\nâœ… **Organizational Success**\n- Culture thriving\n- Talent retained\n- Engagement high\n- Succession ready\n\nâœ… **Stakeholder Success**\n- Board confidence high\n- Investor satisfaction\n- Customer NPS strong\n- Employee approval rating\n\n## Red Flags\n\nâš ï¸ Missing targets consistently  \nâš ï¸ High executive turnover  \nâš ï¸ Board relationship strained  \nâš ï¸ Culture deteriorating  \nâš ï¸ Market share declining  \nâš ï¸ Cash burn increasing  \nâš ï¸ Innovation stalling  \nâš ï¸ Personal burnout signs\n",
      "frontmatter": {
        "name": "ceo-advisor",
        "description": "Executive leadership guidance for strategic decision-making, organizational development, and stakeholder management. Includes strategy analyzer, financial scenario modeling, board governance frameworks, and investor relations playbooks. Use when planning strategy, preparing board presentations, managing investors, developing organizational culture, making executive decisions, or when user mentions CEO, strategic planning, board meetings, investor updates, organizational leadership, or executive strategy.",
        "license": "MIT",
        "metadata": {
          "version": "1.0.0",
          "author": "Alireza Rezvani",
          "category": "c-level",
          "domain": "ceo-leadership",
          "updated": "2025-10-20T00:00:00.000Z",
          "python-tools": "strategy_analyzer.py, financial_scenario_analyzer.py",
          "frameworks": "executive-decision-framework, board-governance, investor-relations"
        }
      },
      "content": "\n# CEO Advisor\n\nStrategic frameworks and tools for chief executive leadership, organizational transformation, and stakeholder management.\n\n## Keywords\nCEO, chief executive officer, executive leadership, strategic planning, board governance, investor relations, board meetings, board presentations, financial modeling, strategic decisions, organizational culture, company culture, leadership development, stakeholder management, executive strategy, crisis management, organizational transformation, investor updates, strategic initiatives, company vision\n\n## Quick Start\n\n### For Strategic Planning\n```bash\npython scripts/strategy_analyzer.py\n```\nAnalyzes strategic position and generates actionable recommendations.\n\n### For Financial Scenarios\n```bash\npython scripts/financial_scenario_analyzer.py\n```\nModels different business scenarios with risk-adjusted projections.\n\n### For Decision Making\nReview `references/executive_decision_framework.md` for structured decision processes.\n\n### For Board Management\nUse templates in `references/board_governance_investor_relations.md` for board packages.\n\n### For Culture Building\nImplement frameworks from `references/leadership_organizational_culture.md` for transformation.\n\n## Core CEO Responsibilities\n\n### 1. Vision & Strategy\n\n#### Setting Direction\n- **Vision Development**: Define 10-year aspirational future\n- **Mission Articulation**: Clear purpose and why we exist\n- **Strategy Formulation**: 3-5 year competitive positioning\n- **Value Definition**: Core beliefs and principles\n\n#### Strategic Planning Cycle\n```\nQ1: Environmental Scan\n- Market analysis\n- Competitive intelligence\n- Technology trends\n- Regulatory landscape\n\nQ2: Strategy Development\n- Strategic options generation\n- Scenario planning\n- Resource allocation\n- Risk assessment\n\nQ3: Planning & Budgeting\n- Annual operating plan\n- Budget allocation\n- OKR setting\n- Initiative prioritization\n\nQ4: Communication & Launch\n- Board approval\n- Investor communication\n- Employee cascade\n- Partner alignment\n```\n\n### 2. Capital & Resource Management\n\n#### Capital Allocation Framework\n```python\n# Run financial scenario analysis\npython scripts/financial_scenario_analyzer.py\n\n# Allocation priorities:\n1. Core Operations (40-50%)\n2. Growth Investments (25-35%)\n3. Innovation/R&D (10-15%)\n4. Strategic Reserve (10-15%)\n5. Shareholder Returns (varies)\n```\n\n#### Fundraising Strategy\n- **Seed/Series A**: Product-market fit focus\n- **Series B/C**: Growth acceleration\n- **Late Stage**: Market expansion\n- **IPO**: Public market access\n- **Debt**: Non-dilutive growth\n\n### 3. Stakeholder Leadership\n\n#### Stakeholder Priority Matrix\n```\n         Influence â†’\n         Low        High\n    High â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nInterest â”‚ Keep    â”‚ Manage  â”‚\n    â†‘    â”‚Informed â”‚ Closely â”‚\n         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n    Low  â”‚Monitor  â”‚  Keep   â”‚\n         â”‚         â”‚Satisfiedâ”‚\n         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nPrimary Stakeholders:\n- Board of Directors\n- Investors\n- Employees\n- Customers\n\nSecondary Stakeholders:\n- Partners\n- Community\n- Media\n- Regulators\n```\n\n### 4. Organizational Leadership\n\n#### Culture Development\nFrom `references/leadership_organizational_culture.md`:\n\n**Culture Transformation Timeline**:\n- Months 1-2: Assessment\n- Months 2-3: Design\n- Months 4-12: Implementation\n- Months 12+: Embedding\n\n**Key Levers**:\n- Leadership modeling\n- Communication\n- Systems alignment\n- Recognition\n- Accountability\n\n### 5. External Representation\n\n#### CEO Communication Calendar\n\n**Daily**:\n- Customer touchpoint\n- Team check-in\n- Metric review\n\n**Weekly**:\n- Executive team meeting\n- Board member update\n- Key customer/partner call\n- Media opportunity\n\n**Monthly**:\n- All-hands meeting\n- Board report\n- Investor update\n- Industry engagement\n\n**Quarterly**:\n- Board meeting\n- Earnings call\n- Strategy review\n- Town hall\n\n## Executive Routines\n\n### Daily CEO Schedule Template\n\n```\n6:00 AM - Personal development (reading, exercise)\n7:00 AM - Day planning & priority review\n8:00 AM - Metric dashboard review\n8:30 AM - Customer/market intelligence\n9:00 AM - Strategic work block\n10:30 AM - Meetings block\n12:00 PM - Lunch (networking/thinking)\n1:00 PM - External meetings\n3:00 PM - Internal meetings\n4:30 PM - Email/communication\n5:30 PM - Team walk-around\n6:00 PM - Transition/reflection\n```\n\n### Weekly Leadership Rhythm\n\n**Monday**: Strategy & Planning\n- Executive team meeting\n- Metrics review\n- Week planning\n\n**Tuesday**: External Focus\n- Customer meetings\n- Partner discussions\n- Investor relations\n\n**Wednesday**: Operations\n- Deep dives\n- Problem solving\n- Process review\n\n**Thursday**: People & Culture\n- 1-on-1s\n- Talent reviews\n- Culture initiatives\n\n**Friday**: Innovation & Future\n- Strategic projects\n- Learning time\n- Planning ahead\n\n## Critical CEO Decisions\n\n### Go/No-Go Decision Framework\n\nUse framework from `references/executive_decision_framework.md`:\n\n**Major Decisions Requiring Framework**:\n- M&A opportunities\n- Market expansion\n- Major pivots\n- Large investments\n- Restructuring\n- Leadership changes\n\n**Decision Checklist**:\n- [ ] Problem clearly defined\n- [ ] Data/evidence gathered\n- [ ] Options evaluated\n- [ ] Stakeholders consulted\n- [ ] Risks assessed\n- [ ] Implementation planned\n- [ ] Success metrics defined\n- [ ] Communication prepared\n\n### Crisis Management\n\n#### Crisis Leadership Playbook\n\n**Level 1 Crisis** (Department)\n- Monitor situation\n- Support as needed\n- Review afterwards\n\n**Level 2 Crisis** (Company)\n- Activate crisis team\n- Lead response\n- Communicate frequently\n\n**Level 3 Crisis** (Existential)\n- Take direct control\n- Board engagement\n- All-hands focus\n- External communication\n\n## Board Management\n\n### Board Meeting Success\n\nFrom `references/board_governance_investor_relations.md`:\n\n**Preparation Timeline**:\n- T-4 weeks: Agenda development\n- T-2 weeks: Material preparation\n- T-1 week: Package distribution\n- T-0: Meeting execution\n\n**Board Package Components**:\n1. CEO Letter (1-2 pages)\n2. Dashboard (1 page)\n3. Financial review (5 pages)\n4. Strategic updates (10 pages)\n5. Risk register (2 pages)\n6. Appendices\n\n### Managing Board Dynamics\n\n**Building Trust**:\n- Regular communication\n- No surprises\n- Transparency\n- Follow-through\n- Respect expertise\n\n**Difficult Conversations**:\n- Prepare thoroughly\n- Lead with facts\n- Own responsibility\n- Present solutions\n- Seek alignment\n\n## Investor Relations\n\n### Investor Communication\n\n**Earnings Cycle**:\n1. Pre-announcement quiet period\n2. Earnings release\n3. Conference call\n4. Follow-up meetings\n5. Conference participation\n\n**Key Messages**:\n- Growth trajectory\n- Competitive position\n- Financial performance\n- Strategic progress\n- Future outlook\n\n### Fundraising Excellence\n\n**Pitch Deck Structure**:\n1. Problem (1 slide)\n2. Solution (1-2 slides)\n3. Market (1-2 slides)\n4. Product (2-3 slides)\n5. Business Model (1 slide)\n6. Go-to-Market (1-2 slides)\n7. Competition (1 slide)\n8. Team (1 slide)\n9. Financials (2 slides)\n10. Ask (1 slide)\n\n## Performance Management\n\n### Company Scorecard\n\n**Financial Metrics**:\n- Revenue growth\n- Gross margin\n- EBITDA\n- Cash flow\n- Runway\n\n**Customer Metrics**:\n- Acquisition\n- Retention\n- NPS\n- LTV/CAC\n\n**Operational Metrics**:\n- Productivity\n- Quality\n- Efficiency\n- Innovation\n\n**People Metrics**:\n- Engagement\n- Retention\n- Diversity\n- Development\n\n### CEO Self-Assessment\n\n**Quarterly Reflection**:\n- What went well?\n- What could improve?\n- Key learnings?\n- Priority adjustments?\n\n**Annual 360 Review**:\n- Board feedback\n- Executive team input\n- Skip-level insights\n- Self-evaluation\n- Development plan\n\n## Succession Planning\n\n### CEO Succession Timeline\n\n**Ongoing**:\n- Identify internal candidates\n- Develop high potentials\n- External benchmarking\n\n**T-3 Years**:\n- Formal succession planning\n- Candidate assessment\n- Development acceleration\n\n**T-1 Year**:\n- Final selection\n- Transition planning\n- Communication strategy\n\n**Transition**:\n- Knowledge transfer\n- Stakeholder handoff\n- Gradual transition\n\n## Personal Development\n\n### CEO Learning Agenda\n\n**Core Competencies**:\n- Strategic thinking\n- Financial acumen\n- Leadership presence\n- Communication\n- Decision making\n\n**Development Activities**:\n- Executive coaching\n- Peer networking (YPO/EO)\n- Board service\n- Industry involvement\n- Continuous education\n\n### Work-Life Integration\n\n**Sustainability Practices**:\n- Protected family time\n- Exercise routine\n- Mental health support\n- Vacation planning\n- Delegation discipline\n\n**Energy Management**:\n- Know peak hours\n- Block deep work time\n- Batch similar tasks\n- Take breaks\n- Reflect daily\n\n## Tools & Resources\n\n### Essential CEO Tools\n\n**Strategy & Planning**:\n- Strategy frameworks (Porter, BCG, McKinsey)\n- Scenario planning tools\n- OKR management systems\n\n**Financial Management**:\n- Financial modeling\n- Cap table management\n- Investor CRM\n\n**Communication**:\n- Board portal\n- Investor relations platform\n- Employee communication tools\n\n**Personal Productivity**:\n- Calendar management\n- Task management\n- Note-taking system\n\n### Key Resources\n\n**Books**:\n- \"Good to Great\" - Jim Collins\n- \"The Hard Thing About Hard Things\" - Ben Horowitz\n- \"High Output Management\" - Andy Grove\n- \"The Lean Startup\" - Eric Ries\n\n**Frameworks**:\n- Jobs-to-be-Done\n- Blue Ocean Strategy\n- Balanced Scorecard\n- OKRs\n\n**Networks**:\n- YPO (Young Presidents' Organization)\n- EO (Entrepreneurs' Organization)\n- Industry associations\n- CEO peer groups\n\n## Success Metrics\n\n### CEO Effectiveness Indicators\n\nâœ… **Strategic Success**\n- Vision clarity and buy-in\n- Strategy execution on track\n- Market position improving\n- Innovation pipeline strong\n\nâœ… **Financial Success**\n- Revenue growth targets met\n- Profitability improving\n- Cash position strong\n- Valuation increasing\n\nâœ… **Organizational Success**\n- Culture thriving\n- Talent retained\n- Engagement high\n- Succession ready\n\nâœ… **Stakeholder Success**\n- Board confidence high\n- Investor satisfaction\n- Customer NPS strong\n- Employee approval rating\n\n## Red Flags\n\nâš ï¸ Missing targets consistently  \nâš ï¸ High executive turnover  \nâš ï¸ Board relationship strained  \nâš ï¸ Culture deteriorating  \nâš ï¸ Market share declining  \nâš ï¸ Cash burn increasing  \nâš ï¸ Innovation stalling  \nâš ï¸ Personal burnout signs\n"
    }
  },
  "alirezarezvani-claude-skills-cto-advisor": {
    "id": "alirezarezvani-claude-skills-cto-advisor",
    "name": "cto-advisor",
    "description": "Technical leadership guidance for engineering teams, architecture decisions, and technology strategy. Includes tech debt analyzer, team scaling calculator, engineering metrics frameworks, technology evaluation tools, and ADR templates. Use when assessing technical debt, scaling engineering teams, evaluating technologies, making architecture decisions, establishing engineering metrics, or when user mentions CTO, tech debt, technical debt, team scaling, architecture decisions, technology evaluation, engineering metrics, DORA metrics, or technology strategy.",
    "repo": {
      "owner": "alirezarezvani",
      "name": "claude-skills",
      "fullName": "alirezarezvani/claude-skills",
      "url": "https://github.com/alirezarezvani/claude-skills/tree/main/c-level-advisor/cto-advisor",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 521,
      "forks": 103,
      "language": "Python",
      "topics": [
        "agentic-ai",
        "agentic-coding",
        "anthropic-claude",
        "claude-ai",
        "claude-ai-skills",
        "claude-code",
        "claude-code-skills",
        "claude-skills",
        "claudecode-subagents"
      ],
      "updatedAt": "2026-01-08T12:36:14Z",
      "pushedAt": "2026-01-08T08:39:21Z",
      "createdAt": "2025-10-19T04:04:05Z",
      "license": "MIT License"
    },
    "category": "business",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: cto-advisor\ndescription: Technical leadership guidance for engineering teams, architecture decisions, and technology strategy. Includes tech debt analyzer, team scaling calculator, engineering metrics frameworks, technology evaluation tools, and ADR templates. Use when assessing technical debt, scaling engineering teams, evaluating technologies, making architecture decisions, establishing engineering metrics, or when user mentions CTO, tech debt, technical debt, team scaling, architecture decisions, technology evaluation, engineering metrics, DORA metrics, or technology strategy.\nlicense: MIT\nmetadata:\n  version: 1.0.0\n  author: Alireza Rezvani\n  category: c-level\n  domain: cto-leadership\n  updated: 2025-10-20\n  python-tools: tech_debt_analyzer.py, team_scaling_calculator.py\n  frameworks: DORA-metrics, architecture-decision-records, engineering-metrics\n  tech-stack: engineering-management, team-organization\n---\n\n# CTO Advisor\n\nStrategic frameworks and tools for technology leadership, team scaling, and engineering excellence.\n\n## Keywords\nCTO, chief technology officer, technical leadership, tech debt, technical debt, engineering team, team scaling, architecture decisions, technology evaluation, engineering metrics, DORA metrics, ADR, architecture decision records, technology strategy, engineering leadership, engineering organization, team structure, hiring plan, technical strategy, vendor evaluation, technology selection\n\n## Quick Start\n\n### For Technical Debt Assessment\n```bash\npython scripts/tech_debt_analyzer.py\n```\nAnalyzes system architecture and provides prioritized debt reduction plan.\n\n### For Team Scaling Planning\n```bash\npython scripts/team_scaling_calculator.py\n```\nCalculates optimal hiring plan and team structure for growth.\n\n### For Architecture Decisions\nReview `references/architecture_decision_records.md` for ADR templates and examples.\n\n### For Technology Evaluation\nUse framework in `references/technology_evaluation_framework.md` for vendor selection.\n\n### For Engineering Metrics\nImplement KPIs from `references/engineering_metrics.md` for team performance tracking.\n\n## Core Responsibilities\n\n### 1. Technology Strategy\n\n#### Vision & Roadmap\n- Define 3-5 year technology vision\n- Create quarterly roadmaps\n- Align with business strategy\n- Communicate to stakeholders\n\n#### Innovation Management\n- Allocate 20% time for innovation\n- Run hackathons quarterly\n- Evaluate emerging technologies\n- Build proof of concepts\n\n#### Technical Debt Strategy\n```bash\n# Assess current debt\npython scripts/tech_debt_analyzer.py\n\n# Allocate capacity\n- Critical debt: 40% capacity\n- High debt: 25% capacity  \n- Medium debt: 15% capacity\n- Low debt: Ongoing maintenance\n```\n\n### 2. Team Leadership\n\n#### Scaling Engineering\n```bash\n# Calculate scaling needs\npython scripts/team_scaling_calculator.py\n\n# Key ratios to maintain:\n- Manager:Engineer = 1:8\n- Senior:Mid:Junior = 3:4:2\n- Product:Engineering = 1:10\n- QA:Engineering = 1.5:10\n```\n\n#### Performance Management\n- Set clear OKRs quarterly\n- Conduct 1:1s weekly\n- Review performance quarterly\n- Provide growth opportunities\n\n#### Culture Building\n- Define engineering values\n- Establish coding standards\n- Create learning programs\n- Foster collaboration\n\n### 3. Architecture Governance\n\n#### Decision Making\nUse ADR template from `references/architecture_decision_records.md`:\n1. Document context and problem\n2. List all options considered\n3. Record decision and rationale\n4. Track consequences\n\n#### Technology Standards\n- Language choices\n- Framework selection\n- Database standards\n- Security requirements\n- API design guidelines\n\n#### System Design Review\n- Weekly architecture reviews\n- Design documentation standards\n- Prototype requirements\n- Performance criteria\n\n### 4. Vendor Management\n\n#### Evaluation Process\nFollow framework in `references/technology_evaluation_framework.md`:\n1. Gather requirements (Week 1)\n2. Market research (Week 1-2)\n3. Deep evaluation (Week 2-4)\n4. Decision and documentation (Week 4)\n\n#### Vendor Relationships\n- Quarterly business reviews\n- SLA monitoring\n- Cost optimization\n- Strategic partnerships\n\n### 5. Engineering Excellence\n\n#### Metrics Implementation\nFrom `references/engineering_metrics.md`:\n\n**DORA Metrics** (Deploy to production targets):\n- Deployment Frequency: >1/day\n- Lead Time: <1 day\n- MTTR: <1 hour\n- Change Failure Rate: <15%\n\n**Quality Metrics**:\n- Test Coverage: >80%\n- Code Review: 100%\n- Technical Debt: <10%\n\n**Team Health**:\n- Sprint Velocity: Â±10% variance\n- Unplanned Work: <20%\n- On-call Incidents: <5/week\n\n## Weekly Cadence\n\n### Monday\n- Leadership team sync\n- Review metrics dashboard\n- Address escalations\n\n### Tuesday\n- Architecture review\n- Technical interviews\n- 1:1s with directs\n\n### Wednesday\n- Cross-functional meetings\n- Vendor meetings\n- Strategy work\n\n### Thursday\n- Team all-hands (monthly)\n- Sprint reviews (bi-weekly)\n- Technical deep dives\n\n### Friday\n- Strategic planning\n- Innovation time\n- Week recap and planning\n\n## Quarterly Planning\n\n### Q1 Focus: Foundation\n- Annual planning\n- Budget allocation\n- Team goal setting\n- Technology assessment\n\n### Q2 Focus: Execution\n- Major initiatives launch\n- Mid-year hiring push\n- Performance reviews\n- Architecture evolution\n\n### Q3 Focus: Innovation\n- Hackathon\n- Technology exploration\n- Team development\n- Process optimization\n\n### Q4 Focus: Planning\n- Next year strategy\n- Budget planning\n- Promotion cycles\n- Debt reduction sprint\n\n## Crisis Management\n\n### Incident Response\n1. **Immediate** (0-15 min):\n   - Assess severity\n   - Activate incident team\n   - Begin communication\n\n2. **Short-term** (15-60 min):\n   - Implement fixes\n   - Update stakeholders\n   - Monitor systems\n\n3. **Resolution** (1-24 hours):\n   - Verify fix\n   - Document timeline\n   - Customer communication\n\n4. **Post-mortem** (48-72 hours):\n   - Root cause analysis\n   - Action items\n   - Process improvements\n\n### Types of Crises\n\n#### Security Breach\n- Isolate affected systems\n- Engage security team\n- Legal/compliance notification\n- Customer communication plan\n\n#### Major Outage\n- All-hands response\n- Status page updates\n- Executive briefings\n- Customer outreach\n\n#### Data Loss\n- Stop writes immediately\n- Assess recovery options\n- Begin restoration\n- Impact analysis\n\n## Stakeholder Management\n\n### Board/Executive Reporting\n**Monthly**:\n- KPI dashboard\n- Risk register\n- Major initiatives status\n\n**Quarterly**:\n- Technology strategy update\n- Team growth and health\n- Innovation highlights\n- Budget review\n\n### Cross-functional Partners\n\n#### Product Team\n- Weekly roadmap sync\n- Sprint planning participation\n- Technical feasibility reviews\n- Feature estimation\n\n#### Sales/Marketing\n- Technical sales support\n- Product capability briefings\n- Customer reference calls\n- Competitive analysis\n\n#### Finance\n- Budget management\n- Cost optimization\n- Vendor negotiations\n- Capex planning\n\n## Strategic Initiatives\n\n### Digital Transformation\n1. Assess current state\n2. Define target architecture\n3. Create migration plan\n4. Execute in phases\n5. Measure and adjust\n\n### Cloud Migration\n1. Application assessment\n2. Migration strategy (7Rs)\n3. Pilot applications\n4. Full migration\n5. Optimization\n\n### Platform Engineering\n1. Define platform vision\n2. Build core services\n3. Create self-service tools\n4. Enable team adoption\n5. Measure efficiency\n\n### AI/ML Integration\n1. Identify use cases\n2. Build data infrastructure\n3. Develop models\n4. Deploy and monitor\n5. Scale adoption\n\n## Communication Templates\n\n### Technology Strategy Presentation\n```\n1. Executive Summary (1 slide)\n2. Current State Assessment (2 slides)\n3. Vision & Strategy (2 slides)\n4. Roadmap & Milestones (3 slides)\n5. Investment Required (1 slide)\n6. Risks & Mitigation (1 slide)\n7. Success Metrics (1 slide)\n```\n\n### Team All-hands\n```\n1. Wins & Recognition (5 min)\n2. Metrics Review (5 min)\n3. Strategic Updates (10 min)\n4. Demo/Deep Dive (15 min)\n5. Q&A (10 min)\n```\n\n### Board Update Email\n```\nSubject: Engineering Update - [Month]\n\nHighlights:\nâ€¢ [Major achievement]\nâ€¢ [Key metric improvement]\nâ€¢ [Strategic progress]\n\nChallenges:\nâ€¢ [Issue and mitigation]\n\nNext Month:\nâ€¢ [Priority 1]\nâ€¢ [Priority 2]\n\nDetailed metrics attached.\n```\n\n## Tools & Resources\n\n### Essential Tools\n- **Architecture**: Draw.io, Lucidchart, C4 Model\n- **Metrics**: DataDog, Grafana, LinearB\n- **Planning**: Jira, Confluence, Notion\n- **Communication**: Slack, Zoom, Loom\n- **Development**: GitHub, GitLab, Bitbucket\n\n### Key Resources\n- **Books**: \n  - \"The Manager's Path\" - Camille Fournier\n  - \"Accelerate\" - Nicole Forsgren\n  - \"Team Topologies\" - Skelton & Pais\n  \n- **Frameworks**:\n  - DORA metrics\n  - SPACE framework\n  - Team Topologies\n  \n- **Communities**:\n  - CTO Craft\n  - Engineering Leadership Slack\n  - LeadDev community\n\n## Success Indicators\n\nâœ… **Technical Excellence**\n- System uptime >99.9%\n- Deploy multiple times daily\n- Technical debt <10% capacity\n- Security incidents = 0\n\nâœ… **Team Success**\n- Team satisfaction >8/10\n- Attrition <10%\n- Filled positions >90%\n- Diversity improving\n\nâœ… **Business Impact**\n- Features on-time >80%\n- Engineering enables revenue\n- Cost per transaction decreasing\n- Innovation driving growth\n\n## Red Flags to Watch\n\nâš ï¸ Increasing technical debt  \nâš ï¸ Rising attrition rate  \nâš ï¸ Slowing velocity  \nâš ï¸ Growing incidents  \nâš ï¸ Team morale declining  \nâš ï¸ Budget overruns  \nâš ï¸ Vendor dependencies  \nâš ï¸ Security vulnerabilities\n",
      "frontmatter": {
        "name": "cto-advisor",
        "description": "Technical leadership guidance for engineering teams, architecture decisions, and technology strategy. Includes tech debt analyzer, team scaling calculator, engineering metrics frameworks, technology evaluation tools, and ADR templates. Use when assessing technical debt, scaling engineering teams, evaluating technologies, making architecture decisions, establishing engineering metrics, or when user mentions CTO, tech debt, technical debt, team scaling, architecture decisions, technology evaluation, engineering metrics, DORA metrics, or technology strategy.",
        "license": "MIT",
        "metadata": {
          "version": "1.0.0",
          "author": "Alireza Rezvani",
          "category": "c-level",
          "domain": "cto-leadership",
          "updated": "2025-10-20T00:00:00.000Z",
          "python-tools": "tech_debt_analyzer.py, team_scaling_calculator.py",
          "frameworks": "DORA-metrics, architecture-decision-records, engineering-metrics",
          "tech-stack": "engineering-management, team-organization"
        }
      },
      "content": "\n# CTO Advisor\n\nStrategic frameworks and tools for technology leadership, team scaling, and engineering excellence.\n\n## Keywords\nCTO, chief technology officer, technical leadership, tech debt, technical debt, engineering team, team scaling, architecture decisions, technology evaluation, engineering metrics, DORA metrics, ADR, architecture decision records, technology strategy, engineering leadership, engineering organization, team structure, hiring plan, technical strategy, vendor evaluation, technology selection\n\n## Quick Start\n\n### For Technical Debt Assessment\n```bash\npython scripts/tech_debt_analyzer.py\n```\nAnalyzes system architecture and provides prioritized debt reduction plan.\n\n### For Team Scaling Planning\n```bash\npython scripts/team_scaling_calculator.py\n```\nCalculates optimal hiring plan and team structure for growth.\n\n### For Architecture Decisions\nReview `references/architecture_decision_records.md` for ADR templates and examples.\n\n### For Technology Evaluation\nUse framework in `references/technology_evaluation_framework.md` for vendor selection.\n\n### For Engineering Metrics\nImplement KPIs from `references/engineering_metrics.md` for team performance tracking.\n\n## Core Responsibilities\n\n### 1. Technology Strategy\n\n#### Vision & Roadmap\n- Define 3-5 year technology vision\n- Create quarterly roadmaps\n- Align with business strategy\n- Communicate to stakeholders\n\n#### Innovation Management\n- Allocate 20% time for innovation\n- Run hackathons quarterly\n- Evaluate emerging technologies\n- Build proof of concepts\n\n#### Technical Debt Strategy\n```bash\n# Assess current debt\npython scripts/tech_debt_analyzer.py\n\n# Allocate capacity\n- Critical debt: 40% capacity\n- High debt: 25% capacity  \n- Medium debt: 15% capacity\n- Low debt: Ongoing maintenance\n```\n\n### 2. Team Leadership\n\n#### Scaling Engineering\n```bash\n# Calculate scaling needs\npython scripts/team_scaling_calculator.py\n\n# Key ratios to maintain:\n- Manager:Engineer = 1:8\n- Senior:Mid:Junior = 3:4:2\n- Product:Engineering = 1:10\n- QA:Engineering = 1.5:10\n```\n\n#### Performance Management\n- Set clear OKRs quarterly\n- Conduct 1:1s weekly\n- Review performance quarterly\n- Provide growth opportunities\n\n#### Culture Building\n- Define engineering values\n- Establish coding standards\n- Create learning programs\n- Foster collaboration\n\n### 3. Architecture Governance\n\n#### Decision Making\nUse ADR template from `references/architecture_decision_records.md`:\n1. Document context and problem\n2. List all options considered\n3. Record decision and rationale\n4. Track consequences\n\n#### Technology Standards\n- Language choices\n- Framework selection\n- Database standards\n- Security requirements\n- API design guidelines\n\n#### System Design Review\n- Weekly architecture reviews\n- Design documentation standards\n- Prototype requirements\n- Performance criteria\n\n### 4. Vendor Management\n\n#### Evaluation Process\nFollow framework in `references/technology_evaluation_framework.md`:\n1. Gather requirements (Week 1)\n2. Market research (Week 1-2)\n3. Deep evaluation (Week 2-4)\n4. Decision and documentation (Week 4)\n\n#### Vendor Relationships\n- Quarterly business reviews\n- SLA monitoring\n- Cost optimization\n- Strategic partnerships\n\n### 5. Engineering Excellence\n\n#### Metrics Implementation\nFrom `references/engineering_metrics.md`:\n\n**DORA Metrics** (Deploy to production targets):\n- Deployment Frequency: >1/day\n- Lead Time: <1 day\n- MTTR: <1 hour\n- Change Failure Rate: <15%\n\n**Quality Metrics**:\n- Test Coverage: >80%\n- Code Review: 100%\n- Technical Debt: <10%\n\n**Team Health**:\n- Sprint Velocity: Â±10% variance\n- Unplanned Work: <20%\n- On-call Incidents: <5/week\n\n## Weekly Cadence\n\n### Monday\n- Leadership team sync\n- Review metrics dashboard\n- Address escalations\n\n### Tuesday\n- Architecture review\n- Technical interviews\n- 1:1s with directs\n\n### Wednesday\n- Cross-functional meetings\n- Vendor meetings\n- Strategy work\n\n### Thursday\n- Team all-hands (monthly)\n- Sprint reviews (bi-weekly)\n- Technical deep dives\n\n### Friday\n- Strategic planning\n- Innovation time\n- Week recap and planning\n\n## Quarterly Planning\n\n### Q1 Focus: Foundation\n- Annual planning\n- Budget allocation\n- Team goal setting\n- Technology assessment\n\n### Q2 Focus: Execution\n- Major initiatives launch\n- Mid-year hiring push\n- Performance reviews\n- Architecture evolution\n\n### Q3 Focus: Innovation\n- Hackathon\n- Technology exploration\n- Team development\n- Process optimization\n\n### Q4 Focus: Planning\n- Next year strategy\n- Budget planning\n- Promotion cycles\n- Debt reduction sprint\n\n## Crisis Management\n\n### Incident Response\n1. **Immediate** (0-15 min):\n   - Assess severity\n   - Activate incident team\n   - Begin communication\n\n2. **Short-term** (15-60 min):\n   - Implement fixes\n   - Update stakeholders\n   - Monitor systems\n\n3. **Resolution** (1-24 hours):\n   - Verify fix\n   - Document timeline\n   - Customer communication\n\n4. **Post-mortem** (48-72 hours):\n   - Root cause analysis\n   - Action items\n   - Process improvements\n\n### Types of Crises\n\n#### Security Breach\n- Isolate affected systems\n- Engage security team\n- Legal/compliance notification\n- Customer communication plan\n\n#### Major Outage\n- All-hands response\n- Status page updates\n- Executive briefings\n- Customer outreach\n\n#### Data Loss\n- Stop writes immediately\n- Assess recovery options\n- Begin restoration\n- Impact analysis\n\n## Stakeholder Management\n\n### Board/Executive Reporting\n**Monthly**:\n- KPI dashboard\n- Risk register\n- Major initiatives status\n\n**Quarterly**:\n- Technology strategy update\n- Team growth and health\n- Innovation highlights\n- Budget review\n\n### Cross-functional Partners\n\n#### Product Team\n- Weekly roadmap sync\n- Sprint planning participation\n- Technical feasibility reviews\n- Feature estimation\n\n#### Sales/Marketing\n- Technical sales support\n- Product capability briefings\n- Customer reference calls\n- Competitive analysis\n\n#### Finance\n- Budget management\n- Cost optimization\n- Vendor negotiations\n- Capex planning\n\n## Strategic Initiatives\n\n### Digital Transformation\n1. Assess current state\n2. Define target architecture\n3. Create migration plan\n4. Execute in phases\n5. Measure and adjust\n\n### Cloud Migration\n1. Application assessment\n2. Migration strategy (7Rs)\n3. Pilot applications\n4. Full migration\n5. Optimization\n\n### Platform Engineering\n1. Define platform vision\n2. Build core services\n3. Create self-service tools\n4. Enable team adoption\n5. Measure efficiency\n\n### AI/ML Integration\n1. Identify use cases\n2. Build data infrastructure\n3. Develop models\n4. Deploy and monitor\n5. Scale adoption\n\n## Communication Templates\n\n### Technology Strategy Presentation\n```\n1. Executive Summary (1 slide)\n2. Current State Assessment (2 slides)\n3. Vision & Strategy (2 slides)\n4. Roadmap & Milestones (3 slides)\n5. Investment Required (1 slide)\n6. Risks & Mitigation (1 slide)\n7. Success Metrics (1 slide)\n```\n\n### Team All-hands\n```\n1. Wins & Recognition (5 min)\n2. Metrics Review (5 min)\n3. Strategic Updates (10 min)\n4. Demo/Deep Dive (15 min)\n5. Q&A (10 min)\n```\n\n### Board Update Email\n```\nSubject: Engineering Update - [Month]\n\nHighlights:\nâ€¢ [Major achievement]\nâ€¢ [Key metric improvement]\nâ€¢ [Strategic progress]\n\nChallenges:\nâ€¢ [Issue and mitigation]\n\nNext Month:\nâ€¢ [Priority 1]\nâ€¢ [Priority 2]\n\nDetailed metrics attached.\n```\n\n## Tools & Resources\n\n### Essential Tools\n- **Architecture**: Draw.io, Lucidchart, C4 Model\n- **Metrics**: DataDog, Grafana, LinearB\n- **Planning**: Jira, Confluence, Notion\n- **Communication**: Slack, Zoom, Loom\n- **Development**: GitHub, GitLab, Bitbucket\n\n### Key Resources\n- **Books**: \n  - \"The Manager's Path\" - Camille Fournier\n  - \"Accelerate\" - Nicole Forsgren\n  - \"Team Topologies\" - Skelton & Pais\n  \n- **Frameworks**:\n  - DORA metrics\n  - SPACE framework\n  - Team Topologies\n  \n- **Communities**:\n  - CTO Craft\n  - Engineering Leadership Slack\n  - LeadDev community\n\n## Success Indicators\n\nâœ… **Technical Excellence**\n- System uptime >99.9%\n- Deploy multiple times daily\n- Technical debt <10% capacity\n- Security incidents = 0\n\nâœ… **Team Success**\n- Team satisfaction >8/10\n- Attrition <10%\n- Filled positions >90%\n- Diversity improving\n\nâœ… **Business Impact**\n- Features on-time >80%\n- Engineering enables revenue\n- Cost per transaction decreasing\n- Innovation driving growth\n\n## Red Flags to Watch\n\nâš ï¸ Increasing technical debt  \nâš ï¸ Rising attrition rate  \nâš ï¸ Slowing velocity  \nâš ï¸ Growing incidents  \nâš ï¸ Team morale declining  \nâš ï¸ Budget overruns  \nâš ï¸ Vendor dependencies  \nâš ï¸ Security vulnerabilities\n"
    }
  },
  "alirezarezvani-claude-skills-product-manager-toolkit": {
    "id": "alirezarezvani-claude-skills-product-manager-toolkit",
    "name": "product-manager-toolkit",
    "description": "Comprehensive toolkit for product managers including RICE prioritization, customer interview analysis, PRD templates, discovery frameworks, and go-to-market strategies. Use for feature prioritization, user research synthesis, requirement documentation, and product strategy development.",
    "repo": {
      "owner": "alirezarezvani",
      "name": "claude-skills",
      "fullName": "alirezarezvani/claude-skills",
      "url": "https://github.com/alirezarezvani/claude-skills/tree/main/product-team/product-manager-toolkit",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 521,
      "forks": 103,
      "language": "Python",
      "topics": [
        "agentic-ai",
        "agentic-coding",
        "anthropic-claude",
        "claude-ai",
        "claude-ai-skills",
        "claude-code",
        "claude-code-skills",
        "claude-skills",
        "claudecode-subagents"
      ],
      "updatedAt": "2026-01-08T12:36:14Z",
      "pushedAt": "2026-01-08T08:39:21Z",
      "createdAt": "2025-10-19T04:04:05Z",
      "license": "MIT License"
    },
    "category": "product",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: product-manager-toolkit\ndescription: Comprehensive toolkit for product managers including RICE prioritization, customer interview analysis, PRD templates, discovery frameworks, and go-to-market strategies. Use for feature prioritization, user research synthesis, requirement documentation, and product strategy development.\n---\n\n# Product Manager Toolkit\n\nEssential tools and frameworks for modern product management, from discovery to delivery.\n\n## Quick Start\n\n### For Feature Prioritization\n```bash\npython scripts/rice_prioritizer.py sample  # Create sample CSV\npython scripts/rice_prioritizer.py sample_features.csv --capacity 15\n```\n\n### For Interview Analysis\n```bash\npython scripts/customer_interview_analyzer.py interview_transcript.txt\n```\n\n### For PRD Creation\n1. Choose template from `references/prd_templates.md`\n2. Fill in sections based on discovery work\n3. Review with stakeholders\n4. Version control in your PM tool\n\n## Core Workflows\n\n### Feature Prioritization Process\n\n1. **Gather Feature Requests**\n   - Customer feedback\n   - Sales requests\n   - Technical debt\n   - Strategic initiatives\n\n2. **Score with RICE**\n   ```bash\n   # Create CSV with: name,reach,impact,confidence,effort\n   python scripts/rice_prioritizer.py features.csv\n   ```\n   - **Reach**: Users affected per quarter\n   - **Impact**: massive/high/medium/low/minimal\n   - **Confidence**: high/medium/low\n   - **Effort**: xl/l/m/s/xs (person-months)\n\n3. **Analyze Portfolio**\n   - Review quick wins vs big bets\n   - Check effort distribution\n   - Validate against strategy\n\n4. **Generate Roadmap**\n   - Quarterly capacity planning\n   - Dependency mapping\n   - Stakeholder alignment\n\n### Customer Discovery Process\n\n1. **Conduct Interviews**\n   - Use semi-structured format\n   - Focus on problems, not solutions\n   - Record with permission\n\n2. **Analyze Insights**\n   ```bash\n   python scripts/customer_interview_analyzer.py transcript.txt\n   ```\n   Extracts:\n   - Pain points with severity\n   - Feature requests with priority\n   - Jobs to be done\n   - Sentiment analysis\n   - Key themes and quotes\n\n3. **Synthesize Findings**\n   - Group similar pain points\n   - Identify patterns across interviews\n   - Map to opportunity areas\n\n4. **Validate Solutions**\n   - Create solution hypotheses\n   - Test with prototypes\n   - Measure actual vs expected behavior\n\n### PRD Development Process\n\n1. **Choose Template**\n   - **Standard PRD**: Complex features (6-8 weeks)\n   - **One-Page PRD**: Simple features (2-4 weeks)\n   - **Feature Brief**: Exploration phase (1 week)\n   - **Agile Epic**: Sprint-based delivery\n\n2. **Structure Content**\n   - Problem â†’ Solution â†’ Success Metrics\n   - Always include out-of-scope\n   - Clear acceptance criteria\n\n3. **Collaborate**\n   - Engineering for feasibility\n   - Design for experience\n   - Sales for market validation\n   - Support for operational impact\n\n## Key Scripts\n\n### rice_prioritizer.py\nAdvanced RICE framework implementation with portfolio analysis.\n\n**Features**:\n- RICE score calculation\n- Portfolio balance analysis (quick wins vs big bets)\n- Quarterly roadmap generation\n- Team capacity planning\n- Multiple output formats (text/json/csv)\n\n**Usage Examples**:\n```bash\n# Basic prioritization\npython scripts/rice_prioritizer.py features.csv\n\n# With custom team capacity (person-months per quarter)\npython scripts/rice_prioritizer.py features.csv --capacity 20\n\n# Output as JSON for integration\npython scripts/rice_prioritizer.py features.csv --output json\n```\n\n### customer_interview_analyzer.py\nNLP-based interview analysis for extracting actionable insights.\n\n**Capabilities**:\n- Pain point extraction with severity assessment\n- Feature request identification and classification\n- Jobs-to-be-done pattern recognition\n- Sentiment analysis\n- Theme extraction\n- Competitor mentions\n- Key quotes identification\n\n**Usage Examples**:\n```bash\n# Analyze single interview\npython scripts/customer_interview_analyzer.py interview.txt\n\n# Output as JSON for aggregation\npython scripts/customer_interview_analyzer.py interview.txt json\n```\n\n## Reference Documents\n\n### prd_templates.md\nMultiple PRD formats for different contexts:\n\n1. **Standard PRD Template**\n   - Comprehensive 11-section format\n   - Best for major features\n   - Includes technical specs\n\n2. **One-Page PRD**\n   - Concise format for quick alignment\n   - Focus on problem/solution/metrics\n   - Good for smaller features\n\n3. **Agile Epic Template**\n   - Sprint-based delivery\n   - User story mapping\n   - Acceptance criteria focus\n\n4. **Feature Brief**\n   - Lightweight exploration\n   - Hypothesis-driven\n   - Pre-PRD phase\n\n## Prioritization Frameworks\n\n### RICE Framework\n```\nScore = (Reach Ã— Impact Ã— Confidence) / Effort\n\nReach: # of users/quarter\nImpact: \n  - Massive = 3x\n  - High = 2x\n  - Medium = 1x\n  - Low = 0.5x\n  - Minimal = 0.25x\nConfidence:\n  - High = 100%\n  - Medium = 80%\n  - Low = 50%\nEffort: Person-months\n```\n\n### Value vs Effort Matrix\n```\n         Low Effort    High Effort\n         \nHigh     QUICK WINS    BIG BETS\nValue    [Prioritize]   [Strategic]\n         \nLow      FILL-INS      TIME SINKS\nValue    [Maybe]       [Avoid]\n```\n\n### MoSCoW Method\n- **Must Have**: Critical for launch\n- **Should Have**: Important but not critical\n- **Could Have**: Nice to have\n- **Won't Have**: Out of scope\n\n## Discovery Frameworks\n\n### Customer Interview Guide\n```\n1. Context Questions (5 min)\n   - Role and responsibilities\n   - Current workflow\n   - Tools used\n\n2. Problem Exploration (15 min)\n   - Pain points\n   - Frequency and impact\n   - Current workarounds\n\n3. Solution Validation (10 min)\n   - Reaction to concepts\n   - Value perception\n   - Willingness to pay\n\n4. Wrap-up (5 min)\n   - Other thoughts\n   - Referrals\n   - Follow-up permission\n```\n\n### Hypothesis Template\n```\nWe believe that [building this feature]\nFor [these users]\nWill [achieve this outcome]\nWe'll know we're right when [metric]\n```\n\n### Opportunity Solution Tree\n```\nOutcome\nâ”œâ”€â”€ Opportunity 1\nâ”‚   â”œâ”€â”€ Solution A\nâ”‚   â””â”€â”€ Solution B\nâ””â”€â”€ Opportunity 2\n    â”œâ”€â”€ Solution C\n    â””â”€â”€ Solution D\n```\n\n## Metrics & Analytics\n\n### North Star Metric Framework\n1. **Identify Core Value**: What's the #1 value to users?\n2. **Make it Measurable**: Quantifiable and trackable\n3. **Ensure It's Actionable**: Teams can influence it\n4. **Check Leading Indicator**: Predicts business success\n\n### Funnel Analysis Template\n```\nAcquisition â†’ Activation â†’ Retention â†’ Revenue â†’ Referral\n\nKey Metrics:\n- Conversion rate at each step\n- Drop-off points\n- Time between steps\n- Cohort variations\n```\n\n### Feature Success Metrics\n- **Adoption**: % of users using feature\n- **Frequency**: Usage per user per time period\n- **Depth**: % of feature capability used\n- **Retention**: Continued usage over time\n- **Satisfaction**: NPS/CSAT for feature\n\n## Best Practices\n\n### Writing Great PRDs\n1. Start with the problem, not solution\n2. Include clear success metrics upfront\n3. Explicitly state what's out of scope\n4. Use visuals (wireframes, flows)\n5. Keep technical details in appendix\n6. Version control changes\n\n### Effective Prioritization\n1. Mix quick wins with strategic bets\n2. Consider opportunity cost\n3. Account for dependencies\n4. Buffer for unexpected work (20%)\n5. Revisit quarterly\n6. Communicate decisions clearly\n\n### Customer Discovery Tips\n1. Ask \"why\" 5 times\n2. Focus on past behavior, not future intentions\n3. Avoid leading questions\n4. Interview in their environment\n5. Look for emotional reactions\n6. Validate with data\n\n### Stakeholder Management\n1. Identify RACI for decisions\n2. Regular async updates\n3. Demo over documentation\n4. Address concerns early\n5. Celebrate wins publicly\n6. Learn from failures openly\n\n## Common Pitfalls to Avoid\n\n1. **Solution-First Thinking**: Jumping to features before understanding problems\n2. **Analysis Paralysis**: Over-researching without shipping\n3. **Feature Factory**: Shipping features without measuring impact\n4. **Ignoring Technical Debt**: Not allocating time for platform health\n5. **Stakeholder Surprise**: Not communicating early and often\n6. **Metric Theater**: Optimizing vanity metrics over real value\n\n## Integration Points\n\nThis toolkit integrates with:\n- **Analytics**: Amplitude, Mixpanel, Google Analytics\n- **Roadmapping**: ProductBoard, Aha!, Roadmunk\n- **Design**: Figma, Sketch, Miro\n- **Development**: Jira, Linear, GitHub\n- **Research**: Dovetail, UserVoice, Pendo\n- **Communication**: Slack, Notion, Confluence\n\n## Quick Commands Cheat Sheet\n\n```bash\n# Prioritization\npython scripts/rice_prioritizer.py features.csv --capacity 15\n\n# Interview Analysis\npython scripts/customer_interview_analyzer.py interview.txt\n\n# Create sample data\npython scripts/rice_prioritizer.py sample\n\n# JSON outputs for integration\npython scripts/rice_prioritizer.py features.csv --output json\npython scripts/customer_interview_analyzer.py interview.txt json\n```\n",
      "frontmatter": {
        "name": "product-manager-toolkit",
        "description": "Comprehensive toolkit for product managers including RICE prioritization, customer interview analysis, PRD templates, discovery frameworks, and go-to-market strategies. Use for feature prioritization, user research synthesis, requirement documentation, and product strategy development."
      },
      "content": "\n# Product Manager Toolkit\n\nEssential tools and frameworks for modern product management, from discovery to delivery.\n\n## Quick Start\n\n### For Feature Prioritization\n```bash\npython scripts/rice_prioritizer.py sample  # Create sample CSV\npython scripts/rice_prioritizer.py sample_features.csv --capacity 15\n```\n\n### For Interview Analysis\n```bash\npython scripts/customer_interview_analyzer.py interview_transcript.txt\n```\n\n### For PRD Creation\n1. Choose template from `references/prd_templates.md`\n2. Fill in sections based on discovery work\n3. Review with stakeholders\n4. Version control in your PM tool\n\n## Core Workflows\n\n### Feature Prioritization Process\n\n1. **Gather Feature Requests**\n   - Customer feedback\n   - Sales requests\n   - Technical debt\n   - Strategic initiatives\n\n2. **Score with RICE**\n   ```bash\n   # Create CSV with: name,reach,impact,confidence,effort\n   python scripts/rice_prioritizer.py features.csv\n   ```\n   - **Reach**: Users affected per quarter\n   - **Impact**: massive/high/medium/low/minimal\n   - **Confidence**: high/medium/low\n   - **Effort**: xl/l/m/s/xs (person-months)\n\n3. **Analyze Portfolio**\n   - Review quick wins vs big bets\n   - Check effort distribution\n   - Validate against strategy\n\n4. **Generate Roadmap**\n   - Quarterly capacity planning\n   - Dependency mapping\n   - Stakeholder alignment\n\n### Customer Discovery Process\n\n1. **Conduct Interviews**\n   - Use semi-structured format\n   - Focus on problems, not solutions\n   - Record with permission\n\n2. **Analyze Insights**\n   ```bash\n   python scripts/customer_interview_analyzer.py transcript.txt\n   ```\n   Extracts:\n   - Pain points with severity\n   - Feature requests with priority\n   - Jobs to be done\n   - Sentiment analysis\n   - Key themes and quotes\n\n3. **Synthesize Findings**\n   - Group similar pain points\n   - Identify patterns across interviews\n   - Map to opportunity areas\n\n4. **Validate Solutions**\n   - Create solution hypotheses\n   - Test with prototypes\n   - Measure actual vs expected behavior\n\n### PRD Development Process\n\n1. **Choose Template**\n   - **Standard PRD**: Complex features (6-8 weeks)\n   - **One-Page PRD**: Simple features (2-4 weeks)\n   - **Feature Brief**: Exploration phase (1 week)\n   - **Agile Epic**: Sprint-based delivery\n\n2. **Structure Content**\n   - Problem â†’ Solution â†’ Success Metrics\n   - Always include out-of-scope\n   - Clear acceptance criteria\n\n3. **Collaborate**\n   - Engineering for feasibility\n   - Design for experience\n   - Sales for market validation\n   - Support for operational impact\n\n## Key Scripts\n\n### rice_prioritizer.py\nAdvanced RICE framework implementation with portfolio analysis.\n\n**Features**:\n- RICE score calculation\n- Portfolio balance analysis (quick wins vs big bets)\n- Quarterly roadmap generation\n- Team capacity planning\n- Multiple output formats (text/json/csv)\n\n**Usage Examples**:\n```bash\n# Basic prioritization\npython scripts/rice_prioritizer.py features.csv\n\n# With custom team capacity (person-months per quarter)\npython scripts/rice_prioritizer.py features.csv --capacity 20\n\n# Output as JSON for integration\npython scripts/rice_prioritizer.py features.csv --output json\n```\n\n### customer_interview_analyzer.py\nNLP-based interview analysis for extracting actionable insights.\n\n**Capabilities**:\n- Pain point extraction with severity assessment\n- Feature request identification and classification\n- Jobs-to-be-done pattern recognition\n- Sentiment analysis\n- Theme extraction\n- Competitor mentions\n- Key quotes identification\n\n**Usage Examples**:\n```bash\n# Analyze single interview\npython scripts/customer_interview_analyzer.py interview.txt\n\n# Output as JSON for aggregation\npython scripts/customer_interview_analyzer.py interview.txt json\n```\n\n## Reference Documents\n\n### prd_templates.md\nMultiple PRD formats for different contexts:\n\n1. **Standard PRD Template**\n   - Comprehensive 11-section format\n   - Best for major features\n   - Includes technical specs\n\n2. **One-Page PRD**\n   - Concise format for quick alignment\n   - Focus on problem/solution/metrics\n   - Good for smaller features\n\n3. **Agile Epic Template**\n   - Sprint-based delivery\n   - User story mapping\n   - Acceptance criteria focus\n\n4. **Feature Brief**\n   - Lightweight exploration\n   - Hypothesis-driven\n   - Pre-PRD phase\n\n## Prioritization Frameworks\n\n### RICE Framework\n```\nScore = (Reach Ã— Impact Ã— Confidence) / Effort\n\nReach: # of users/quarter\nImpact: \n  - Massive = 3x\n  - High = 2x\n  - Medium = 1x\n  - Low = 0.5x\n  - Minimal = 0.25x\nConfidence:\n  - High = 100%\n  - Medium = 80%\n  - Low = 50%\nEffort: Person-months\n```\n\n### Value vs Effort Matrix\n```\n         Low Effort    High Effort\n         \nHigh     QUICK WINS    BIG BETS\nValue    [Prioritize]   [Strategic]\n         \nLow      FILL-INS      TIME SINKS\nValue    [Maybe]       [Avoid]\n```\n\n### MoSCoW Method\n- **Must Have**: Critical for launch\n- **Should Have**: Important but not critical\n- **Could Have**: Nice to have\n- **Won't Have**: Out of scope\n\n## Discovery Frameworks\n\n### Customer Interview Guide\n```\n1. Context Questions (5 min)\n   - Role and responsibilities\n   - Current workflow\n   - Tools used\n\n2. Problem Exploration (15 min)\n   - Pain points\n   - Frequency and impact\n   - Current workarounds\n\n3. Solution Validation (10 min)\n   - Reaction to concepts\n   - Value perception\n   - Willingness to pay\n\n4. Wrap-up (5 min)\n   - Other thoughts\n   - Referrals\n   - Follow-up permission\n```\n\n### Hypothesis Template\n```\nWe believe that [building this feature]\nFor [these users]\nWill [achieve this outcome]\nWe'll know we're right when [metric]\n```\n\n### Opportunity Solution Tree\n```\nOutcome\nâ”œâ”€â”€ Opportunity 1\nâ”‚   â”œâ”€â”€ Solution A\nâ”‚   â””â”€â”€ Solution B\nâ””â”€â”€ Opportunity 2\n    â”œâ”€â”€ Solution C\n    â””â”€â”€ Solution D\n```\n\n## Metrics & Analytics\n\n### North Star Metric Framework\n1. **Identify Core Value**: What's the #1 value to users?\n2. **Make it Measurable**: Quantifiable and trackable\n3. **Ensure It's Actionable**: Teams can influence it\n4. **Check Leading Indicator**: Predicts business success\n\n### Funnel Analysis Template\n```\nAcquisition â†’ Activation â†’ Retention â†’ Revenue â†’ Referral\n\nKey Metrics:\n- Conversion rate at each step\n- Drop-off points\n- Time between steps\n- Cohort variations\n```\n\n### Feature Success Metrics\n- **Adoption**: % of users using feature\n- **Frequency**: Usage per user per time period\n- **Depth**: % of feature capability used\n- **Retention**: Continued usage over time\n- **Satisfaction**: NPS/CSAT for feature\n\n## Best Practices\n\n### Writing Great PRDs\n1. Start with the problem, not solution\n2. Include clear success metrics upfront\n3. Explicitly state what's out of scope\n4. Use visuals (wireframes, flows)\n5. Keep technical details in appendix\n6. Version control changes\n\n### Effective Prioritization\n1. Mix quick wins with strategic bets\n2. Consider opportunity cost\n3. Account for dependencies\n4. Buffer for unexpected work (20%)\n5. Revisit quarterly\n6. Communicate decisions clearly\n\n### Customer Discovery Tips\n1. Ask \"why\" 5 times\n2. Focus on past behavior, not future intentions\n3. Avoid leading questions\n4. Interview in their environment\n5. Look for emotional reactions\n6. Validate with data\n\n### Stakeholder Management\n1. Identify RACI for decisions\n2. Regular async updates\n3. Demo over documentation\n4. Address concerns early\n5. Celebrate wins publicly\n6. Learn from failures openly\n\n## Common Pitfalls to Avoid\n\n1. **Solution-First Thinking**: Jumping to features before understanding problems\n2. **Analysis Paralysis**: Over-researching without shipping\n3. **Feature Factory**: Shipping features without measuring impact\n4. **Ignoring Technical Debt**: Not allocating time for platform health\n5. **Stakeholder Surprise**: Not communicating early and often\n6. **Metric Theater**: Optimizing vanity metrics over real value\n\n## Integration Points\n\nThis toolkit integrates with:\n- **Analytics**: Amplitude, Mixpanel, Google Analytics\n- **Roadmapping**: ProductBoard, Aha!, Roadmunk\n- **Design**: Figma, Sketch, Miro\n- **Development**: Jira, Linear, GitHub\n- **Research**: Dovetail, UserVoice, Pendo\n- **Communication**: Slack, Notion, Confluence\n\n## Quick Commands Cheat Sheet\n\n```bash\n# Prioritization\npython scripts/rice_prioritizer.py features.csv --capacity 15\n\n# Interview Analysis\npython scripts/customer_interview_analyzer.py interview.txt\n\n# Create sample data\npython scripts/rice_prioritizer.py sample\n\n# JSON outputs for integration\npython scripts/rice_prioritizer.py features.csv --output json\npython scripts/customer_interview_analyzer.py interview.txt json\n```\n"
    }
  },
  "alirezarezvani-claude-skills-agile-product-owner": {
    "id": "alirezarezvani-claude-skills-agile-product-owner",
    "name": "agile-product-owner",
    "description": "Agile product ownership toolkit for Senior Product Owner including INVEST-compliant user story generation, sprint planning, backlog management, and velocity tracking. Use for story writing, sprint planning, stakeholder communication, and agile ceremonies.",
    "repo": {
      "owner": "alirezarezvani",
      "name": "claude-skills",
      "fullName": "alirezarezvani/claude-skills",
      "url": "https://github.com/alirezarezvani/claude-skills/tree/main/product-team/agile-product-owner",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 521,
      "forks": 103,
      "language": "Python",
      "topics": [
        "agentic-ai",
        "agentic-coding",
        "anthropic-claude",
        "claude-ai",
        "claude-ai-skills",
        "claude-code",
        "claude-code-skills",
        "claude-skills",
        "claudecode-subagents"
      ],
      "updatedAt": "2026-01-08T12:36:14Z",
      "pushedAt": "2026-01-08T08:39:21Z",
      "createdAt": "2025-10-19T04:04:05Z",
      "license": "MIT License"
    },
    "category": "product",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: agile-product-owner\ndescription: Agile product ownership toolkit for Senior Product Owner including INVEST-compliant user story generation, sprint planning, backlog management, and velocity tracking. Use for story writing, sprint planning, stakeholder communication, and agile ceremonies.\n---\n\n# Agile Product Owner\n\nComplete toolkit for Product Owners to excel at backlog management and sprint execution.\n\n## Core Capabilities\n- INVEST-compliant user story generation\n- Automatic acceptance criteria creation\n- Sprint capacity planning\n- Backlog prioritization\n- Velocity tracking and metrics\n\n## Key Scripts\n\n### user_story_generator.py\nGenerates well-formed user stories with acceptance criteria from epics.\n\n**Usage**: \n- Generate stories: `python scripts/user_story_generator.py`\n- Plan sprint: `python scripts/user_story_generator.py sprint [capacity]`\n\n**Features**:\n- Breaks epics into stories\n- INVEST criteria validation\n- Automatic point estimation\n- Priority assignment\n- Sprint planning with capacity\n",
      "frontmatter": {
        "name": "agile-product-owner",
        "description": "Agile product ownership toolkit for Senior Product Owner including INVEST-compliant user story generation, sprint planning, backlog management, and velocity tracking. Use for story writing, sprint planning, stakeholder communication, and agile ceremonies."
      },
      "content": "\n# Agile Product Owner\n\nComplete toolkit for Product Owners to excel at backlog management and sprint execution.\n\n## Core Capabilities\n- INVEST-compliant user story generation\n- Automatic acceptance criteria creation\n- Sprint capacity planning\n- Backlog prioritization\n- Velocity tracking and metrics\n\n## Key Scripts\n\n### user_story_generator.py\nGenerates well-formed user stories with acceptance criteria from epics.\n\n**Usage**: \n- Generate stories: `python scripts/user_story_generator.py`\n- Plan sprint: `python scripts/user_story_generator.py sprint [capacity]`\n\n**Features**:\n- Breaks epics into stories\n- INVEST criteria validation\n- Automatic point estimation\n- Priority assignment\n- Sprint planning with capacity\n"
    }
  },
  "alirezarezvani-claude-skills-product-strategist": {
    "id": "alirezarezvani-claude-skills-product-strategist",
    "name": "product-strategist",
    "description": "Strategic product leadership toolkit for Head of Product including OKR cascade generation, market analysis, vision setting, and team scaling. Use for strategic planning, goal alignment, competitive analysis, and organizational design.",
    "repo": {
      "owner": "alirezarezvani",
      "name": "claude-skills",
      "fullName": "alirezarezvani/claude-skills",
      "url": "https://github.com/alirezarezvani/claude-skills/tree/main/product-team/product-strategist",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 521,
      "forks": 103,
      "language": "Python",
      "topics": [
        "agentic-ai",
        "agentic-coding",
        "anthropic-claude",
        "claude-ai",
        "claude-ai-skills",
        "claude-code",
        "claude-code-skills",
        "claude-skills",
        "claudecode-subagents"
      ],
      "updatedAt": "2026-01-08T12:36:14Z",
      "pushedAt": "2026-01-08T08:39:21Z",
      "createdAt": "2025-10-19T04:04:05Z",
      "license": "MIT License"
    },
    "category": "product",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: product-strategist\ndescription: Strategic product leadership toolkit for Head of Product including OKR cascade generation, market analysis, vision setting, and team scaling. Use for strategic planning, goal alignment, competitive analysis, and organizational design.\n---\n\n# Product Strategist\n\nStrategic toolkit for Head of Product to drive vision, alignment, and organizational excellence.\n\n## Core Capabilities\n- OKR cascade generation and alignment\n- Market and competitive analysis\n- Product vision and strategy frameworks\n- Team scaling and organizational design\n- Metrics and KPI definition\n\n## Key Scripts\n\n### okr_cascade_generator.py\nAutomatically cascades company OKRs down to product and team levels with alignment tracking.\n\n**Usage**: `python scripts/okr_cascade_generator.py [strategy]`\n- Strategies: growth, retention, revenue, innovation, operational\n- Generates company â†’ product â†’ team OKR cascade\n- Calculates alignment scores\n- Tracks contribution percentages\n",
      "frontmatter": {
        "name": "product-strategist",
        "description": "Strategic product leadership toolkit for Head of Product including OKR cascade generation, market analysis, vision setting, and team scaling. Use for strategic planning, goal alignment, competitive analysis, and organizational design."
      },
      "content": "\n# Product Strategist\n\nStrategic toolkit for Head of Product to drive vision, alignment, and organizational excellence.\n\n## Core Capabilities\n- OKR cascade generation and alignment\n- Market and competitive analysis\n- Product vision and strategy frameworks\n- Team scaling and organizational design\n- Metrics and KPI definition\n\n## Key Scripts\n\n### okr_cascade_generator.py\nAutomatically cascades company OKRs down to product and team levels with alignment tracking.\n\n**Usage**: `python scripts/okr_cascade_generator.py [strategy]`\n- Strategies: growth, retention, revenue, innovation, operational\n- Generates company â†’ product â†’ team OKR cascade\n- Calculates alignment scores\n- Tracks contribution percentages\n"
    }
  },
  "alirezarezvani-claude-skills-ux-researcher-designer": {
    "id": "alirezarezvani-claude-skills-ux-researcher-designer",
    "name": "ux-researcher-designer",
    "description": "UX research and design toolkit for Senior UX Designer/Researcher including data-driven persona generation, journey mapping, usability testing frameworks, and research synthesis. Use for user research, persona creation, journey mapping, and design validation.",
    "repo": {
      "owner": "alirezarezvani",
      "name": "claude-skills",
      "fullName": "alirezarezvani/claude-skills",
      "url": "https://github.com/alirezarezvani/claude-skills/tree/main/product-team/ux-researcher-designer",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 521,
      "forks": 103,
      "language": "Python",
      "topics": [
        "agentic-ai",
        "agentic-coding",
        "anthropic-claude",
        "claude-ai",
        "claude-ai-skills",
        "claude-code",
        "claude-code-skills",
        "claude-skills",
        "claudecode-subagents"
      ],
      "updatedAt": "2026-01-08T12:36:14Z",
      "pushedAt": "2026-01-08T08:39:21Z",
      "createdAt": "2025-10-19T04:04:05Z",
      "license": "MIT License"
    },
    "category": "product",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: ux-researcher-designer\ndescription: UX research and design toolkit for Senior UX Designer/Researcher including data-driven persona generation, journey mapping, usability testing frameworks, and research synthesis. Use for user research, persona creation, journey mapping, and design validation.\n---\n\n# UX Researcher & Designer\n\nComprehensive toolkit for user-centered research and experience design.\n\n## Core Capabilities\n- Data-driven persona generation\n- Customer journey mapping\n- Usability testing frameworks\n- Research synthesis and insights\n- Design validation methods\n\n## Key Scripts\n\n### persona_generator.py\nCreates research-backed personas from user data and interviews.\n\n**Usage**: `python scripts/persona_generator.py [json]`\n\n**Features**:\n- Analyzes user behavior patterns\n- Identifies persona archetypes\n- Extracts psychographics\n- Generates scenarios\n- Provides design implications\n- Confidence scoring based on sample size\n",
      "frontmatter": {
        "name": "ux-researcher-designer",
        "description": "UX research and design toolkit for Senior UX Designer/Researcher including data-driven persona generation, journey mapping, usability testing frameworks, and research synthesis. Use for user research, persona creation, journey mapping, and design validation."
      },
      "content": "\n# UX Researcher & Designer\n\nComprehensive toolkit for user-centered research and experience design.\n\n## Core Capabilities\n- Data-driven persona generation\n- Customer journey mapping\n- Usability testing frameworks\n- Research synthesis and insights\n- Design validation methods\n\n## Key Scripts\n\n### persona_generator.py\nCreates research-backed personas from user data and interviews.\n\n**Usage**: `python scripts/persona_generator.py [json]`\n\n**Features**:\n- Analyzes user behavior patterns\n- Identifies persona archetypes\n- Extracts psychographics\n- Generates scenarios\n- Provides design implications\n- Confidence scoring based on sample size\n"
    }
  },
  "alirezarezvani-claude-skills-ui-design-system": {
    "id": "alirezarezvani-claude-skills-ui-design-system",
    "name": "ui-design-system",
    "description": "UI design system toolkit for Senior UI Designer including design token generation, component documentation, responsive design calculations, and developer handoff tools. Use for creating design systems, maintaining visual consistency, and facilitating design-dev collaboration.",
    "repo": {
      "owner": "alirezarezvani",
      "name": "claude-skills",
      "fullName": "alirezarezvani/claude-skills",
      "url": "https://github.com/alirezarezvani/claude-skills/tree/main/product-team/ui-design-system",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 521,
      "forks": 103,
      "language": "Python",
      "topics": [
        "agentic-ai",
        "agentic-coding",
        "anthropic-claude",
        "claude-ai",
        "claude-ai-skills",
        "claude-code",
        "claude-code-skills",
        "claude-skills",
        "claudecode-subagents"
      ],
      "updatedAt": "2026-01-08T12:36:14Z",
      "pushedAt": "2026-01-08T08:39:21Z",
      "createdAt": "2025-10-19T04:04:05Z",
      "license": "MIT License"
    },
    "category": "product",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: ui-design-system\ndescription: UI design system toolkit for Senior UI Designer including design token generation, component documentation, responsive design calculations, and developer handoff tools. Use for creating design systems, maintaining visual consistency, and facilitating design-dev collaboration.\n---\n\n# UI Design System\n\nProfessional toolkit for creating and maintaining scalable design systems.\n\n## Core Capabilities\n- Design token generation (colors, typography, spacing)\n- Component system architecture\n- Responsive design calculations\n- Accessibility compliance\n- Developer handoff documentation\n\n## Key Scripts\n\n### design_token_generator.py\nGenerates complete design system tokens from brand colors.\n\n**Usage**: `python scripts/design_token_generator.py [brand_color] [style] [format]`\n- Styles: modern, classic, playful\n- Formats: json, css, scss\n\n**Features**:\n- Complete color palette generation\n- Modular typography scale\n- 8pt spacing grid system\n- Shadow and animation tokens\n- Responsive breakpoints\n- Multiple export formats\n",
      "frontmatter": {
        "name": "ui-design-system",
        "description": "UI design system toolkit for Senior UI Designer including design token generation, component documentation, responsive design calculations, and developer handoff tools. Use for creating design systems, maintaining visual consistency, and facilitating design-dev collaboration."
      },
      "content": "\n# UI Design System\n\nProfessional toolkit for creating and maintaining scalable design systems.\n\n## Core Capabilities\n- Design token generation (colors, typography, spacing)\n- Component system architecture\n- Responsive design calculations\n- Accessibility compliance\n- Developer handoff documentation\n\n## Key Scripts\n\n### design_token_generator.py\nGenerates complete design system tokens from brand colors.\n\n**Usage**: `python scripts/design_token_generator.py [brand_color] [style] [format]`\n- Styles: modern, classic, playful\n- Formats: json, css, scss\n\n**Features**:\n- Complete color palette generation\n- Modular typography scale\n- 8pt spacing grid system\n- Shadow and animation tokens\n- Responsive breakpoints\n- Multiple export formats\n"
    }
  },
  "alirezarezvani-claude-skills-code-reviewer": {
    "id": "alirezarezvani-claude-skills-code-reviewer",
    "name": "code-reviewer",
    "description": "Comprehensive code review skill for TypeScript, JavaScript, Python, Swift, Kotlin, Go. Includes automated code analysis, best practice checking, security scanning, and review checklist generation. Use when reviewing pull requests, providing code feedback, identifying issues, or ensuring code quality standards.",
    "repo": {
      "owner": "alirezarezvani",
      "name": "claude-skills",
      "fullName": "alirezarezvani/claude-skills",
      "url": "https://github.com/alirezarezvani/claude-skills/tree/main/engineering-team/code-reviewer",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 521,
      "forks": 103,
      "language": "Python",
      "topics": [
        "agentic-ai",
        "agentic-coding",
        "anthropic-claude",
        "claude-ai",
        "claude-ai-skills",
        "claude-code",
        "claude-code-skills",
        "claude-skills",
        "claudecode-subagents"
      ],
      "updatedAt": "2026-01-08T12:36:14Z",
      "pushedAt": "2026-01-08T08:39:21Z",
      "createdAt": "2025-10-19T04:04:05Z",
      "license": "MIT License"
    },
    "category": "development",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: code-reviewer\ndescription: Comprehensive code review skill for TypeScript, JavaScript, Python, Swift, Kotlin, Go. Includes automated code analysis, best practice checking, security scanning, and review checklist generation. Use when reviewing pull requests, providing code feedback, identifying issues, or ensuring code quality standards.\n---\n\n# Code Reviewer\n\nComplete toolkit for code reviewer with modern tools and best practices.\n\n## Quick Start\n\n### Main Capabilities\n\nThis skill provides three core capabilities through automated scripts:\n\n```bash\n# Script 1: Pr Analyzer\npython scripts/pr_analyzer.py [options]\n\n# Script 2: Code Quality Checker\npython scripts/code_quality_checker.py [options]\n\n# Script 3: Review Report Generator\npython scripts/review_report_generator.py [options]\n```\n\n## Core Capabilities\n\n### 1. Pr Analyzer\n\nAutomated tool for pr analyzer tasks.\n\n**Features:**\n- Automated scaffolding\n- Best practices built-in\n- Configurable templates\n- Quality checks\n\n**Usage:**\n```bash\npython scripts/pr_analyzer.py <project-path> [options]\n```\n\n### 2. Code Quality Checker\n\nComprehensive analysis and optimization tool.\n\n**Features:**\n- Deep analysis\n- Performance metrics\n- Recommendations\n- Automated fixes\n\n**Usage:**\n```bash\npython scripts/code_quality_checker.py <target-path> [--verbose]\n```\n\n### 3. Review Report Generator\n\nAdvanced tooling for specialized tasks.\n\n**Features:**\n- Expert-level automation\n- Custom configurations\n- Integration ready\n- Production-grade output\n\n**Usage:**\n```bash\npython scripts/review_report_generator.py [arguments] [options]\n```\n\n## Reference Documentation\n\n### Code Review Checklist\n\nComprehensive guide available in `references/code_review_checklist.md`:\n\n- Detailed patterns and practices\n- Code examples\n- Best practices\n- Anti-patterns to avoid\n- Real-world scenarios\n\n### Coding Standards\n\nComplete workflow documentation in `references/coding_standards.md`:\n\n- Step-by-step processes\n- Optimization strategies\n- Tool integrations\n- Performance tuning\n- Troubleshooting guide\n\n### Common Antipatterns\n\nTechnical reference guide in `references/common_antipatterns.md`:\n\n- Technology stack details\n- Configuration examples\n- Integration patterns\n- Security considerations\n- Scalability guidelines\n\n## Tech Stack\n\n**Languages:** TypeScript, JavaScript, Python, Go, Swift, Kotlin\n**Frontend:** React, Next.js, React Native, Flutter\n**Backend:** Node.js, Express, GraphQL, REST APIs\n**Database:** PostgreSQL, Prisma, NeonDB, Supabase\n**DevOps:** Docker, Kubernetes, Terraform, GitHub Actions, CircleCI\n**Cloud:** AWS, GCP, Azure\n\n## Development Workflow\n\n### 1. Setup and Configuration\n\n```bash\n# Install dependencies\nnpm install\n# or\npip install -r requirements.txt\n\n# Configure environment\ncp .env.example .env\n```\n\n### 2. Run Quality Checks\n\n```bash\n# Use the analyzer script\npython scripts/code_quality_checker.py .\n\n# Review recommendations\n# Apply fixes\n```\n\n### 3. Implement Best Practices\n\nFollow the patterns and practices documented in:\n- `references/code_review_checklist.md`\n- `references/coding_standards.md`\n- `references/common_antipatterns.md`\n\n## Best Practices Summary\n\n### Code Quality\n- Follow established patterns\n- Write comprehensive tests\n- Document decisions\n- Review regularly\n\n### Performance\n- Measure before optimizing\n- Use appropriate caching\n- Optimize critical paths\n- Monitor in production\n\n### Security\n- Validate all inputs\n- Use parameterized queries\n- Implement proper authentication\n- Keep dependencies updated\n\n### Maintainability\n- Write clear code\n- Use consistent naming\n- Add helpful comments\n- Keep it simple\n\n## Common Commands\n\n```bash\n# Development\nnpm run dev\nnpm run build\nnpm run test\nnpm run lint\n\n# Analysis\npython scripts/code_quality_checker.py .\npython scripts/review_report_generator.py --analyze\n\n# Deployment\ndocker build -t app:latest .\ndocker-compose up -d\nkubectl apply -f k8s/\n```\n\n## Troubleshooting\n\n### Common Issues\n\nCheck the comprehensive troubleshooting section in `references/common_antipatterns.md`.\n\n### Getting Help\n\n- Review reference documentation\n- Check script output messages\n- Consult tech stack documentation\n- Review error logs\n\n## Resources\n\n- Pattern Reference: `references/code_review_checklist.md`\n- Workflow Guide: `references/coding_standards.md`\n- Technical Guide: `references/common_antipatterns.md`\n- Tool Scripts: `scripts/` directory\n",
      "frontmatter": {
        "name": "code-reviewer",
        "description": "Comprehensive code review skill for TypeScript, JavaScript, Python, Swift, Kotlin, Go. Includes automated code analysis, best practice checking, security scanning, and review checklist generation. Use when reviewing pull requests, providing code feedback, identifying issues, or ensuring code quality standards."
      },
      "content": "\n# Code Reviewer\n\nComplete toolkit for code reviewer with modern tools and best practices.\n\n## Quick Start\n\n### Main Capabilities\n\nThis skill provides three core capabilities through automated scripts:\n\n```bash\n# Script 1: Pr Analyzer\npython scripts/pr_analyzer.py [options]\n\n# Script 2: Code Quality Checker\npython scripts/code_quality_checker.py [options]\n\n# Script 3: Review Report Generator\npython scripts/review_report_generator.py [options]\n```\n\n## Core Capabilities\n\n### 1. Pr Analyzer\n\nAutomated tool for pr analyzer tasks.\n\n**Features:**\n- Automated scaffolding\n- Best practices built-in\n- Configurable templates\n- Quality checks\n\n**Usage:**\n```bash\npython scripts/pr_analyzer.py <project-path> [options]\n```\n\n### 2. Code Quality Checker\n\nComprehensive analysis and optimization tool.\n\n**Features:**\n- Deep analysis\n- Performance metrics\n- Recommendations\n- Automated fixes\n\n**Usage:**\n```bash\npython scripts/code_quality_checker.py <target-path> [--verbose]\n```\n\n### 3. Review Report Generator\n\nAdvanced tooling for specialized tasks.\n\n**Features:**\n- Expert-level automation\n- Custom configurations\n- Integration ready\n- Production-grade output\n\n**Usage:**\n```bash\npython scripts/review_report_generator.py [arguments] [options]\n```\n\n## Reference Documentation\n\n### Code Review Checklist\n\nComprehensive guide available in `references/code_review_checklist.md`:\n\n- Detailed patterns and practices\n- Code examples\n- Best practices\n- Anti-patterns to avoid\n- Real-world scenarios\n\n### Coding Standards\n\nComplete workflow documentation in `references/coding_standards.md`:\n\n- Step-by-step processes\n- Optimization strategies\n- Tool integrations\n- Performance tuning\n- Troubleshooting guide\n\n### Common Antipatterns\n\nTechnical reference guide in `references/common_antipatterns.md`:\n\n- Technology stack details\n- Configuration examples\n- Integration patterns\n- Security considerations\n- Scalability guidelines\n\n## Tech Stack\n\n**Languages:** TypeScript, JavaScript, Python, Go, Swift, Kotlin\n**Frontend:** React, Next.js, React Native, Flutter\n**Backend:** Node.js, Express, GraphQL, REST APIs\n**Database:** PostgreSQL, Prisma, NeonDB, Supabase\n**DevOps:** Docker, Kubernetes, Terraform, GitHub Actions, CircleCI\n**Cloud:** AWS, GCP, Azure\n\n## Development Workflow\n\n### 1. Setup and Configuration\n\n```bash\n# Install dependencies\nnpm install\n# or\npip install -r requirements.txt\n\n# Configure environment\ncp .env.example .env\n```\n\n### 2. Run Quality Checks\n\n```bash\n# Use the analyzer script\npython scripts/code_quality_checker.py .\n\n# Review recommendations\n# Apply fixes\n```\n\n### 3. Implement Best Practices\n\nFollow the patterns and practices documented in:\n- `references/code_review_checklist.md`\n- `references/coding_standards.md`\n- `references/common_antipatterns.md`\n\n## Best Practices Summary\n\n### Code Quality\n- Follow established patterns\n- Write comprehensive tests\n- Document decisions\n- Review regularly\n\n### Performance\n- Measure before optimizing\n- Use appropriate caching\n- Optimize critical paths\n- Monitor in production\n\n### Security\n- Validate all inputs\n- Use parameterized queries\n- Implement proper authentication\n- Keep dependencies updated\n\n### Maintainability\n- Write clear code\n- Use consistent naming\n- Add helpful comments\n- Keep it simple\n\n## Common Commands\n\n```bash\n# Development\nnpm run dev\nnpm run build\nnpm run test\nnpm run lint\n\n# Analysis\npython scripts/code_quality_checker.py .\npython scripts/review_report_generator.py --analyze\n\n# Deployment\ndocker build -t app:latest .\ndocker-compose up -d\nkubectl apply -f k8s/\n```\n\n## Troubleshooting\n\n### Common Issues\n\nCheck the comprehensive troubleshooting section in `references/common_antipatterns.md`.\n\n### Getting Help\n\n- Review reference documentation\n- Check script output messages\n- Consult tech stack documentation\n- Review error logs\n\n## Resources\n\n- Pattern Reference: `references/code_review_checklist.md`\n- Workflow Guide: `references/coding_standards.md`\n- Technical Guide: `references/common_antipatterns.md`\n- Tool Scripts: `scripts/` directory\n"
    }
  },
  "alirezarezvani-claude-skills-aws-solution-architect": {
    "id": "alirezarezvani-claude-skills-aws-solution-architect",
    "name": "aws-solution-architect",
    "description": "Expert AWS solution architecture for startups focusing on serverless, scalable, and cost-effective cloud infrastructure with modern DevOps practices and infrastructure-as-code",
    "repo": {
      "owner": "alirezarezvani",
      "name": "claude-skills",
      "fullName": "alirezarezvani/claude-skills",
      "url": "https://github.com/alirezarezvani/claude-skills/tree/main/engineering-team/aws-solution-architect",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 521,
      "forks": 103,
      "language": "Python",
      "topics": [
        "agentic-ai",
        "agentic-coding",
        "anthropic-claude",
        "claude-ai",
        "claude-ai-skills",
        "claude-code",
        "claude-code-skills",
        "claude-skills",
        "claudecode-subagents"
      ],
      "updatedAt": "2026-01-08T12:36:14Z",
      "pushedAt": "2026-01-08T08:39:21Z",
      "createdAt": "2025-10-19T04:04:05Z",
      "license": "MIT License"
    },
    "category": "development",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: aws-solution-architect\ndescription: Expert AWS solution architecture for startups focusing on serverless, scalable, and cost-effective cloud infrastructure with modern DevOps practices and infrastructure-as-code\n---\n\n# AWS Solution Architect for Startups\n\nThis skill provides comprehensive AWS architecture design expertise for startup companies, emphasizing serverless technologies, scalability, cost optimization, and modern cloud-native patterns.\n\n## Capabilities\n\n- **Serverless Architecture Design**: Lambda, API Gateway, DynamoDB, EventBridge, Step Functions, AppSync\n- **Infrastructure as Code**: CloudFormation, CDK (Cloud Development Kit), Terraform templates\n- **Scalable Application Architecture**: Auto-scaling, load balancing, multi-region deployment\n- **Data & Storage Solutions**: S3, RDS Aurora Serverless, DynamoDB, ElastiCache, Neptune\n- **Event-Driven Architecture**: EventBridge, SNS, SQS, Kinesis, Lambda triggers\n- **API Design**: API Gateway (REST & WebSocket), AppSync (GraphQL), rate limiting, authentication\n- **Authentication & Authorization**: Cognito, IAM, fine-grained access control, federated identity\n- **CI/CD Pipelines**: CodePipeline, CodeBuild, CodeDeploy, GitHub Actions integration\n- **Monitoring & Observability**: CloudWatch, X-Ray, CloudTrail, alarms, dashboards\n- **Cost Optimization**: Reserved instances, Savings Plans, right-sizing, budget alerts\n- **Security Best Practices**: VPC design, security groups, WAF, Secrets Manager, encryption\n- **Microservices Patterns**: Service mesh, API composition, saga patterns, CQRS\n- **Container Orchestration**: ECS Fargate, EKS (Kubernetes), App Runner\n- **Content Delivery**: CloudFront, edge locations, origin shield, caching strategies\n- **Database Migration**: DMS, schema conversion, zero-downtime migrations\n\n## Input Requirements\n\nArchitecture design requires:\n- **Application type**: Web app, mobile backend, data pipeline, microservices, SaaS platform\n- **Traffic expectations**: Users/day, requests/second, geographic distribution\n- **Data requirements**: Storage needs, database type, backup/retention policies\n- **Budget constraints**: Monthly spend limits, cost optimization priorities\n- **Team size & expertise**: Developer count, AWS experience level, DevOps maturity\n- **Compliance needs**: GDPR, HIPAA, SOC 2, PCI-DSS, data residency\n- **Availability requirements**: SLA targets, uptime goals, disaster recovery RPO/RTO\n\nFormats accepted:\n- Text description of application requirements\n- JSON with structured architecture specifications\n- Existing architecture diagrams or documentation\n- Current AWS resource inventory (for optimization)\n\n## Output Formats\n\nResults include:\n- **Architecture diagrams**: Visual representations using draw.io or Lucidchart format\n- **CloudFormation/CDK templates**: Infrastructure as Code (IaC) ready to deploy\n- **Terraform configurations**: Multi-cloud compatible infrastructure definitions\n- **Cost estimates**: Detailed monthly cost breakdown with optimization suggestions\n- **Security assessment**: Best practices checklist, compliance validation\n- **Deployment guides**: Step-by-step implementation instructions\n- **Runbooks**: Operational procedures, troubleshooting guides, disaster recovery plans\n- **Migration strategies**: Phased migration plans, rollback procedures\n\n## How to Use\n\n\"Design a serverless API backend for a mobile app with 100k users using Lambda and DynamoDB\"\n\"Create a cost-optimized architecture for a SaaS platform with multi-tenancy\"\n\"Generate CloudFormation template for a three-tier web application with auto-scaling\"\n\"Design event-driven microservices architecture using EventBridge and Step Functions\"\n\"Optimize my current AWS setup to reduce costs by 30%\"\n\n## Scripts\n\n- `architecture_designer.py`: Generates architecture patterns and service recommendations\n- `serverless_stack.py`: Creates serverless application stacks (Lambda, API Gateway, DynamoDB)\n- `cost_optimizer.py`: Analyzes AWS costs and provides optimization recommendations\n- `iac_generator.py`: Generates CloudFormation, CDK, or Terraform templates\n- `security_auditor.py`: AWS security best practices validation and compliance checks\n\n## Architecture Patterns\n\n### 1. Serverless Web Application\n**Use Case**: SaaS platforms, mobile backends, low-traffic websites\n\n**Stack**:\n- **Frontend**: S3 + CloudFront (static hosting)\n- **API**: API Gateway + Lambda\n- **Database**: DynamoDB or Aurora Serverless\n- **Auth**: Cognito\n- **CI/CD**: Amplify or CodePipeline\n\n**Benefits**: Zero server management, pay-per-use, auto-scaling, low operational overhead\n\n**Cost**: $50-500/month for small to medium traffic\n\n### 2. Event-Driven Microservices\n**Use Case**: Complex business workflows, asynchronous processing, decoupled systems\n\n**Stack**:\n- **Events**: EventBridge (event bus)\n- **Processing**: Lambda functions or ECS Fargate\n- **Queue**: SQS (dead letter queues for failures)\n- **State Management**: Step Functions\n- **Storage**: DynamoDB, S3\n\n**Benefits**: Loose coupling, independent scaling, failure isolation, easy testing\n\n**Cost**: $100-1000/month depending on event volume\n\n### 3. Modern Three-Tier Application\n**Use Case**: Traditional web apps with dynamic content, e-commerce, CMS\n\n**Stack**:\n- **Load Balancer**: ALB (Application Load Balancer)\n- **Compute**: ECS Fargate or EC2 Auto Scaling\n- **Database**: RDS Aurora (MySQL/PostgreSQL)\n- **Cache**: ElastiCache (Redis)\n- **CDN**: CloudFront\n- **Storage**: S3\n\n**Benefits**: Proven pattern, easy to understand, flexible scaling\n\n**Cost**: $300-2000/month depending on traffic and instance sizes\n\n### 4. Real-Time Data Processing\n**Use Case**: Analytics, IoT data ingestion, log processing, streaming\n\n**Stack**:\n- **Ingestion**: Kinesis Data Streams or Firehose\n- **Processing**: Lambda or Kinesis Analytics\n- **Storage**: S3 (data lake) + Athena (queries)\n- **Visualization**: QuickSight\n- **Alerting**: CloudWatch + SNS\n\n**Benefits**: Handle millions of events, real-time insights, cost-effective storage\n\n**Cost**: $200-1500/month depending on data volume\n\n### 5. GraphQL API Backend\n**Use Case**: Mobile apps, single-page applications, flexible data queries\n\n**Stack**:\n- **API**: AppSync (managed GraphQL)\n- **Resolvers**: Lambda or direct DynamoDB integration\n- **Database**: DynamoDB\n- **Real-time**: AppSync subscriptions (WebSocket)\n- **Auth**: Cognito or API keys\n\n**Benefits**: Single endpoint, reduce over/under-fetching, real-time subscriptions\n\n**Cost**: $50-400/month for moderate usage\n\n### 6. Multi-Region High Availability\n**Use Case**: Global applications, disaster recovery, compliance requirements\n\n**Stack**:\n- **DNS**: Route 53 (geolocation routing)\n- **CDN**: CloudFront with multiple origins\n- **Compute**: Multi-region Lambda or ECS\n- **Database**: DynamoDB Global Tables or Aurora Global Database\n- **Replication**: S3 cross-region replication\n\n**Benefits**: Low latency globally, disaster recovery, data sovereignty\n\n**Cost**: 1.5-2x single region costs\n\n## Best Practices\n\n### Serverless Design Principles\n1. **Stateless functions** - Store state in DynamoDB, S3, or ElastiCache\n2. **Idempotency** - Handle retries gracefully, use unique request IDs\n3. **Cold start optimization** - Use provisioned concurrency for critical paths, optimize package size\n4. **Timeout management** - Set appropriate timeouts, use Step Functions for long processes\n5. **Error handling** - Implement retry logic, dead letter queues, exponential backoff\n\n### Cost Optimization\n1. **Right-sizing** - Start small, monitor metrics, scale based on actual usage\n2. **Reserved capacity** - Use Savings Plans or Reserved Instances for predictable workloads\n3. **S3 lifecycle policies** - Transition to cheaper storage tiers (IA, Glacier)\n4. **Lambda memory optimization** - Test different memory settings for cost/performance balance\n5. **CloudWatch log retention** - Set appropriate retention periods (7-30 days for most)\n6. **NAT Gateway alternatives** - Use VPC endpoints, consider single NAT in dev environments\n\n### Security Hardening\n1. **Principle of least privilege** - IAM roles with minimal permissions\n2. **Encryption everywhere** - At rest (KMS) and in transit (TLS/SSL)\n3. **Network isolation** - Private subnets, security groups, NACLs\n4. **Secrets management** - Use Secrets Manager or Parameter Store, never hardcode\n5. **API protection** - WAF rules, rate limiting, API keys, OAuth2\n6. **Audit logging** - CloudTrail for API calls, VPC Flow Logs for network traffic\n\n### Scalability Design\n1. **Horizontal over vertical** - Scale out with more small instances vs. larger instances\n2. **Database sharding** - Partition data by tenant, geography, or time\n3. **Read replicas** - Offload read traffic from primary database\n4. **Caching layers** - CloudFront (edge), ElastiCache (application), DAX (DynamoDB)\n5. **Async processing** - Use queues (SQS) for non-critical operations\n6. **Auto-scaling policies** - Target tracking (CPU, requests) vs. step scaling\n\n### DevOps & Reliability\n1. **Infrastructure as Code** - Version control, peer review, automated testing\n2. **Blue/Green deployments** - Zero-downtime releases, instant rollback\n3. **Canary releases** - Test new versions with small traffic percentage\n4. **Health checks** - Application-level health endpoints, graceful degradation\n5. **Chaos engineering** - Test failure scenarios, validate recovery procedures\n6. **Monitoring & alerting** - Set up CloudWatch alarms for critical metrics\n\n## Service Selection Guide\n\n### Compute\n- **Lambda**: Event-driven, short-duration tasks (<15 min), variable traffic\n- **Fargate**: Containerized apps, long-running processes, predictable traffic\n- **EC2**: Custom configurations, GPU/FPGA needs, Windows apps\n- **App Runner**: Simple container deployment from source code\n\n### Database\n- **DynamoDB**: Key-value, document store, serverless, single-digit ms latency\n- **Aurora Serverless**: Relational DB, variable workloads, auto-scaling\n- **Aurora Standard**: High-performance relational, predictable traffic\n- **RDS**: Traditional databases (MySQL, PostgreSQL, MariaDB, SQL Server)\n- **DocumentDB**: MongoDB-compatible, document store\n- **Neptune**: Graph database for connected data\n- **Timestream**: Time-series data, IoT metrics\n\n### Storage\n- **S3 Standard**: Frequent access, low latency\n- **S3 Intelligent-Tiering**: Automatic cost optimization\n- **S3 IA (Infrequent Access)**: Backups, archives (30-day minimum)\n- **S3 Glacier**: Long-term archives, compliance\n- **EFS**: Network file system, shared storage across instances\n- **EBS**: Block storage for EC2, high IOPS\n\n### Messaging & Events\n- **EventBridge**: Event bus, loosely coupled microservices\n- **SNS**: Pub/sub, fan-out notifications\n- **SQS**: Message queuing, decoupling, buffering\n- **Kinesis**: Real-time streaming data, analytics\n- **MQ**: Managed message brokers (RabbitMQ, ActiveMQ)\n\n### API & Integration\n- **API Gateway**: REST APIs, WebSocket, throttling, caching\n- **AppSync**: GraphQL APIs, real-time subscriptions\n- **AppFlow**: SaaS integration (Salesforce, Slack, etc.)\n- **Step Functions**: Workflow orchestration, state machines\n\n## Startup-Specific Considerations\n\n### MVP (Minimum Viable Product) Architecture\n**Goal**: Launch fast, minimal infrastructure\n\n**Recommended**:\n- Amplify (full-stack deployment)\n- Lambda + API Gateway + DynamoDB\n- Cognito for auth\n- CloudFront + S3 for frontend\n\n**Cost**: $20-100/month\n**Setup time**: 1-3 days\n\n### Growth Stage (Scaling to 10k-100k users)\n**Goal**: Handle growth, maintain cost efficiency\n\n**Add**:\n- ElastiCache for caching\n- Aurora Serverless for complex queries\n- CloudWatch dashboards and alarms\n- CI/CD pipeline (CodePipeline)\n- Multi-AZ deployment\n\n**Cost**: $500-2000/month\n**Migration time**: 1-2 weeks\n\n### Scale-Up (100k+ users, Series A+)\n**Goal**: Reliability, observability, global reach\n\n**Add**:\n- Multi-region deployment\n- DynamoDB Global Tables\n- Advanced monitoring (X-Ray, third-party APM)\n- WAF and Shield for DDoS protection\n- Dedicated support plan\n- Reserved instances/Savings Plans\n\n**Cost**: $3000-10000/month\n**Migration time**: 1-3 months\n\n## Common Pitfalls to Avoid\n\n### Technical Debt\n- **Over-engineering early** - Don't build for 10M users when you have 100\n- **Under-monitoring** - Set up basic monitoring from day one\n- **Ignoring costs** - Enable Cost Explorer and billing alerts immediately\n- **Single region dependency** - Plan for multi-region from start\n\n### Security Mistakes\n- **Public S3 buckets** - Use bucket policies, block public access\n- **Overly permissive IAM** - Avoid \"*\" permissions, use specific resources\n- **Hardcoded credentials** - Use IAM roles, Secrets Manager\n- **Unencrypted data** - Enable encryption by default\n\n### Performance Issues\n- **No caching** - Add CloudFront, ElastiCache early\n- **Inefficient queries** - Use indexes, avoid scans in DynamoDB\n- **Large Lambda packages** - Use layers, minimize dependencies\n- **N+1 queries** - Implement DataLoader pattern, batch operations\n\n### Cost Surprises\n- **Undeleted resources** - Tag everything, review regularly\n- **Data transfer costs** - Keep traffic within same AZ/region when possible\n- **NAT Gateway charges** - Use VPC endpoints for AWS services\n- **CloudWatch Logs accumulation** - Set retention policies\n\n## Compliance & Governance\n\n### Data Residency\n- Use specific regions (eu-west-1 for GDPR)\n- Enable S3 bucket replication restrictions\n- Configure Route 53 geolocation routing\n\n### HIPAA Compliance\n- Use BAA-eligible services only\n- Enable encryption at rest and in transit\n- Implement audit logging (CloudTrail)\n- Configure VPC with private subnets\n\n### SOC 2 / ISO 27001\n- Enable AWS Config for compliance rules\n- Use AWS Audit Manager\n- Implement least privilege access\n- Regular security assessments\n\n## Limitations\n\n- **Lambda limitations**: 15-minute execution limit, 10GB memory max, cold start latency\n- **API Gateway limits**: 29-second timeout, 10MB payload size\n- **DynamoDB limits**: 400KB item size, eventually consistent reads by default\n- **Regional availability**: Not all services available in all regions\n- **Vendor lock-in**: Some serverless services are AWS-specific (consider abstraction layers)\n- **Learning curve**: Requires AWS expertise, DevOps knowledge\n- **Debugging complexity**: Distributed systems harder to troubleshoot than monoliths\n\n## Helpful Resources\n\n- **AWS Well-Architected Framework**: https://aws.amazon.com/architecture/well-architected/\n- **AWS Architecture Center**: https://aws.amazon.com/architecture/\n- **Serverless Land**: https://serverlessland.com/\n- **AWS Pricing Calculator**: https://calculator.aws/\n- **AWS Cost Explorer**: Track and analyze spending\n- **AWS Trusted Advisor**: Automated best practice checks\n- **CloudFormation Templates**: https://github.com/awslabs/aws-cloudformation-templates\n- **AWS CDK Examples**: https://github.com/aws-samples/aws-cdk-examples\n",
      "frontmatter": {
        "name": "aws-solution-architect",
        "description": "Expert AWS solution architecture for startups focusing on serverless, scalable, and cost-effective cloud infrastructure with modern DevOps practices and infrastructure-as-code"
      },
      "content": "\n# AWS Solution Architect for Startups\n\nThis skill provides comprehensive AWS architecture design expertise for startup companies, emphasizing serverless technologies, scalability, cost optimization, and modern cloud-native patterns.\n\n## Capabilities\n\n- **Serverless Architecture Design**: Lambda, API Gateway, DynamoDB, EventBridge, Step Functions, AppSync\n- **Infrastructure as Code**: CloudFormation, CDK (Cloud Development Kit), Terraform templates\n- **Scalable Application Architecture**: Auto-scaling, load balancing, multi-region deployment\n- **Data & Storage Solutions**: S3, RDS Aurora Serverless, DynamoDB, ElastiCache, Neptune\n- **Event-Driven Architecture**: EventBridge, SNS, SQS, Kinesis, Lambda triggers\n- **API Design**: API Gateway (REST & WebSocket), AppSync (GraphQL), rate limiting, authentication\n- **Authentication & Authorization**: Cognito, IAM, fine-grained access control, federated identity\n- **CI/CD Pipelines**: CodePipeline, CodeBuild, CodeDeploy, GitHub Actions integration\n- **Monitoring & Observability**: CloudWatch, X-Ray, CloudTrail, alarms, dashboards\n- **Cost Optimization**: Reserved instances, Savings Plans, right-sizing, budget alerts\n- **Security Best Practices**: VPC design, security groups, WAF, Secrets Manager, encryption\n- **Microservices Patterns**: Service mesh, API composition, saga patterns, CQRS\n- **Container Orchestration**: ECS Fargate, EKS (Kubernetes), App Runner\n- **Content Delivery**: CloudFront, edge locations, origin shield, caching strategies\n- **Database Migration**: DMS, schema conversion, zero-downtime migrations\n\n## Input Requirements\n\nArchitecture design requires:\n- **Application type**: Web app, mobile backend, data pipeline, microservices, SaaS platform\n- **Traffic expectations**: Users/day, requests/second, geographic distribution\n- **Data requirements**: Storage needs, database type, backup/retention policies\n- **Budget constraints**: Monthly spend limits, cost optimization priorities\n- **Team size & expertise**: Developer count, AWS experience level, DevOps maturity\n- **Compliance needs**: GDPR, HIPAA, SOC 2, PCI-DSS, data residency\n- **Availability requirements**: SLA targets, uptime goals, disaster recovery RPO/RTO\n\nFormats accepted:\n- Text description of application requirements\n- JSON with structured architecture specifications\n- Existing architecture diagrams or documentation\n- Current AWS resource inventory (for optimization)\n\n## Output Formats\n\nResults include:\n- **Architecture diagrams**: Visual representations using draw.io or Lucidchart format\n- **CloudFormation/CDK templates**: Infrastructure as Code (IaC) ready to deploy\n- **Terraform configurations**: Multi-cloud compatible infrastructure definitions\n- **Cost estimates**: Detailed monthly cost breakdown with optimization suggestions\n- **Security assessment**: Best practices checklist, compliance validation\n- **Deployment guides**: Step-by-step implementation instructions\n- **Runbooks**: Operational procedures, troubleshooting guides, disaster recovery plans\n- **Migration strategies**: Phased migration plans, rollback procedures\n\n## How to Use\n\n\"Design a serverless API backend for a mobile app with 100k users using Lambda and DynamoDB\"\n\"Create a cost-optimized architecture for a SaaS platform with multi-tenancy\"\n\"Generate CloudFormation template for a three-tier web application with auto-scaling\"\n\"Design event-driven microservices architecture using EventBridge and Step Functions\"\n\"Optimize my current AWS setup to reduce costs by 30%\"\n\n## Scripts\n\n- `architecture_designer.py`: Generates architecture patterns and service recommendations\n- `serverless_stack.py`: Creates serverless application stacks (Lambda, API Gateway, DynamoDB)\n- `cost_optimizer.py`: Analyzes AWS costs and provides optimization recommendations\n- `iac_generator.py`: Generates CloudFormation, CDK, or Terraform templates\n- `security_auditor.py`: AWS security best practices validation and compliance checks\n\n## Architecture Patterns\n\n### 1. Serverless Web Application\n**Use Case**: SaaS platforms, mobile backends, low-traffic websites\n\n**Stack**:\n- **Frontend**: S3 + CloudFront (static hosting)\n- **API**: API Gateway + Lambda\n- **Database**: DynamoDB or Aurora Serverless\n- **Auth**: Cognito\n- **CI/CD**: Amplify or CodePipeline\n\n**Benefits**: Zero server management, pay-per-use, auto-scaling, low operational overhead\n\n**Cost**: $50-500/month for small to medium traffic\n\n### 2. Event-Driven Microservices\n**Use Case**: Complex business workflows, asynchronous processing, decoupled systems\n\n**Stack**:\n- **Events**: EventBridge (event bus)\n- **Processing**: Lambda functions or ECS Fargate\n- **Queue**: SQS (dead letter queues for failures)\n- **State Management**: Step Functions\n- **Storage**: DynamoDB, S3\n\n**Benefits**: Loose coupling, independent scaling, failure isolation, easy testing\n\n**Cost**: $100-1000/month depending on event volume\n\n### 3. Modern Three-Tier Application\n**Use Case**: Traditional web apps with dynamic content, e-commerce, CMS\n\n**Stack**:\n- **Load Balancer**: ALB (Application Load Balancer)\n- **Compute**: ECS Fargate or EC2 Auto Scaling\n- **Database**: RDS Aurora (MySQL/PostgreSQL)\n- **Cache**: ElastiCache (Redis)\n- **CDN**: CloudFront\n- **Storage**: S3\n\n**Benefits**: Proven pattern, easy to understand, flexible scaling\n\n**Cost**: $300-2000/month depending on traffic and instance sizes\n\n### 4. Real-Time Data Processing\n**Use Case**: Analytics, IoT data ingestion, log processing, streaming\n\n**Stack**:\n- **Ingestion**: Kinesis Data Streams or Firehose\n- **Processing**: Lambda or Kinesis Analytics\n- **Storage**: S3 (data lake) + Athena (queries)\n- **Visualization**: QuickSight\n- **Alerting**: CloudWatch + SNS\n\n**Benefits**: Handle millions of events, real-time insights, cost-effective storage\n\n**Cost**: $200-1500/month depending on data volume\n\n### 5. GraphQL API Backend\n**Use Case**: Mobile apps, single-page applications, flexible data queries\n\n**Stack**:\n- **API**: AppSync (managed GraphQL)\n- **Resolvers**: Lambda or direct DynamoDB integration\n- **Database**: DynamoDB\n- **Real-time**: AppSync subscriptions (WebSocket)\n- **Auth**: Cognito or API keys\n\n**Benefits**: Single endpoint, reduce over/under-fetching, real-time subscriptions\n\n**Cost**: $50-400/month for moderate usage\n\n### 6. Multi-Region High Availability\n**Use Case**: Global applications, disaster recovery, compliance requirements\n\n**Stack**:\n- **DNS**: Route 53 (geolocation routing)\n- **CDN**: CloudFront with multiple origins\n- **Compute**: Multi-region Lambda or ECS\n- **Database**: DynamoDB Global Tables or Aurora Global Database\n- **Replication**: S3 cross-region replication\n\n**Benefits**: Low latency globally, disaster recovery, data sovereignty\n\n**Cost**: 1.5-2x single region costs\n\n## Best Practices\n\n### Serverless Design Principles\n1. **Stateless functions** - Store state in DynamoDB, S3, or ElastiCache\n2. **Idempotency** - Handle retries gracefully, use unique request IDs\n3. **Cold start optimization** - Use provisioned concurrency for critical paths, optimize package size\n4. **Timeout management** - Set appropriate timeouts, use Step Functions for long processes\n5. **Error handling** - Implement retry logic, dead letter queues, exponential backoff\n\n### Cost Optimization\n1. **Right-sizing** - Start small, monitor metrics, scale based on actual usage\n2. **Reserved capacity** - Use Savings Plans or Reserved Instances for predictable workloads\n3. **S3 lifecycle policies** - Transition to cheaper storage tiers (IA, Glacier)\n4. **Lambda memory optimization** - Test different memory settings for cost/performance balance\n5. **CloudWatch log retention** - Set appropriate retention periods (7-30 days for most)\n6. **NAT Gateway alternatives** - Use VPC endpoints, consider single NAT in dev environments\n\n### Security Hardening\n1. **Principle of least privilege** - IAM roles with minimal permissions\n2. **Encryption everywhere** - At rest (KMS) and in transit (TLS/SSL)\n3. **Network isolation** - Private subnets, security groups, NACLs\n4. **Secrets management** - Use Secrets Manager or Parameter Store, never hardcode\n5. **API protection** - WAF rules, rate limiting, API keys, OAuth2\n6. **Audit logging** - CloudTrail for API calls, VPC Flow Logs for network traffic\n\n### Scalability Design\n1. **Horizontal over vertical** - Scale out with more small instances vs. larger instances\n2. **Database sharding** - Partition data by tenant, geography, or time\n3. **Read replicas** - Offload read traffic from primary database\n4. **Caching layers** - CloudFront (edge), ElastiCache (application), DAX (DynamoDB)\n5. **Async processing** - Use queues (SQS) for non-critical operations\n6. **Auto-scaling policies** - Target tracking (CPU, requests) vs. step scaling\n\n### DevOps & Reliability\n1. **Infrastructure as Code** - Version control, peer review, automated testing\n2. **Blue/Green deployments** - Zero-downtime releases, instant rollback\n3. **Canary releases** - Test new versions with small traffic percentage\n4. **Health checks** - Application-level health endpoints, graceful degradation\n5. **Chaos engineering** - Test failure scenarios, validate recovery procedures\n6. **Monitoring & alerting** - Set up CloudWatch alarms for critical metrics\n\n## Service Selection Guide\n\n### Compute\n- **Lambda**: Event-driven, short-duration tasks (<15 min), variable traffic\n- **Fargate**: Containerized apps, long-running processes, predictable traffic\n- **EC2**: Custom configurations, GPU/FPGA needs, Windows apps\n- **App Runner**: Simple container deployment from source code\n\n### Database\n- **DynamoDB**: Key-value, document store, serverless, single-digit ms latency\n- **Aurora Serverless**: Relational DB, variable workloads, auto-scaling\n- **Aurora Standard**: High-performance relational, predictable traffic\n- **RDS**: Traditional databases (MySQL, PostgreSQL, MariaDB, SQL Server)\n- **DocumentDB**: MongoDB-compatible, document store\n- **Neptune**: Graph database for connected data\n- **Timestream**: Time-series data, IoT metrics\n\n### Storage\n- **S3 Standard**: Frequent access, low latency\n- **S3 Intelligent-Tiering**: Automatic cost optimization\n- **S3 IA (Infrequent Access)**: Backups, archives (30-day minimum)\n- **S3 Glacier**: Long-term archives, compliance\n- **EFS**: Network file system, shared storage across instances\n- **EBS**: Block storage for EC2, high IOPS\n\n### Messaging & Events\n- **EventBridge**: Event bus, loosely coupled microservices\n- **SNS**: Pub/sub, fan-out notifications\n- **SQS**: Message queuing, decoupling, buffering\n- **Kinesis**: Real-time streaming data, analytics\n- **MQ**: Managed message brokers (RabbitMQ, ActiveMQ)\n\n### API & Integration\n- **API Gateway**: REST APIs, WebSocket, throttling, caching\n- **AppSync**: GraphQL APIs, real-time subscriptions\n- **AppFlow**: SaaS integration (Salesforce, Slack, etc.)\n- **Step Functions**: Workflow orchestration, state machines\n\n## Startup-Specific Considerations\n\n### MVP (Minimum Viable Product) Architecture\n**Goal**: Launch fast, minimal infrastructure\n\n**Recommended**:\n- Amplify (full-stack deployment)\n- Lambda + API Gateway + DynamoDB\n- Cognito for auth\n- CloudFront + S3 for frontend\n\n**Cost**: $20-100/month\n**Setup time**: 1-3 days\n\n### Growth Stage (Scaling to 10k-100k users)\n**Goal**: Handle growth, maintain cost efficiency\n\n**Add**:\n- ElastiCache for caching\n- Aurora Serverless for complex queries\n- CloudWatch dashboards and alarms\n- CI/CD pipeline (CodePipeline)\n- Multi-AZ deployment\n\n**Cost**: $500-2000/month\n**Migration time**: 1-2 weeks\n\n### Scale-Up (100k+ users, Series A+)\n**Goal**: Reliability, observability, global reach\n\n**Add**:\n- Multi-region deployment\n- DynamoDB Global Tables\n- Advanced monitoring (X-Ray, third-party APM)\n- WAF and Shield for DDoS protection\n- Dedicated support plan\n- Reserved instances/Savings Plans\n\n**Cost**: $3000-10000/month\n**Migration time**: 1-3 months\n\n## Common Pitfalls to Avoid\n\n### Technical Debt\n- **Over-engineering early** - Don't build for 10M users when you have 100\n- **Under-monitoring** - Set up basic monitoring from day one\n- **Ignoring costs** - Enable Cost Explorer and billing alerts immediately\n- **Single region dependency** - Plan for multi-region from start\n\n### Security Mistakes\n- **Public S3 buckets** - Use bucket policies, block public access\n- **Overly permissive IAM** - Avoid \"*\" permissions, use specific resources\n- **Hardcoded credentials** - Use IAM roles, Secrets Manager\n- **Unencrypted data** - Enable encryption by default\n\n### Performance Issues\n- **No caching** - Add CloudFront, ElastiCache early\n- **Inefficient queries** - Use indexes, avoid scans in DynamoDB\n- **Large Lambda packages** - Use layers, minimize dependencies\n- **N+1 queries** - Implement DataLoader pattern, batch operations\n\n### Cost Surprises\n- **Undeleted resources** - Tag everything, review regularly\n- **Data transfer costs** - Keep traffic within same AZ/region when possible\n- **NAT Gateway charges** - Use VPC endpoints for AWS services\n- **CloudWatch Logs accumulation** - Set retention policies\n\n## Compliance & Governance\n\n### Data Residency\n- Use specific regions (eu-west-1 for GDPR)\n- Enable S3 bucket replication restrictions\n- Configure Route 53 geolocation routing\n\n### HIPAA Compliance\n- Use BAA-eligible services only\n- Enable encryption at rest and in transit\n- Implement audit logging (CloudTrail)\n- Configure VPC with private subnets\n\n### SOC 2 / ISO 27001\n- Enable AWS Config for compliance rules\n- Use AWS Audit Manager\n- Implement least privilege access\n- Regular security assessments\n\n## Limitations\n\n- **Lambda limitations**: 15-minute execution limit, 10GB memory max, cold start latency\n- **API Gateway limits**: 29-second timeout, 10MB payload size\n- **DynamoDB limits**: 400KB item size, eventually consistent reads by default\n- **Regional availability**: Not all services available in all regions\n- **Vendor lock-in**: Some serverless services are AWS-specific (consider abstraction layers)\n- **Learning curve**: Requires AWS expertise, DevOps knowledge\n- **Debugging complexity**: Distributed systems harder to troubleshoot than monoliths\n\n## Helpful Resources\n\n- **AWS Well-Architected Framework**: https://aws.amazon.com/architecture/well-architected/\n- **AWS Architecture Center**: https://aws.amazon.com/architecture/\n- **Serverless Land**: https://serverlessland.com/\n- **AWS Pricing Calculator**: https://calculator.aws/\n- **AWS Cost Explorer**: Track and analyze spending\n- **AWS Trusted Advisor**: Automated best practice checks\n- **CloudFormation Templates**: https://github.com/awslabs/aws-cloudformation-templates\n- **AWS CDK Examples**: https://github.com/aws-samples/aws-cdk-examples\n"
    }
  },
  "alirezarezvani-claude-skills-ms365-tenant-manager": {
    "id": "alirezarezvani-claude-skills-ms365-tenant-manager",
    "name": "ms365-tenant-manager",
    "description": "Comprehensive Microsoft 365 tenant administration skill for setup, configuration, user management, security policies, and organizational structure optimization for Global Administrators",
    "repo": {
      "owner": "alirezarezvani",
      "name": "claude-skills",
      "fullName": "alirezarezvani/claude-skills",
      "url": "https://github.com/alirezarezvani/claude-skills/tree/main/engineering-team/ms365-tenant-manager",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 521,
      "forks": 103,
      "language": "Python",
      "topics": [
        "agentic-ai",
        "agentic-coding",
        "anthropic-claude",
        "claude-ai",
        "claude-ai-skills",
        "claude-code",
        "claude-code-skills",
        "claude-skills",
        "claudecode-subagents"
      ],
      "updatedAt": "2026-01-08T12:36:14Z",
      "pushedAt": "2026-01-08T08:39:21Z",
      "createdAt": "2025-10-19T04:04:05Z",
      "license": "MIT License"
    },
    "category": "development",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: ms365-tenant-manager\ndescription: Comprehensive Microsoft 365 tenant administration skill for setup, configuration, user management, security policies, and organizational structure optimization for Global Administrators\n---\n\n# Microsoft 365 Tenant Manager\n\nThis skill provides expert guidance and automation for Microsoft 365 Global Administrators managing tenant setup, configuration, user lifecycle, security policies, and organizational optimization.\n\n## Capabilities\n\n- **Tenant Setup & Configuration**: Initial tenant setup, domain configuration, DNS records, service provisioning\n- **User & Group Management**: User lifecycle (create, modify, disable, delete), group creation, license assignment\n- **Security & Compliance**: Conditional Access policies, MFA setup, DLP policies, retention policies, security baselines\n- **SharePoint & OneDrive**: Site provisioning, permissions management, storage quotas, sharing policies\n- **Teams Administration**: Team creation, policy management, guest access, compliance settings\n- **Exchange Online**: Mailbox management, distribution groups, mail flow rules, anti-spam/malware policies\n- **License Management**: License allocation, optimization, cost analysis, usage reporting\n- **Reporting & Auditing**: Activity reports, audit logs, compliance reporting, usage analytics\n- **Automation Scripts**: PowerShell script generation for bulk operations and recurring tasks\n- **Best Practices**: Microsoft recommended configurations, security hardening, governance frameworks\n\n## Input Requirements\n\nTenant management tasks require:\n- **Action type**: setup, configure, create, modify, delete, report, audit\n- **Resource details**: User info, group names, policy settings, service configurations\n- **Organizational context**: Company size, industry, compliance requirements (GDPR, HIPAA, etc.)\n- **Current state**: Existing configurations, licenses, user count\n- **Desired outcome**: Specific goals, requirements, or changes needed\n\nFormats accepted:\n- Text descriptions of administrative tasks\n- JSON with structured configuration data\n- CSV for bulk user/group operations\n- Existing PowerShell scripts to review or modify\n\n## Output Formats\n\nResults include:\n- **Step-by-step instructions**: Detailed guidance for manual configuration via Admin Center\n- **PowerShell scripts**: Ready-to-use scripts for automation (with safety checks)\n- **Configuration recommendations**: Security and governance best practices\n- **Validation checklists**: Pre/post-implementation verification steps\n- **Documentation**: Markdown documentation of changes and configurations\n- **Rollback procedures**: Instructions to undo changes if needed\n- **Compliance reports**: Security posture and compliance status\n\n## How to Use\n\n\"Set up a new Microsoft 365 tenant for a 50-person company with security best practices\"\n\"Create a PowerShell script to provision 100 users from a CSV file with appropriate licenses\"\n\"Configure Conditional Access policy requiring MFA for all admin accounts\"\n\"Generate a report of all inactive users in the past 90 days\"\n\"Set up Teams policies for external collaboration with security controls\"\n\n## Scripts\n\n- `tenant_setup.py`: Initial tenant configuration and service provisioning automation\n- `user_management.py`: User lifecycle operations and bulk provisioning\n- `security_policies.py`: Security policy configuration and compliance checks\n- `reporting.py`: Analytics, audit logs, and compliance reporting\n- `powershell_generator.py`: Generates PowerShell scripts for Microsoft Graph API and admin modules\n\n## Best Practices\n\n### Tenant Setup\n1. **Enable MFA first** - Before adding users, enforce multi-factor authentication\n2. **Configure named locations** - Define trusted IP ranges for Conditional Access\n3. **Set up privileged access** - Use separate admin accounts, enable PIM (Privileged Identity Management)\n4. **Domain verification** - Add and verify custom domains before bulk user creation\n5. **Baseline security** - Apply Microsoft Secure Score recommendations immediately\n\n### User Management\n1. **License assignment** - Use group-based licensing for scalability\n2. **Naming conventions** - Establish consistent user principal names (UPNs) and display names\n3. **Lifecycle management** - Implement automated onboarding/offboarding workflows\n4. **Guest access** - Enable only when necessary, set expiration policies\n5. **Shared mailboxes** - Use for department emails instead of assigning licenses\n\n### Security & Compliance\n1. **Zero Trust approach** - Verify explicitly, use least privilege access, assume breach\n2. **Conditional Access** - Start with report-only mode, then enforce gradually\n3. **Data Loss Prevention** - Define sensitive information types, test policies before enforcement\n4. **Retention policies** - Balance compliance requirements with storage costs\n5. **Regular audits** - Review permissions, licenses, and security settings quarterly\n\n### SharePoint & Teams\n1. **Site provisioning** - Use templates and governance policies\n2. **External sharing** - Restrict to specific domains, require authentication\n3. **Storage management** - Set quotas, enable auto-cleanup of old content\n4. **Teams templates** - Create standardized team structures for consistency\n5. **Guest lifecycle** - Set expiration and regular recertification\n\n### PowerShell Automation\n1. **Use Microsoft Graph** - Prefer Graph API over legacy MSOnline modules\n2. **Error handling** - Include try/catch blocks and validation checks\n3. **Dry-run mode** - Test scripts with -WhatIf before executing\n4. **Logging** - Capture all operations for audit trails\n5. **Credential management** - Use Azure Key Vault or managed identities, never hardcode\n\n## Common Tasks\n\n### Initial Tenant Setup\n- Configure company branding\n- Add and verify custom domains\n- Set up DNS records (MX, SPF, DKIM, DMARC)\n- Enable required services (Teams, SharePoint, Exchange)\n- Create organizational structure (departments, locations)\n- Set default user settings and policies\n\n### User Onboarding\n- Create user accounts (single or bulk)\n- Assign appropriate licenses\n- Add to security and distribution groups\n- Configure mailbox and OneDrive\n- Set up multi-factor authentication\n- Provision Teams access\n\n### Security Hardening\n- Enable Security Defaults or Conditional Access\n- Configure MFA enforcement\n- Set up admin role assignments\n- Enable audit logging\n- Configure anti-phishing policies\n- Set up DLP and retention policies\n\n### Reporting & Monitoring\n- Active users and license utilization\n- Security incidents and alerts\n- Mailbox usage and storage\n- SharePoint site activity\n- Teams usage and adoption\n- Compliance and audit logs\n\n## Limitations\n\n- **Permissions required**: Global Administrator or specific role-based permissions\n- **API rate limits**: Microsoft Graph API has throttling limits for bulk operations\n- **License dependencies**: Some features require specific license tiers (E3, E5)\n- **Delegation constraints**: Some tasks cannot be delegated to service principals\n- **Regional variations**: Compliance features may vary by geographic region\n- **Hybrid scenarios**: On-premises Active Directory integration requires additional configuration\n- **Third-party integrations**: External apps may require separate authentication and permissions\n- **PowerShell prerequisites**: Requires appropriate modules installed (Microsoft.Graph, ExchangeOnlineManagement, etc.)\n\n## Security Considerations\n\n### Authentication\n- Never store credentials in scripts or configuration files\n- Use Azure Key Vault for credential management\n- Implement certificate-based authentication for automation\n- Enable Conditional Access for admin accounts\n- Use Privileged Identity Management (PIM) for JIT access\n\n### Authorization\n- Follow principle of least privilege\n- Use custom admin roles instead of Global Admin when possible\n- Regularly review and audit admin role assignments\n- Enable PIM for temporary elevated access\n- Separate user accounts from admin accounts\n\n### Compliance\n- Enable audit logging for all activities\n- Retain logs according to compliance requirements\n- Configure data residency for regulated industries\n- Implement information barriers where needed\n- Regular compliance assessments and reporting\n\n## PowerShell Modules Required\n\nTo execute generated scripts, ensure these modules are installed:\n- `Microsoft.Graph` (recommended, modern Graph API)\n- `ExchangeOnlineManagement` (Exchange Online management)\n- `MicrosoftTeams` (Teams administration)\n- `SharePointPnPPowerShellOnline` (SharePoint management)\n- `AzureAD` or `AzureADPreview` (Azure AD management - being deprecated)\n- `MSOnline` (Legacy, being deprecated - avoid when possible)\n\n## Updates & Maintenance\n\n- Microsoft 365 features and APIs evolve rapidly\n- Review Microsoft 365 Roadmap regularly for upcoming changes\n- Test scripts in non-production tenant before production deployment\n- Subscribe to Microsoft 365 Admin Center message center for updates\n- Keep PowerShell modules updated to latest versions\n- Regular security baseline reviews (quarterly recommended)\n\n## Helpful Resources\n\n- **Microsoft 365 Admin Center**: https://admin.microsoft.com\n- **Microsoft Graph Explorer**: https://developer.microsoft.com/graph/graph-explorer\n- **PowerShell Gallery**: https://www.powershellgallery.com\n- **Microsoft Secure Score**: Security posture assessment in Admin Center\n- **Microsoft 365 Compliance Center**: https://compliance.microsoft.com\n- **Azure AD Conditional Access**: Identity and access management policies\n",
      "frontmatter": {
        "name": "ms365-tenant-manager",
        "description": "Comprehensive Microsoft 365 tenant administration skill for setup, configuration, user management, security policies, and organizational structure optimization for Global Administrators"
      },
      "content": "\n# Microsoft 365 Tenant Manager\n\nThis skill provides expert guidance and automation for Microsoft 365 Global Administrators managing tenant setup, configuration, user lifecycle, security policies, and organizational optimization.\n\n## Capabilities\n\n- **Tenant Setup & Configuration**: Initial tenant setup, domain configuration, DNS records, service provisioning\n- **User & Group Management**: User lifecycle (create, modify, disable, delete), group creation, license assignment\n- **Security & Compliance**: Conditional Access policies, MFA setup, DLP policies, retention policies, security baselines\n- **SharePoint & OneDrive**: Site provisioning, permissions management, storage quotas, sharing policies\n- **Teams Administration**: Team creation, policy management, guest access, compliance settings\n- **Exchange Online**: Mailbox management, distribution groups, mail flow rules, anti-spam/malware policies\n- **License Management**: License allocation, optimization, cost analysis, usage reporting\n- **Reporting & Auditing**: Activity reports, audit logs, compliance reporting, usage analytics\n- **Automation Scripts**: PowerShell script generation for bulk operations and recurring tasks\n- **Best Practices**: Microsoft recommended configurations, security hardening, governance frameworks\n\n## Input Requirements\n\nTenant management tasks require:\n- **Action type**: setup, configure, create, modify, delete, report, audit\n- **Resource details**: User info, group names, policy settings, service configurations\n- **Organizational context**: Company size, industry, compliance requirements (GDPR, HIPAA, etc.)\n- **Current state**: Existing configurations, licenses, user count\n- **Desired outcome**: Specific goals, requirements, or changes needed\n\nFormats accepted:\n- Text descriptions of administrative tasks\n- JSON with structured configuration data\n- CSV for bulk user/group operations\n- Existing PowerShell scripts to review or modify\n\n## Output Formats\n\nResults include:\n- **Step-by-step instructions**: Detailed guidance for manual configuration via Admin Center\n- **PowerShell scripts**: Ready-to-use scripts for automation (with safety checks)\n- **Configuration recommendations**: Security and governance best practices\n- **Validation checklists**: Pre/post-implementation verification steps\n- **Documentation**: Markdown documentation of changes and configurations\n- **Rollback procedures**: Instructions to undo changes if needed\n- **Compliance reports**: Security posture and compliance status\n\n## How to Use\n\n\"Set up a new Microsoft 365 tenant for a 50-person company with security best practices\"\n\"Create a PowerShell script to provision 100 users from a CSV file with appropriate licenses\"\n\"Configure Conditional Access policy requiring MFA for all admin accounts\"\n\"Generate a report of all inactive users in the past 90 days\"\n\"Set up Teams policies for external collaboration with security controls\"\n\n## Scripts\n\n- `tenant_setup.py`: Initial tenant configuration and service provisioning automation\n- `user_management.py`: User lifecycle operations and bulk provisioning\n- `security_policies.py`: Security policy configuration and compliance checks\n- `reporting.py`: Analytics, audit logs, and compliance reporting\n- `powershell_generator.py`: Generates PowerShell scripts for Microsoft Graph API and admin modules\n\n## Best Practices\n\n### Tenant Setup\n1. **Enable MFA first** - Before adding users, enforce multi-factor authentication\n2. **Configure named locations** - Define trusted IP ranges for Conditional Access\n3. **Set up privileged access** - Use separate admin accounts, enable PIM (Privileged Identity Management)\n4. **Domain verification** - Add and verify custom domains before bulk user creation\n5. **Baseline security** - Apply Microsoft Secure Score recommendations immediately\n\n### User Management\n1. **License assignment** - Use group-based licensing for scalability\n2. **Naming conventions** - Establish consistent user principal names (UPNs) and display names\n3. **Lifecycle management** - Implement automated onboarding/offboarding workflows\n4. **Guest access** - Enable only when necessary, set expiration policies\n5. **Shared mailboxes** - Use for department emails instead of assigning licenses\n\n### Security & Compliance\n1. **Zero Trust approach** - Verify explicitly, use least privilege access, assume breach\n2. **Conditional Access** - Start with report-only mode, then enforce gradually\n3. **Data Loss Prevention** - Define sensitive information types, test policies before enforcement\n4. **Retention policies** - Balance compliance requirements with storage costs\n5. **Regular audits** - Review permissions, licenses, and security settings quarterly\n\n### SharePoint & Teams\n1. **Site provisioning** - Use templates and governance policies\n2. **External sharing** - Restrict to specific domains, require authentication\n3. **Storage management** - Set quotas, enable auto-cleanup of old content\n4. **Teams templates** - Create standardized team structures for consistency\n5. **Guest lifecycle** - Set expiration and regular recertification\n\n### PowerShell Automation\n1. **Use Microsoft Graph** - Prefer Graph API over legacy MSOnline modules\n2. **Error handling** - Include try/catch blocks and validation checks\n3. **Dry-run mode** - Test scripts with -WhatIf before executing\n4. **Logging** - Capture all operations for audit trails\n5. **Credential management** - Use Azure Key Vault or managed identities, never hardcode\n\n## Common Tasks\n\n### Initial Tenant Setup\n- Configure company branding\n- Add and verify custom domains\n- Set up DNS records (MX, SPF, DKIM, DMARC)\n- Enable required services (Teams, SharePoint, Exchange)\n- Create organizational structure (departments, locations)\n- Set default user settings and policies\n\n### User Onboarding\n- Create user accounts (single or bulk)\n- Assign appropriate licenses\n- Add to security and distribution groups\n- Configure mailbox and OneDrive\n- Set up multi-factor authentication\n- Provision Teams access\n\n### Security Hardening\n- Enable Security Defaults or Conditional Access\n- Configure MFA enforcement\n- Set up admin role assignments\n- Enable audit logging\n- Configure anti-phishing policies\n- Set up DLP and retention policies\n\n### Reporting & Monitoring\n- Active users and license utilization\n- Security incidents and alerts\n- Mailbox usage and storage\n- SharePoint site activity\n- Teams usage and adoption\n- Compliance and audit logs\n\n## Limitations\n\n- **Permissions required**: Global Administrator or specific role-based permissions\n- **API rate limits**: Microsoft Graph API has throttling limits for bulk operations\n- **License dependencies**: Some features require specific license tiers (E3, E5)\n- **Delegation constraints**: Some tasks cannot be delegated to service principals\n- **Regional variations**: Compliance features may vary by geographic region\n- **Hybrid scenarios**: On-premises Active Directory integration requires additional configuration\n- **Third-party integrations**: External apps may require separate authentication and permissions\n- **PowerShell prerequisites**: Requires appropriate modules installed (Microsoft.Graph, ExchangeOnlineManagement, etc.)\n\n## Security Considerations\n\n### Authentication\n- Never store credentials in scripts or configuration files\n- Use Azure Key Vault for credential management\n- Implement certificate-based authentication for automation\n- Enable Conditional Access for admin accounts\n- Use Privileged Identity Management (PIM) for JIT access\n\n### Authorization\n- Follow principle of least privilege\n- Use custom admin roles instead of Global Admin when possible\n- Regularly review and audit admin role assignments\n- Enable PIM for temporary elevated access\n- Separate user accounts from admin accounts\n\n### Compliance\n- Enable audit logging for all activities\n- Retain logs according to compliance requirements\n- Configure data residency for regulated industries\n- Implement information barriers where needed\n- Regular compliance assessments and reporting\n\n## PowerShell Modules Required\n\nTo execute generated scripts, ensure these modules are installed:\n- `Microsoft.Graph` (recommended, modern Graph API)\n- `ExchangeOnlineManagement` (Exchange Online management)\n- `MicrosoftTeams` (Teams administration)\n- `SharePointPnPPowerShellOnline` (SharePoint management)\n- `AzureAD` or `AzureADPreview` (Azure AD management - being deprecated)\n- `MSOnline` (Legacy, being deprecated - avoid when possible)\n\n## Updates & Maintenance\n\n- Microsoft 365 features and APIs evolve rapidly\n- Review Microsoft 365 Roadmap regularly for upcoming changes\n- Test scripts in non-production tenant before production deployment\n- Subscribe to Microsoft 365 Admin Center message center for updates\n- Keep PowerShell modules updated to latest versions\n- Regular security baseline reviews (quarterly recommended)\n\n## Helpful Resources\n\n- **Microsoft 365 Admin Center**: https://admin.microsoft.com\n- **Microsoft Graph Explorer**: https://developer.microsoft.com/graph/graph-explorer\n- **PowerShell Gallery**: https://www.powershellgallery.com\n- **Microsoft Secure Score**: Security posture assessment in Admin Center\n- **Microsoft 365 Compliance Center**: https://compliance.microsoft.com\n- **Azure AD Conditional Access**: Identity and access management policies\n"
    }
  },
  "alirezarezvani-claude-skills-tdd-guide": {
    "id": "alirezarezvani-claude-skills-tdd-guide",
    "name": "tdd-guide",
    "description": "Comprehensive Test Driven Development guide for engineering subagents with multi-framework support, coverage analysis, and intelligent test generation",
    "repo": {
      "owner": "alirezarezvani",
      "name": "claude-skills",
      "fullName": "alirezarezvani/claude-skills",
      "url": "https://github.com/alirezarezvani/claude-skills/tree/main/engineering-team/tdd-guide",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 521,
      "forks": 103,
      "language": "Python",
      "topics": [
        "agentic-ai",
        "agentic-coding",
        "anthropic-claude",
        "claude-ai",
        "claude-ai-skills",
        "claude-code",
        "claude-code-skills",
        "claude-skills",
        "claudecode-subagents"
      ],
      "updatedAt": "2026-01-08T12:36:14Z",
      "pushedAt": "2026-01-08T08:39:21Z",
      "createdAt": "2025-10-19T04:04:05Z",
      "license": "MIT License"
    },
    "category": "development",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: tdd-guide\ndescription: Comprehensive Test Driven Development guide for engineering subagents with multi-framework support, coverage analysis, and intelligent test generation\n---\n\n# TDD Guide - Test Driven Development for Engineering Teams\n\nA comprehensive Test Driven Development skill that provides intelligent test generation, coverage analysis, framework integration, and TDD workflow guidance across multiple languages and testing frameworks.\n\n## Capabilities\n\n### Test Generation\n- **Generate Test Cases from Requirements**: Convert user stories, API specs, and business requirements into executable test cases\n- **Create Test Stubs**: Generate test function scaffolding with proper naming, imports, and setup/teardown\n- **Generate Test Fixtures**: Create realistic test data, mocks, and fixtures for various scenarios\n\n### TDD Workflow Support\n- **Guide Red-Green-Refactor**: Step-by-step guidance through TDD cycles with validation\n- **Suggest Missing Scenarios**: Identify untested edge cases, error conditions, and boundary scenarios\n- **Review Test Quality**: Analyze test isolation, assertions quality, naming conventions, and maintainability\n\n### Coverage & Metrics Analysis\n- **Calculate Coverage**: Parse LCOV, JSON, and XML coverage reports for line/branch/function coverage\n- **Identify Untested Paths**: Find code paths, branches, and error handlers without test coverage\n- **Recommend Improvements**: Prioritized recommendations (P0/P1/P2) for coverage gaps and test quality\n\n### Framework Integration\n- **Multi-Framework Support**: Jest, Pytest, JUnit, Vitest, Mocha, RSpec adapters\n- **Generate Boilerplate**: Create test files with proper imports, describe blocks, and best practices\n- **Configure Test Runners**: Set up test configuration, coverage tools, and CI integration\n\n### Comprehensive Metrics\n- **Test Coverage**: Line, branch, function coverage with gap analysis\n- **Code Complexity**: Cyclomatic complexity, cognitive complexity, testability scoring\n- **Test Quality**: Assertions per test, isolation score, naming quality, test smell detection\n- **Test Data**: Boundary value analysis, edge case identification, mock data generation\n- **Test Execution**: Timing analysis, slow test detection, flakiness detection\n- **Missing Tests**: Uncovered edge cases, error handling gaps, missing integration scenarios\n\n## Input Requirements\n\nThe skill supports **automatic format detection** for flexible input:\n\n### Source Code\n- **Languages**: TypeScript, JavaScript, Python, Java\n- **Format**: Direct file paths or copy-pasted code blocks\n- **Detection**: Automatic language/framework detection from syntax and imports\n\n### Test Artifacts\n- **Coverage Reports**: LCOV (.lcov), JSON (coverage-final.json), XML (cobertura.xml)\n- **Test Results**: JUnit XML, Jest JSON, Pytest JSON, TAP format\n- **Format**: File paths or raw coverage data\n\n### Requirements (Optional)\n- **User Stories**: Text descriptions of functionality\n- **API Specifications**: OpenAPI/Swagger, REST endpoints, GraphQL schemas\n- **Business Requirements**: Acceptance criteria, business rules\n\n### Input Methods\n- **Option A**: Provide file paths (skill will read files)\n- **Option B**: Copy-paste code/data directly\n- **Option C**: Mix of both (automatically detected)\n\n## Output Formats\n\nThe skill provides **context-aware output** optimized for your environment:\n\n### Code Files\n- **Test Files**: Generated tests (Jest/Pytest/JUnit/Vitest) with proper structure\n- **Fixtures**: Test data files, mock objects, factory functions\n- **Mocks**: Mock implementations, stub functions, test doubles\n\n### Reports\n- **Markdown**: Rich coverage reports, recommendations, quality analysis (Claude Desktop)\n- **JSON**: Machine-readable metrics, structured data for CI/CD integration\n- **Terminal-Friendly**: Simplified output for Claude Code CLI\n\n### Smart Defaults\n- **Desktop/Apps**: Rich markdown with tables, code blocks, visual hierarchy\n- **CLI**: Concise, terminal-friendly format with clear sections\n- **CI/CD**: JSON output for automated processing\n\n### Progressive Disclosure\n- **Summary First**: High-level overview (<200 tokens)\n- **Details on Demand**: Full analysis available (500-1000 tokens)\n- **Prioritized**: P0 (critical) â†’ P1 (important) â†’ P2 (nice-to-have)\n\n## How to Use\n\n### Basic Usage\n```\n@tdd-guide\n\nI need tests for my authentication module. Here's the code:\n[paste code or provide file path]\n\nGenerate comprehensive test cases covering happy path, error cases, and edge cases.\n```\n\n### Coverage Analysis\n```\n@tdd-guide\n\nAnalyze test coverage for my TypeScript project. Coverage report: coverage/lcov.info\n\nIdentify gaps and provide prioritized recommendations.\n```\n\n### TDD Workflow\n```\n@tdd-guide\n\nGuide me through TDD for implementing a password validation function.\n\nRequirements:\n- Min 8 characters\n- At least 1 uppercase, 1 lowercase, 1 number, 1 special char\n- No common passwords\n```\n\n### Multi-Framework Support\n```\n@tdd-guide\n\nConvert these Jest tests to Pytest format:\n[paste Jest tests]\n```\n\n## Scripts\n\n### Core Modules\n\n- **test_generator.py**: Intelligent test case generation from requirements and code\n- **coverage_analyzer.py**: Parse and analyze coverage reports (LCOV, JSON, XML)\n- **metrics_calculator.py**: Calculate comprehensive test and code quality metrics\n- **framework_adapter.py**: Multi-framework adapter (Jest, Pytest, JUnit, Vitest)\n- **tdd_workflow.py**: Red-green-refactor workflow guidance and validation\n- **fixture_generator.py**: Generate realistic test data and fixtures\n- **format_detector.py**: Automatic language and framework detection\n\n### Utilities\n\n- **complexity_analyzer.py**: Cyclomatic and cognitive complexity analysis\n- **test_quality_scorer.py**: Test quality scoring (isolation, assertions, naming)\n- **missing_test_detector.py**: Identify untested paths and missing scenarios\n- **output_formatter.py**: Context-aware output formatting (Desktop vs CLI)\n\n## Best Practices\n\n### Test Generation\n1. **Start with Requirements**: Write tests from user stories before seeing implementation\n2. **Test Behavior, Not Implementation**: Focus on what code does, not how it does it\n3. **One Assertion Focus**: Each test should verify one specific behavior\n4. **Descriptive Names**: Test names should read like specifications\n\n### TDD Workflow\n1. **Red**: Write failing test first\n2. **Green**: Write minimal code to make it pass\n3. **Refactor**: Improve code while keeping tests green\n4. **Repeat**: Small iterations, frequent commits\n\n### Coverage Goals\n1. **Aim for 80%+**: Line coverage baseline for most projects\n2. **100% Critical Paths**: Authentication, payments, data validation must be fully covered\n3. **Branch Coverage Matters**: Line coverage alone is insufficient\n4. **Don't Game Metrics**: Focus on meaningful tests, not coverage numbers\n\n### Test Quality\n1. **Independent Tests**: Each test should run in isolation\n2. **Fast Execution**: Keep unit tests under 100ms each\n3. **Deterministic**: Tests should always produce same results\n4. **Clear Failures**: Assertion messages should explain what went wrong\n\n### Framework Selection\n1. **Jest**: JavaScript/TypeScript projects (React, Node.js)\n2. **Pytest**: Python projects (Django, Flask, FastAPI)\n3. **JUnit**: Java projects (Spring, Android)\n4. **Vitest**: Modern Vite-based projects\n\n## Multi-Language Support\n\n### TypeScript/JavaScript\n- Frameworks: Jest, Vitest, Mocha, Jasmine\n- Runners: Node.js, Karma, Playwright\n- Coverage: Istanbul/nyc, c8\n\n### Python\n- Frameworks: Pytest, unittest, nose2\n- Runners: pytest, tox, nox\n- Coverage: coverage.py, pytest-cov\n\n### Java\n- Frameworks: JUnit 5, TestNG, Mockito\n- Runners: Maven Surefire, Gradle Test\n- Coverage: JaCoCo, Cobertura\n\n## Limitations\n\n### Scope\n- **Unit Tests Focus**: Primarily optimized for unit tests (integration tests require different patterns)\n- **Static Analysis Only**: Cannot execute tests or measure actual code behavior\n- **Language Support**: Best support for TypeScript, JavaScript, Python, Java (other languages limited)\n\n### Coverage Analysis\n- **Report Dependency**: Requires existing coverage reports (cannot generate coverage from scratch)\n- **Format Support**: LCOV, JSON, XML only (other formats need conversion)\n- **Interpretation Context**: Coverage numbers need human judgment for meaningfulness\n\n### Test Generation\n- **Baseline Quality**: Generated tests provide scaffolding, require human review and refinement\n- **Complex Logic**: Advanced business logic and integration scenarios need manual test design\n- **Mocking Strategy**: Mock/stub strategies should align with project patterns\n\n### Framework Integration\n- **Configuration Required**: Test runners need proper setup (this skill doesn't modify package.json or pom.xml)\n- **Version Compatibility**: Generated code targets recent stable versions (Jest 29+, Pytest 7+, JUnit 5+)\n\n### When NOT to Use This Skill\n- **E2E Testing**: Use dedicated E2E tools (Playwright, Cypress, Selenium)\n- **Performance Testing**: Use JMeter, k6, or Locust\n- **Security Testing**: Use OWASP ZAP, Burp Suite, or security-focused tools\n- **Manual Testing**: Some scenarios require human exploratory testing\n\n## Example Workflows\n\n### Workflow 1: Generate Tests from Requirements\n```\nInput: User story + API specification\nProcess: Parse requirements â†’ Generate test cases â†’ Create test stubs\nOutput: Complete test files ready for implementation\n```\n\n### Workflow 2: Improve Coverage\n```\nInput: Coverage report + source code\nProcess: Identify gaps â†’ Suggest tests â†’ Generate test code\nOutput: Prioritized test cases for uncovered code\n```\n\n### Workflow 3: TDD New Feature\n```\nInput: Feature requirements\nProcess: Guide red-green-refactor â†’ Validate each step â†’ Suggest refactorings\nOutput: Well-tested feature with clean code\n```\n\n### Workflow 4: Framework Migration\n```\nInput: Tests in Framework A\nProcess: Parse tests â†’ Translate patterns â†’ Generate equivalent tests\nOutput: Tests in Framework B with same coverage\n```\n\n## Integration Points\n\n### CI/CD Integration\n- Parse coverage reports from CI artifacts\n- Generate coverage badges and reports\n- Fail builds on coverage thresholds\n- Track coverage trends over time\n\n### IDE Integration\n- Generate tests for selected code\n- Run coverage analysis on save\n- Highlight untested code paths\n- Quick-fix suggestions for test gaps\n\n### Code Review\n- Validate test coverage in PRs\n- Check test quality standards\n- Identify missing test scenarios\n- Suggest improvements before merge\n\n## Version Support\n\n- **Node.js**: 16+ (Jest 29+, Vitest 0.34+)\n- **Python**: 3.8+ (Pytest 7+)\n- **Java**: 11+ (JUnit 5.9+)\n- **TypeScript**: 4.5+\n\n## Related Skills\n\nThis skill works well with:\n- **code-review**: Validate test quality during reviews\n- **refactoring-assistant**: Maintain tests during refactoring\n- **ci-cd-helper**: Integrate coverage in pipelines\n- **documentation-generator**: Generate test documentation\n",
      "frontmatter": {
        "name": "tdd-guide",
        "description": "Comprehensive Test Driven Development guide for engineering subagents with multi-framework support, coverage analysis, and intelligent test generation"
      },
      "content": "\n# TDD Guide - Test Driven Development for Engineering Teams\n\nA comprehensive Test Driven Development skill that provides intelligent test generation, coverage analysis, framework integration, and TDD workflow guidance across multiple languages and testing frameworks.\n\n## Capabilities\n\n### Test Generation\n- **Generate Test Cases from Requirements**: Convert user stories, API specs, and business requirements into executable test cases\n- **Create Test Stubs**: Generate test function scaffolding with proper naming, imports, and setup/teardown\n- **Generate Test Fixtures**: Create realistic test data, mocks, and fixtures for various scenarios\n\n### TDD Workflow Support\n- **Guide Red-Green-Refactor**: Step-by-step guidance through TDD cycles with validation\n- **Suggest Missing Scenarios**: Identify untested edge cases, error conditions, and boundary scenarios\n- **Review Test Quality**: Analyze test isolation, assertions quality, naming conventions, and maintainability\n\n### Coverage & Metrics Analysis\n- **Calculate Coverage**: Parse LCOV, JSON, and XML coverage reports for line/branch/function coverage\n- **Identify Untested Paths**: Find code paths, branches, and error handlers without test coverage\n- **Recommend Improvements**: Prioritized recommendations (P0/P1/P2) for coverage gaps and test quality\n\n### Framework Integration\n- **Multi-Framework Support**: Jest, Pytest, JUnit, Vitest, Mocha, RSpec adapters\n- **Generate Boilerplate**: Create test files with proper imports, describe blocks, and best practices\n- **Configure Test Runners**: Set up test configuration, coverage tools, and CI integration\n\n### Comprehensive Metrics\n- **Test Coverage**: Line, branch, function coverage with gap analysis\n- **Code Complexity**: Cyclomatic complexity, cognitive complexity, testability scoring\n- **Test Quality**: Assertions per test, isolation score, naming quality, test smell detection\n- **Test Data**: Boundary value analysis, edge case identification, mock data generation\n- **Test Execution**: Timing analysis, slow test detection, flakiness detection\n- **Missing Tests**: Uncovered edge cases, error handling gaps, missing integration scenarios\n\n## Input Requirements\n\nThe skill supports **automatic format detection** for flexible input:\n\n### Source Code\n- **Languages**: TypeScript, JavaScript, Python, Java\n- **Format**: Direct file paths or copy-pasted code blocks\n- **Detection**: Automatic language/framework detection from syntax and imports\n\n### Test Artifacts\n- **Coverage Reports**: LCOV (.lcov), JSON (coverage-final.json), XML (cobertura.xml)\n- **Test Results**: JUnit XML, Jest JSON, Pytest JSON, TAP format\n- **Format**: File paths or raw coverage data\n\n### Requirements (Optional)\n- **User Stories**: Text descriptions of functionality\n- **API Specifications**: OpenAPI/Swagger, REST endpoints, GraphQL schemas\n- **Business Requirements**: Acceptance criteria, business rules\n\n### Input Methods\n- **Option A**: Provide file paths (skill will read files)\n- **Option B**: Copy-paste code/data directly\n- **Option C**: Mix of both (automatically detected)\n\n## Output Formats\n\nThe skill provides **context-aware output** optimized for your environment:\n\n### Code Files\n- **Test Files**: Generated tests (Jest/Pytest/JUnit/Vitest) with proper structure\n- **Fixtures**: Test data files, mock objects, factory functions\n- **Mocks**: Mock implementations, stub functions, test doubles\n\n### Reports\n- **Markdown**: Rich coverage reports, recommendations, quality analysis (Claude Desktop)\n- **JSON**: Machine-readable metrics, structured data for CI/CD integration\n- **Terminal-Friendly**: Simplified output for Claude Code CLI\n\n### Smart Defaults\n- **Desktop/Apps**: Rich markdown with tables, code blocks, visual hierarchy\n- **CLI**: Concise, terminal-friendly format with clear sections\n- **CI/CD**: JSON output for automated processing\n\n### Progressive Disclosure\n- **Summary First**: High-level overview (<200 tokens)\n- **Details on Demand**: Full analysis available (500-1000 tokens)\n- **Prioritized**: P0 (critical) â†’ P1 (important) â†’ P2 (nice-to-have)\n\n## How to Use\n\n### Basic Usage\n```\n@tdd-guide\n\nI need tests for my authentication module. Here's the code:\n[paste code or provide file path]\n\nGenerate comprehensive test cases covering happy path, error cases, and edge cases.\n```\n\n### Coverage Analysis\n```\n@tdd-guide\n\nAnalyze test coverage for my TypeScript project. Coverage report: coverage/lcov.info\n\nIdentify gaps and provide prioritized recommendations.\n```\n\n### TDD Workflow\n```\n@tdd-guide\n\nGuide me through TDD for implementing a password validation function.\n\nRequirements:\n- Min 8 characters\n- At least 1 uppercase, 1 lowercase, 1 number, 1 special char\n- No common passwords\n```\n\n### Multi-Framework Support\n```\n@tdd-guide\n\nConvert these Jest tests to Pytest format:\n[paste Jest tests]\n```\n\n## Scripts\n\n### Core Modules\n\n- **test_generator.py**: Intelligent test case generation from requirements and code\n- **coverage_analyzer.py**: Parse and analyze coverage reports (LCOV, JSON, XML)\n- **metrics_calculator.py**: Calculate comprehensive test and code quality metrics\n- **framework_adapter.py**: Multi-framework adapter (Jest, Pytest, JUnit, Vitest)\n- **tdd_workflow.py**: Red-green-refactor workflow guidance and validation\n- **fixture_generator.py**: Generate realistic test data and fixtures\n- **format_detector.py**: Automatic language and framework detection\n\n### Utilities\n\n- **complexity_analyzer.py**: Cyclomatic and cognitive complexity analysis\n- **test_quality_scorer.py**: Test quality scoring (isolation, assertions, naming)\n- **missing_test_detector.py**: Identify untested paths and missing scenarios\n- **output_formatter.py**: Context-aware output formatting (Desktop vs CLI)\n\n## Best Practices\n\n### Test Generation\n1. **Start with Requirements**: Write tests from user stories before seeing implementation\n2. **Test Behavior, Not Implementation**: Focus on what code does, not how it does it\n3. **One Assertion Focus**: Each test should verify one specific behavior\n4. **Descriptive Names**: Test names should read like specifications\n\n### TDD Workflow\n1. **Red**: Write failing test first\n2. **Green**: Write minimal code to make it pass\n3. **Refactor**: Improve code while keeping tests green\n4. **Repeat**: Small iterations, frequent commits\n\n### Coverage Goals\n1. **Aim for 80%+**: Line coverage baseline for most projects\n2. **100% Critical Paths**: Authentication, payments, data validation must be fully covered\n3. **Branch Coverage Matters**: Line coverage alone is insufficient\n4. **Don't Game Metrics**: Focus on meaningful tests, not coverage numbers\n\n### Test Quality\n1. **Independent Tests**: Each test should run in isolation\n2. **Fast Execution**: Keep unit tests under 100ms each\n3. **Deterministic**: Tests should always produce same results\n4. **Clear Failures**: Assertion messages should explain what went wrong\n\n### Framework Selection\n1. **Jest**: JavaScript/TypeScript projects (React, Node.js)\n2. **Pytest**: Python projects (Django, Flask, FastAPI)\n3. **JUnit**: Java projects (Spring, Android)\n4. **Vitest**: Modern Vite-based projects\n\n## Multi-Language Support\n\n### TypeScript/JavaScript\n- Frameworks: Jest, Vitest, Mocha, Jasmine\n- Runners: Node.js, Karma, Playwright\n- Coverage: Istanbul/nyc, c8\n\n### Python\n- Frameworks: Pytest, unittest, nose2\n- Runners: pytest, tox, nox\n- Coverage: coverage.py, pytest-cov\n\n### Java\n- Frameworks: JUnit 5, TestNG, Mockito\n- Runners: Maven Surefire, Gradle Test\n- Coverage: JaCoCo, Cobertura\n\n## Limitations\n\n### Scope\n- **Unit Tests Focus**: Primarily optimized for unit tests (integration tests require different patterns)\n- **Static Analysis Only**: Cannot execute tests or measure actual code behavior\n- **Language Support**: Best support for TypeScript, JavaScript, Python, Java (other languages limited)\n\n### Coverage Analysis\n- **Report Dependency**: Requires existing coverage reports (cannot generate coverage from scratch)\n- **Format Support**: LCOV, JSON, XML only (other formats need conversion)\n- **Interpretation Context**: Coverage numbers need human judgment for meaningfulness\n\n### Test Generation\n- **Baseline Quality**: Generated tests provide scaffolding, require human review and refinement\n- **Complex Logic**: Advanced business logic and integration scenarios need manual test design\n- **Mocking Strategy**: Mock/stub strategies should align with project patterns\n\n### Framework Integration\n- **Configuration Required**: Test runners need proper setup (this skill doesn't modify package.json or pom.xml)\n- **Version Compatibility**: Generated code targets recent stable versions (Jest 29+, Pytest 7+, JUnit 5+)\n\n### When NOT to Use This Skill\n- **E2E Testing**: Use dedicated E2E tools (Playwright, Cypress, Selenium)\n- **Performance Testing**: Use JMeter, k6, or Locust\n- **Security Testing**: Use OWASP ZAP, Burp Suite, or security-focused tools\n- **Manual Testing**: Some scenarios require human exploratory testing\n\n## Example Workflows\n\n### Workflow 1: Generate Tests from Requirements\n```\nInput: User story + API specification\nProcess: Parse requirements â†’ Generate test cases â†’ Create test stubs\nOutput: Complete test files ready for implementation\n```\n\n### Workflow 2: Improve Coverage\n```\nInput: Coverage report + source code\nProcess: Identify gaps â†’ Suggest tests â†’ Generate test code\nOutput: Prioritized test cases for uncovered code\n```\n\n### Workflow 3: TDD New Feature\n```\nInput: Feature requirements\nProcess: Guide red-green-refactor â†’ Validate each step â†’ Suggest refactorings\nOutput: Well-tested feature with clean code\n```\n\n### Workflow 4: Framework Migration\n```\nInput: Tests in Framework A\nProcess: Parse tests â†’ Translate patterns â†’ Generate equivalent tests\nOutput: Tests in Framework B with same coverage\n```\n\n## Integration Points\n\n### CI/CD Integration\n- Parse coverage reports from CI artifacts\n- Generate coverage badges and reports\n- Fail builds on coverage thresholds\n- Track coverage trends over time\n\n### IDE Integration\n- Generate tests for selected code\n- Run coverage analysis on save\n- Highlight untested code paths\n- Quick-fix suggestions for test gaps\n\n### Code Review\n- Validate test coverage in PRs\n- Check test quality standards\n- Identify missing test scenarios\n- Suggest improvements before merge\n\n## Version Support\n\n- **Node.js**: 16+ (Jest 29+, Vitest 0.34+)\n- **Python**: 3.8+ (Pytest 7+)\n- **Java**: 11+ (JUnit 5.9+)\n- **TypeScript**: 4.5+\n\n## Related Skills\n\nThis skill works well with:\n- **code-review**: Validate test quality during reviews\n- **refactoring-assistant**: Maintain tests during refactoring\n- **ci-cd-helper**: Integrate coverage in pipelines\n- **documentation-generator**: Generate test documentation\n"
    }
  },
  "alirezarezvani-claude-skills-tech-stack-evaluator": {
    "id": "alirezarezvani-claude-skills-tech-stack-evaluator",
    "name": "tech-stack-evaluator",
    "description": "Comprehensive technology stack evaluation and comparison tool with TCO analysis, security assessment, and intelligent recommendations for engineering teams",
    "repo": {
      "owner": "alirezarezvani",
      "name": "claude-skills",
      "fullName": "alirezarezvani/claude-skills",
      "url": "https://github.com/alirezarezvani/claude-skills/tree/main/engineering-team/tech-stack-evaluator",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 521,
      "forks": 103,
      "language": "Python",
      "topics": [
        "agentic-ai",
        "agentic-coding",
        "anthropic-claude",
        "claude-ai",
        "claude-ai-skills",
        "claude-code",
        "claude-code-skills",
        "claude-skills",
        "claudecode-subagents"
      ],
      "updatedAt": "2026-01-08T12:36:14Z",
      "pushedAt": "2026-01-08T08:39:21Z",
      "createdAt": "2025-10-19T04:04:05Z",
      "license": "MIT License"
    },
    "category": "development",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: tech-stack-evaluator\ndescription: Comprehensive technology stack evaluation and comparison tool with TCO analysis, security assessment, and intelligent recommendations for engineering teams\n---\n\n# Technology Stack Evaluator\n\nA comprehensive evaluation framework for comparing technologies, frameworks, cloud providers, and complete technology stacks. Provides data-driven recommendations with TCO analysis, security assessment, ecosystem health scoring, and migration path analysis.\n\n## Capabilities\n\nThis skill provides eight comprehensive evaluation capabilities:\n\n- **Technology Comparison**: Head-to-head comparisons of frameworks, languages, and tools (React vs Vue, PostgreSQL vs MongoDB, Node.js vs Python)\n- **Stack Evaluation**: Assess complete technology stacks for specific use cases (real-time collaboration, API-heavy SaaS, data-intensive platforms)\n- **Maturity & Ecosystem Analysis**: Evaluate community health, maintenance status, long-term viability, and ecosystem strength\n- **Total Cost of Ownership (TCO)**: Calculate comprehensive costs including licensing, hosting, developer productivity, and scaling\n- **Security & Compliance**: Analyze vulnerabilities, compliance readiness (GDPR, SOC2, HIPAA), and security posture\n- **Migration Path Analysis**: Assess migration complexity, risks, timelines, and strategies from legacy to modern stacks\n- **Cloud Provider Comparison**: Compare AWS vs Azure vs GCP for specific workloads with cost and feature analysis\n- **Decision Reports**: Generate comprehensive decision matrices with pros/cons, confidence scores, and actionable recommendations\n\n## Input Requirements\n\n### Flexible Input Formats (Automatic Detection)\n\nThe skill automatically detects and processes multiple input formats:\n\n**Text/Conversational**:\n```\n\"Compare React vs Vue for building a SaaS dashboard\"\n\"Evaluate technology stack for real-time collaboration platform\"\n\"Should we migrate from MongoDB to PostgreSQL?\"\n```\n\n**Structured (YAML)**:\n```yaml\ncomparison:\n  technologies:\n    - name: \"React\"\n    - name: \"Vue\"\n  use_case: \"SaaS dashboard\"\n  priorities:\n    - \"Developer productivity\"\n    - \"Ecosystem maturity\"\n    - \"Performance\"\n```\n\n**Structured (JSON)**:\n```json\n{\n  \"comparison\": {\n    \"technologies\": [\"React\", \"Vue\"],\n    \"use_case\": \"SaaS dashboard\",\n    \"priorities\": [\"Developer productivity\", \"Ecosystem maturity\"]\n  }\n}\n```\n\n**URLs for Ecosystem Analysis**:\n- GitHub repository URLs (for health scoring)\n- npm package URLs (for download statistics)\n- Technology documentation URLs (for feature extraction)\n\n### Analysis Scope Selection\n\nUsers can select which analyses to run:\n- **Quick Comparison**: Basic scoring and comparison (200-300 tokens)\n- **Standard Analysis**: Scoring + TCO + Security (500-800 tokens)\n- **Comprehensive Report**: All analyses including migration paths (1200-1500 tokens)\n- **Custom**: User selects specific sections (modular)\n\n## Output Formats\n\n### Context-Aware Output\n\nThe skill automatically adapts output based on environment:\n\n**Claude Desktop (Rich Markdown)**:\n- Formatted tables with color indicators\n- Expandable sections for detailed analysis\n- Visual decision matrices\n- Charts and graphs (when appropriate)\n\n**CLI/Terminal (Terminal-Friendly)**:\n- Plain text tables with ASCII borders\n- Compact formatting\n- Clear section headers\n- Copy-paste friendly code blocks\n\n### Progressive Disclosure Structure\n\n**Executive Summary (200-300 tokens)**:\n- Recommendation summary\n- Top 3 pros and cons\n- Confidence level (High/Medium/Low)\n- Key decision factors\n\n**Detailed Breakdown (on-demand)**:\n- Complete scoring matrices\n- Detailed TCO calculations\n- Full security analysis\n- Migration complexity assessment\n- All supporting data and calculations\n\n### Report Sections (User-Selectable)\n\nUsers choose which sections to include:\n\n1. **Scoring & Comparison Matrix**\n   - Weighted decision scores\n   - Head-to-head comparison tables\n   - Strengths and weaknesses\n\n2. **Financial Analysis**\n   - TCO breakdown (5-year projection)\n   - ROI analysis\n   - Cost per user/request metrics\n   - Hidden cost identification\n\n3. **Ecosystem Health**\n   - Community size and activity\n   - GitHub stars, npm downloads\n   - Release frequency and maintenance\n   - Issue response times\n   - Viability assessment\n\n4. **Security & Compliance**\n   - Vulnerability count (CVE database)\n   - Security patch frequency\n   - Compliance readiness (GDPR, SOC2, HIPAA)\n   - Security scoring\n\n5. **Migration Analysis** (when applicable)\n   - Migration complexity scoring\n   - Code change estimates\n   - Data migration requirements\n   - Downtime assessment\n   - Risk mitigation strategies\n\n6. **Performance Benchmarks**\n   - Throughput/latency comparisons\n   - Resource usage analysis\n   - Scalability characteristics\n\n## How to Use\n\n### Basic Invocations\n\n**Quick Comparison**:\n```\n\"Compare React vs Vue for our SaaS dashboard project\"\n\"PostgreSQL vs MongoDB for our application\"\n```\n\n**Stack Evaluation**:\n```\n\"Evaluate technology stack for real-time collaboration platform:\nNode.js, WebSockets, Redis, PostgreSQL\"\n```\n\n**TCO Analysis**:\n```\n\"Calculate total cost of ownership for AWS vs Azure for our workload:\n- 50 EC2/VM instances\n- 10TB storage\n- High bandwidth requirements\"\n```\n\n**Security Assessment**:\n```\n\"Analyze security posture of our current stack:\nExpress.js, MongoDB, JWT authentication.\nNeed SOC2 compliance.\"\n```\n\n**Migration Path**:\n```\n\"Assess migration from Angular.js (1.x) to React.\nApplication has 50,000 lines of code, 200 components.\"\n```\n\n### Advanced Invocations\n\n**Custom Analysis Sections**:\n```\n\"Compare Next.js vs Nuxt.js.\nInclude: Ecosystem health, TCO, and performance benchmarks.\nSkip: Migration analysis, compliance.\"\n```\n\n**Weighted Decision Criteria**:\n```\n\"Compare cloud providers for ML workloads.\nPriorities (weighted):\n- GPU availability (40%)\n- Cost (30%)\n- Ecosystem (20%)\n- Support (10%)\"\n```\n\n**Multi-Technology Comparison**:\n```\n\"Compare: React, Vue, Svelte, Angular for enterprise SaaS.\nUse case: Large team (20+ developers), complex state management.\nGenerate comprehensive decision matrix.\"\n```\n\n## Scripts\n\n### Core Modules\n\n- **`stack_comparator.py`**: Main comparison engine with weighted scoring algorithms\n- **`tco_calculator.py`**: Total Cost of Ownership calculations (licensing, hosting, developer productivity, scaling)\n- **`ecosystem_analyzer.py`**: Community health scoring, GitHub/npm metrics, viability assessment\n- **`security_assessor.py`**: Vulnerability analysis, compliance readiness, security scoring\n- **`migration_analyzer.py`**: Migration complexity scoring, risk assessment, effort estimation\n- **`format_detector.py`**: Automatic input format detection (text, YAML, JSON, URLs)\n- **`report_generator.py`**: Context-aware report generation with progressive disclosure\n\n### Utility Modules\n\n- **`data_fetcher.py`**: Fetch real-time data from GitHub, npm, CVE databases\n- **`benchmark_processor.py`**: Process and normalize performance benchmark data\n- **`confidence_scorer.py`**: Calculate confidence levels for recommendations\n\n## Metrics and Calculations\n\n### 1. Scoring & Comparison Metrics\n\n**Technology Comparison Matrix**:\n- Feature completeness (0-100 scale)\n- Learning curve assessment (Easy/Medium/Hard)\n- Developer experience scoring\n- Documentation quality (0-10 scale)\n- Weighted total scores\n\n**Decision Scoring Algorithm**:\n- User-defined weights for criteria\n- Normalized scoring (0-100)\n- Confidence intervals\n- Sensitivity analysis\n\n### 2. Financial Calculations\n\n**TCO Components**:\n- **Initial Costs**: Licensing, training, migration\n- **Operational Costs**: Hosting, support, maintenance (monthly/yearly)\n- **Scaling Costs**: Per-user costs, infrastructure scaling projections\n- **Developer Productivity**: Time-to-market impact, development speed multipliers\n- **Hidden Costs**: Technical debt, vendor lock-in risks\n\n**ROI Calculations**:\n- Cost savings projections (3-year, 5-year)\n- Productivity gains (developer hours saved)\n- Break-even analysis\n- Risk-adjusted returns\n\n**Cost Per Metric**:\n- Cost per user (monthly/yearly)\n- Cost per API request\n- Cost per GB stored/transferred\n- Cost per compute hour\n\n### 3. Maturity & Ecosystem Metrics\n\n**Health Scoring (0-100 scale)**:\n- **GitHub Metrics**: Stars, forks, contributors, commit frequency\n- **npm Metrics**: Weekly downloads, version stability, dependency count\n- **Release Cadence**: Regular releases, semantic versioning adherence\n- **Issue Management**: Response time, resolution rate, open vs closed issues\n\n**Community Metrics**:\n- Active maintainers count\n- Contributor growth rate\n- Stack Overflow question volume\n- Job market demand (job postings analysis)\n\n**Viability Assessment**:\n- Corporate backing strength\n- Community sustainability\n- Alternative availability\n- Long-term risk scoring\n\n### 4. Security & Compliance Metrics\n\n**Security Scoring**:\n- **CVE Count**: Known vulnerabilities (last 12 months, last 3 years)\n- **Severity Distribution**: Critical/High/Medium/Low vulnerability counts\n- **Patch Frequency**: Average time to patch (days)\n- **Security Track Record**: Historical security posture\n\n**Compliance Readiness**:\n- **GDPR**: Data privacy features, consent management, data portability\n- **SOC2**: Access controls, encryption, audit logging\n- **HIPAA**: PHI handling, encryption standards, access controls\n- **PCI-DSS**: Payment data security (if applicable)\n\n**Compliance Scoring (per standard)**:\n- Ready: 90-100% compliant\n- Mostly Ready: 70-89% (minor gaps)\n- Partial: 50-69% (significant work needed)\n- Not Ready: <50% (major gaps)\n\n### 5. Migration Analysis Metrics\n\n**Complexity Scoring (1-10 scale)**:\n- **Code Changes**: Estimated lines of code affected\n- **Architecture Impact**: Breaking changes, API compatibility\n- **Data Migration**: Schema changes, data transformation complexity\n- **Downtime Requirements**: Zero-downtime possible vs planned outage\n\n**Effort Estimation**:\n- Development hours (by component)\n- Testing hours\n- Training hours\n- Total person-months\n\n**Risk Assessment**:\n- **Technical Risks**: API incompatibilities, performance regressions\n- **Business Risks**: Downtime impact, feature parity gaps\n- **Team Risks**: Learning curve, skill gaps\n- **Mitigation Strategies**: Risk-specific recommendations\n\n**Migration Phases**:\n- Phase 1: Planning and prototyping (timeline, effort)\n- Phase 2: Core migration (timeline, effort)\n- Phase 3: Testing and validation (timeline, effort)\n- Phase 4: Deployment and monitoring (timeline, effort)\n\n### 6. Performance Benchmark Metrics\n\n**Throughput/Latency**:\n- Requests per second (RPS)\n- Average response time (ms)\n- P95/P99 latency percentiles\n- Concurrent user capacity\n\n**Resource Usage**:\n- Memory consumption (MB/GB)\n- CPU utilization (%)\n- Storage requirements\n- Network bandwidth\n\n**Scalability Characteristics**:\n- Horizontal scaling efficiency\n- Vertical scaling limits\n- Cost per performance unit\n- Scaling inflection points\n\n## Best Practices\n\n### For Accurate Evaluations\n\n1. **Define Clear Use Case**: Specify exact requirements, constraints, and priorities\n2. **Provide Complete Context**: Team size, existing stack, timeline, budget constraints\n3. **Set Realistic Priorities**: Use weighted criteria (total = 100%) for multi-factor decisions\n4. **Consider Team Skills**: Factor in learning curve and existing expertise\n5. **Think Long-Term**: Evaluate 3-5 year outlook, not just immediate needs\n\n### For TCO Analysis\n\n1. **Include All Cost Components**: Don't forget training, migration, technical debt\n2. **Use Realistic Scaling Projections**: Base on actual growth metrics, not wishful thinking\n3. **Account for Developer Productivity**: Time-to-market and development speed are critical costs\n4. **Consider Hidden Costs**: Vendor lock-in, exit costs, technical debt accumulation\n5. **Validate Assumptions**: Document all TCO assumptions for review\n\n### For Migration Decisions\n\n1. **Start with Risk Assessment**: Identify showstoppers early\n2. **Plan Incremental Migration**: Avoid big-bang rewrites when possible\n3. **Prototype Critical Paths**: Test complex migration scenarios before committing\n4. **Build Rollback Plans**: Always have a fallback strategy\n5. **Measure Baseline Performance**: Establish current metrics before migration\n\n### For Security Evaluation\n\n1. **Check Recent Vulnerabilities**: Focus on last 12 months for current security posture\n2. **Review Patch Response Time**: Fast patching is more important than zero vulnerabilities\n3. **Validate Compliance Claims**: Vendor claims â‰  actual compliance readiness\n4. **Consider Supply Chain**: Evaluate security of all dependencies\n5. **Test Security Features**: Don't assume features work as documented\n\n## Limitations\n\n### Data Accuracy\n\n- **Ecosystem metrics** are point-in-time snapshots (GitHub stars, npm downloads change rapidly)\n- **TCO calculations** are estimates based on provided assumptions and market rates\n- **Benchmark data** may not reflect your specific use case or configuration\n- **Security vulnerability counts** depend on public CVE database completeness\n\n### Scope Boundaries\n\n- **Industry-Specific Requirements**: Some specialized industries may have unique constraints not covered by standard analysis\n- **Emerging Technologies**: Very new technologies (<1 year old) may lack sufficient data for accurate assessment\n- **Custom/Proprietary Solutions**: Cannot evaluate closed-source or internal tools without data\n- **Political/Organizational Factors**: Cannot account for company politics, vendor relationships, or legacy commitments\n\n### Contextual Limitations\n\n- **Team Skill Assessment**: Cannot directly evaluate your team's specific skills and learning capacity\n- **Existing Architecture**: Recommendations assume greenfield unless migration context provided\n- **Budget Constraints**: TCO analysis provides costs but cannot make budget decisions for you\n- **Timeline Pressure**: Cannot account for business deadlines and time-to-market urgency\n\n### When NOT to Use This Skill\n\n- **Trivial Decisions**: Choosing between nearly-identical tools (use team preference)\n- **Mandated Solutions**: When technology choice is already decided by management/policy\n- **Insufficient Context**: When you don't know your requirements, priorities, or constraints\n- **Real-Time Production Decisions**: Use for planning, not emergency production issues\n- **Non-Technical Decisions**: Business strategy, hiring, organizational issues\n\n## Confidence Levels\n\nThe skill provides confidence scores with all recommendations:\n\n- **High Confidence (80-100%)**: Strong data, clear winner, low risk\n- **Medium Confidence (50-79%)**: Good data, trade-offs present, moderate risk\n- **Low Confidence (<50%)**: Limited data, close call, high uncertainty\n- **Insufficient Data**: Cannot make recommendation without more information\n\nConfidence is based on:\n- Data completeness and recency\n- Consensus across multiple metrics\n- Clarity of use case requirements\n- Industry maturity and standards\n",
      "frontmatter": {
        "name": "tech-stack-evaluator",
        "description": "Comprehensive technology stack evaluation and comparison tool with TCO analysis, security assessment, and intelligent recommendations for engineering teams"
      },
      "content": "\n# Technology Stack Evaluator\n\nA comprehensive evaluation framework for comparing technologies, frameworks, cloud providers, and complete technology stacks. Provides data-driven recommendations with TCO analysis, security assessment, ecosystem health scoring, and migration path analysis.\n\n## Capabilities\n\nThis skill provides eight comprehensive evaluation capabilities:\n\n- **Technology Comparison**: Head-to-head comparisons of frameworks, languages, and tools (React vs Vue, PostgreSQL vs MongoDB, Node.js vs Python)\n- **Stack Evaluation**: Assess complete technology stacks for specific use cases (real-time collaboration, API-heavy SaaS, data-intensive platforms)\n- **Maturity & Ecosystem Analysis**: Evaluate community health, maintenance status, long-term viability, and ecosystem strength\n- **Total Cost of Ownership (TCO)**: Calculate comprehensive costs including licensing, hosting, developer productivity, and scaling\n- **Security & Compliance**: Analyze vulnerabilities, compliance readiness (GDPR, SOC2, HIPAA), and security posture\n- **Migration Path Analysis**: Assess migration complexity, risks, timelines, and strategies from legacy to modern stacks\n- **Cloud Provider Comparison**: Compare AWS vs Azure vs GCP for specific workloads with cost and feature analysis\n- **Decision Reports**: Generate comprehensive decision matrices with pros/cons, confidence scores, and actionable recommendations\n\n## Input Requirements\n\n### Flexible Input Formats (Automatic Detection)\n\nThe skill automatically detects and processes multiple input formats:\n\n**Text/Conversational**:\n```\n\"Compare React vs Vue for building a SaaS dashboard\"\n\"Evaluate technology stack for real-time collaboration platform\"\n\"Should we migrate from MongoDB to PostgreSQL?\"\n```\n\n**Structured (YAML)**:\n```yaml\ncomparison:\n  technologies:\n    - name: \"React\"\n    - name: \"Vue\"\n  use_case: \"SaaS dashboard\"\n  priorities:\n    - \"Developer productivity\"\n    - \"Ecosystem maturity\"\n    - \"Performance\"\n```\n\n**Structured (JSON)**:\n```json\n{\n  \"comparison\": {\n    \"technologies\": [\"React\", \"Vue\"],\n    \"use_case\": \"SaaS dashboard\",\n    \"priorities\": [\"Developer productivity\", \"Ecosystem maturity\"]\n  }\n}\n```\n\n**URLs for Ecosystem Analysis**:\n- GitHub repository URLs (for health scoring)\n- npm package URLs (for download statistics)\n- Technology documentation URLs (for feature extraction)\n\n### Analysis Scope Selection\n\nUsers can select which analyses to run:\n- **Quick Comparison**: Basic scoring and comparison (200-300 tokens)\n- **Standard Analysis**: Scoring + TCO + Security (500-800 tokens)\n- **Comprehensive Report**: All analyses including migration paths (1200-1500 tokens)\n- **Custom**: User selects specific sections (modular)\n\n## Output Formats\n\n### Context-Aware Output\n\nThe skill automatically adapts output based on environment:\n\n**Claude Desktop (Rich Markdown)**:\n- Formatted tables with color indicators\n- Expandable sections for detailed analysis\n- Visual decision matrices\n- Charts and graphs (when appropriate)\n\n**CLI/Terminal (Terminal-Friendly)**:\n- Plain text tables with ASCII borders\n- Compact formatting\n- Clear section headers\n- Copy-paste friendly code blocks\n\n### Progressive Disclosure Structure\n\n**Executive Summary (200-300 tokens)**:\n- Recommendation summary\n- Top 3 pros and cons\n- Confidence level (High/Medium/Low)\n- Key decision factors\n\n**Detailed Breakdown (on-demand)**:\n- Complete scoring matrices\n- Detailed TCO calculations\n- Full security analysis\n- Migration complexity assessment\n- All supporting data and calculations\n\n### Report Sections (User-Selectable)\n\nUsers choose which sections to include:\n\n1. **Scoring & Comparison Matrix**\n   - Weighted decision scores\n   - Head-to-head comparison tables\n   - Strengths and weaknesses\n\n2. **Financial Analysis**\n   - TCO breakdown (5-year projection)\n   - ROI analysis\n   - Cost per user/request metrics\n   - Hidden cost identification\n\n3. **Ecosystem Health**\n   - Community size and activity\n   - GitHub stars, npm downloads\n   - Release frequency and maintenance\n   - Issue response times\n   - Viability assessment\n\n4. **Security & Compliance**\n   - Vulnerability count (CVE database)\n   - Security patch frequency\n   - Compliance readiness (GDPR, SOC2, HIPAA)\n   - Security scoring\n\n5. **Migration Analysis** (when applicable)\n   - Migration complexity scoring\n   - Code change estimates\n   - Data migration requirements\n   - Downtime assessment\n   - Risk mitigation strategies\n\n6. **Performance Benchmarks**\n   - Throughput/latency comparisons\n   - Resource usage analysis\n   - Scalability characteristics\n\n## How to Use\n\n### Basic Invocations\n\n**Quick Comparison**:\n```\n\"Compare React vs Vue for our SaaS dashboard project\"\n\"PostgreSQL vs MongoDB for our application\"\n```\n\n**Stack Evaluation**:\n```\n\"Evaluate technology stack for real-time collaboration platform:\nNode.js, WebSockets, Redis, PostgreSQL\"\n```\n\n**TCO Analysis**:\n```\n\"Calculate total cost of ownership for AWS vs Azure for our workload:\n- 50 EC2/VM instances\n- 10TB storage\n- High bandwidth requirements\"\n```\n\n**Security Assessment**:\n```\n\"Analyze security posture of our current stack:\nExpress.js, MongoDB, JWT authentication.\nNeed SOC2 compliance.\"\n```\n\n**Migration Path**:\n```\n\"Assess migration from Angular.js (1.x) to React.\nApplication has 50,000 lines of code, 200 components.\"\n```\n\n### Advanced Invocations\n\n**Custom Analysis Sections**:\n```\n\"Compare Next.js vs Nuxt.js.\nInclude: Ecosystem health, TCO, and performance benchmarks.\nSkip: Migration analysis, compliance.\"\n```\n\n**Weighted Decision Criteria**:\n```\n\"Compare cloud providers for ML workloads.\nPriorities (weighted):\n- GPU availability (40%)\n- Cost (30%)\n- Ecosystem (20%)\n- Support (10%)\"\n```\n\n**Multi-Technology Comparison**:\n```\n\"Compare: React, Vue, Svelte, Angular for enterprise SaaS.\nUse case: Large team (20+ developers), complex state management.\nGenerate comprehensive decision matrix.\"\n```\n\n## Scripts\n\n### Core Modules\n\n- **`stack_comparator.py`**: Main comparison engine with weighted scoring algorithms\n- **`tco_calculator.py`**: Total Cost of Ownership calculations (licensing, hosting, developer productivity, scaling)\n- **`ecosystem_analyzer.py`**: Community health scoring, GitHub/npm metrics, viability assessment\n- **`security_assessor.py`**: Vulnerability analysis, compliance readiness, security scoring\n- **`migration_analyzer.py`**: Migration complexity scoring, risk assessment, effort estimation\n- **`format_detector.py`**: Automatic input format detection (text, YAML, JSON, URLs)\n- **`report_generator.py`**: Context-aware report generation with progressive disclosure\n\n### Utility Modules\n\n- **`data_fetcher.py`**: Fetch real-time data from GitHub, npm, CVE databases\n- **`benchmark_processor.py`**: Process and normalize performance benchmark data\n- **`confidence_scorer.py`**: Calculate confidence levels for recommendations\n\n## Metrics and Calculations\n\n### 1. Scoring & Comparison Metrics\n\n**Technology Comparison Matrix**:\n- Feature completeness (0-100 scale)\n- Learning curve assessment (Easy/Medium/Hard)\n- Developer experience scoring\n- Documentation quality (0-10 scale)\n- Weighted total scores\n\n**Decision Scoring Algorithm**:\n- User-defined weights for criteria\n- Normalized scoring (0-100)\n- Confidence intervals\n- Sensitivity analysis\n\n### 2. Financial Calculations\n\n**TCO Components**:\n- **Initial Costs**: Licensing, training, migration\n- **Operational Costs**: Hosting, support, maintenance (monthly/yearly)\n- **Scaling Costs**: Per-user costs, infrastructure scaling projections\n- **Developer Productivity**: Time-to-market impact, development speed multipliers\n- **Hidden Costs**: Technical debt, vendor lock-in risks\n\n**ROI Calculations**:\n- Cost savings projections (3-year, 5-year)\n- Productivity gains (developer hours saved)\n- Break-even analysis\n- Risk-adjusted returns\n\n**Cost Per Metric**:\n- Cost per user (monthly/yearly)\n- Cost per API request\n- Cost per GB stored/transferred\n- Cost per compute hour\n\n### 3. Maturity & Ecosystem Metrics\n\n**Health Scoring (0-100 scale)**:\n- **GitHub Metrics**: Stars, forks, contributors, commit frequency\n- **npm Metrics**: Weekly downloads, version stability, dependency count\n- **Release Cadence**: Regular releases, semantic versioning adherence\n- **Issue Management**: Response time, resolution rate, open vs closed issues\n\n**Community Metrics**:\n- Active maintainers count\n- Contributor growth rate\n- Stack Overflow question volume\n- Job market demand (job postings analysis)\n\n**Viability Assessment**:\n- Corporate backing strength\n- Community sustainability\n- Alternative availability\n- Long-term risk scoring\n\n### 4. Security & Compliance Metrics\n\n**Security Scoring**:\n- **CVE Count**: Known vulnerabilities (last 12 months, last 3 years)\n- **Severity Distribution**: Critical/High/Medium/Low vulnerability counts\n- **Patch Frequency**: Average time to patch (days)\n- **Security Track Record**: Historical security posture\n\n**Compliance Readiness**:\n- **GDPR**: Data privacy features, consent management, data portability\n- **SOC2**: Access controls, encryption, audit logging\n- **HIPAA**: PHI handling, encryption standards, access controls\n- **PCI-DSS**: Payment data security (if applicable)\n\n**Compliance Scoring (per standard)**:\n- Ready: 90-100% compliant\n- Mostly Ready: 70-89% (minor gaps)\n- Partial: 50-69% (significant work needed)\n- Not Ready: <50% (major gaps)\n\n### 5. Migration Analysis Metrics\n\n**Complexity Scoring (1-10 scale)**:\n- **Code Changes**: Estimated lines of code affected\n- **Architecture Impact**: Breaking changes, API compatibility\n- **Data Migration**: Schema changes, data transformation complexity\n- **Downtime Requirements**: Zero-downtime possible vs planned outage\n\n**Effort Estimation**:\n- Development hours (by component)\n- Testing hours\n- Training hours\n- Total person-months\n\n**Risk Assessment**:\n- **Technical Risks**: API incompatibilities, performance regressions\n- **Business Risks**: Downtime impact, feature parity gaps\n- **Team Risks**: Learning curve, skill gaps\n- **Mitigation Strategies**: Risk-specific recommendations\n\n**Migration Phases**:\n- Phase 1: Planning and prototyping (timeline, effort)\n- Phase 2: Core migration (timeline, effort)\n- Phase 3: Testing and validation (timeline, effort)\n- Phase 4: Deployment and monitoring (timeline, effort)\n\n### 6. Performance Benchmark Metrics\n\n**Throughput/Latency**:\n- Requests per second (RPS)\n- Average response time (ms)\n- P95/P99 latency percentiles\n- Concurrent user capacity\n\n**Resource Usage**:\n- Memory consumption (MB/GB)\n- CPU utilization (%)\n- Storage requirements\n- Network bandwidth\n\n**Scalability Characteristics**:\n- Horizontal scaling efficiency\n- Vertical scaling limits\n- Cost per performance unit\n- Scaling inflection points\n\n## Best Practices\n\n### For Accurate Evaluations\n\n1. **Define Clear Use Case**: Specify exact requirements, constraints, and priorities\n2. **Provide Complete Context**: Team size, existing stack, timeline, budget constraints\n3. **Set Realistic Priorities**: Use weighted criteria (total = 100%) for multi-factor decisions\n4. **Consider Team Skills**: Factor in learning curve and existing expertise\n5. **Think Long-Term**: Evaluate 3-5 year outlook, not just immediate needs\n\n### For TCO Analysis\n\n1. **Include All Cost Components**: Don't forget training, migration, technical debt\n2. **Use Realistic Scaling Projections**: Base on actual growth metrics, not wishful thinking\n3. **Account for Developer Productivity**: Time-to-market and development speed are critical costs\n4. **Consider Hidden Costs**: Vendor lock-in, exit costs, technical debt accumulation\n5. **Validate Assumptions**: Document all TCO assumptions for review\n\n### For Migration Decisions\n\n1. **Start with Risk Assessment**: Identify showstoppers early\n2. **Plan Incremental Migration**: Avoid big-bang rewrites when possible\n3. **Prototype Critical Paths**: Test complex migration scenarios before committing\n4. **Build Rollback Plans**: Always have a fallback strategy\n5. **Measure Baseline Performance**: Establish current metrics before migration\n\n### For Security Evaluation\n\n1. **Check Recent Vulnerabilities**: Focus on last 12 months for current security posture\n2. **Review Patch Response Time**: Fast patching is more important than zero vulnerabilities\n3. **Validate Compliance Claims**: Vendor claims â‰  actual compliance readiness\n4. **Consider Supply Chain**: Evaluate security of all dependencies\n5. **Test Security Features**: Don't assume features work as documented\n\n## Limitations\n\n### Data Accuracy\n\n- **Ecosystem metrics** are point-in-time snapshots (GitHub stars, npm downloads change rapidly)\n- **TCO calculations** are estimates based on provided assumptions and market rates\n- **Benchmark data** may not reflect your specific use case or configuration\n- **Security vulnerability counts** depend on public CVE database completeness\n\n### Scope Boundaries\n\n- **Industry-Specific Requirements**: Some specialized industries may have unique constraints not covered by standard analysis\n- **Emerging Technologies**: Very new technologies (<1 year old) may lack sufficient data for accurate assessment\n- **Custom/Proprietary Solutions**: Cannot evaluate closed-source or internal tools without data\n- **Political/Organizational Factors**: Cannot account for company politics, vendor relationships, or legacy commitments\n\n### Contextual Limitations\n\n- **Team Skill Assessment**: Cannot directly evaluate your team's specific skills and learning capacity\n- **Existing Architecture**: Recommendations assume greenfield unless migration context provided\n- **Budget Constraints**: TCO analysis provides costs but cannot make budget decisions for you\n- **Timeline Pressure**: Cannot account for business deadlines and time-to-market urgency\n\n### When NOT to Use This Skill\n\n- **Trivial Decisions**: Choosing between nearly-identical tools (use team preference)\n- **Mandated Solutions**: When technology choice is already decided by management/policy\n- **Insufficient Context**: When you don't know your requirements, priorities, or constraints\n- **Real-Time Production Decisions**: Use for planning, not emergency production issues\n- **Non-Technical Decisions**: Business strategy, hiring, organizational issues\n\n## Confidence Levels\n\nThe skill provides confidence scores with all recommendations:\n\n- **High Confidence (80-100%)**: Strong data, clear winner, low risk\n- **Medium Confidence (50-79%)**: Good data, trade-offs present, moderate risk\n- **Low Confidence (<50%)**: Limited data, close call, high uncertainty\n- **Insufficient Data**: Cannot make recommendation without more information\n\nConfidence is based on:\n- Data completeness and recency\n- Consensus across multiple metrics\n- Clarity of use case requirements\n- Industry maturity and standards\n"
    }
  },
  "alirezarezvani-claude-skills-senior-data-scientist": {
    "id": "alirezarezvani-claude-skills-senior-data-scientist",
    "name": "senior-data-scientist",
    "description": "World-class data science skill for statistical modeling, experimentation, causal inference, and advanced analytics. Expertise in Python (NumPy, Pandas, Scikit-learn), R, SQL, statistical methods, A/B testing, time series, and business intelligence. Includes experiment design, feature engineering, model evaluation, and stakeholder communication. Use when designing experiments, building predictive models, performing causal analysis, or driving data-driven decisions.",
    "repo": {
      "owner": "alirezarezvani",
      "name": "claude-skills",
      "fullName": "alirezarezvani/claude-skills",
      "url": "https://github.com/alirezarezvani/claude-skills/tree/main/engineering-team/senior-data-scientist",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 521,
      "forks": 103,
      "language": "Python",
      "topics": [
        "agentic-ai",
        "agentic-coding",
        "anthropic-claude",
        "claude-ai",
        "claude-ai-skills",
        "claude-code",
        "claude-code-skills",
        "claude-skills",
        "claudecode-subagents"
      ],
      "updatedAt": "2026-01-08T12:36:14Z",
      "pushedAt": "2026-01-08T08:39:21Z",
      "createdAt": "2025-10-19T04:04:05Z",
      "license": "MIT License"
    },
    "category": "development",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: senior-data-scientist\ndescription: World-class data science skill for statistical modeling, experimentation, causal inference, and advanced analytics. Expertise in Python (NumPy, Pandas, Scikit-learn), R, SQL, statistical methods, A/B testing, time series, and business intelligence. Includes experiment design, feature engineering, model evaluation, and stakeholder communication. Use when designing experiments, building predictive models, performing causal analysis, or driving data-driven decisions.\n---\n\n# Senior Data Scientist\n\nWorld-class senior data scientist skill for production-grade AI/ML/Data systems.\n\n## Quick Start\n\n### Main Capabilities\n\n```bash\n# Core Tool 1\npython scripts/experiment_designer.py --input data/ --output results/\n\n# Core Tool 2  \npython scripts/feature_engineering_pipeline.py --target project/ --analyze\n\n# Core Tool 3\npython scripts/model_evaluation_suite.py --config config.yaml --deploy\n```\n\n## Core Expertise\n\nThis skill covers world-class capabilities in:\n\n- Advanced production patterns and architectures\n- Scalable system design and implementation\n- Performance optimization at scale\n- MLOps and DataOps best practices\n- Real-time processing and inference\n- Distributed computing frameworks\n- Model deployment and monitoring\n- Security and compliance\n- Cost optimization\n- Team leadership and mentoring\n\n## Tech Stack\n\n**Languages:** Python, SQL, R, Scala, Go\n**ML Frameworks:** PyTorch, TensorFlow, Scikit-learn, XGBoost\n**Data Tools:** Spark, Airflow, dbt, Kafka, Databricks\n**LLM Frameworks:** LangChain, LlamaIndex, DSPy\n**Deployment:** Docker, Kubernetes, AWS/GCP/Azure\n**Monitoring:** MLflow, Weights & Biases, Prometheus\n**Databases:** PostgreSQL, BigQuery, Snowflake, Pinecone\n\n## Reference Documentation\n\n### 1. Statistical Methods Advanced\n\nComprehensive guide available in `references/statistical_methods_advanced.md` covering:\n\n- Advanced patterns and best practices\n- Production implementation strategies\n- Performance optimization techniques\n- Scalability considerations\n- Security and compliance\n- Real-world case studies\n\n### 2. Experiment Design Frameworks\n\nComplete workflow documentation in `references/experiment_design_frameworks.md` including:\n\n- Step-by-step processes\n- Architecture design patterns\n- Tool integration guides\n- Performance tuning strategies\n- Troubleshooting procedures\n\n### 3. Feature Engineering Patterns\n\nTechnical reference guide in `references/feature_engineering_patterns.md` with:\n\n- System design principles\n- Implementation examples\n- Configuration best practices\n- Deployment strategies\n- Monitoring and observability\n\n## Production Patterns\n\n### Pattern 1: Scalable Data Processing\n\nEnterprise-scale data processing with distributed computing:\n\n- Horizontal scaling architecture\n- Fault-tolerant design\n- Real-time and batch processing\n- Data quality validation\n- Performance monitoring\n\n### Pattern 2: ML Model Deployment\n\nProduction ML system with high availability:\n\n- Model serving with low latency\n- A/B testing infrastructure\n- Feature store integration\n- Model monitoring and drift detection\n- Automated retraining pipelines\n\n### Pattern 3: Real-Time Inference\n\nHigh-throughput inference system:\n\n- Batching and caching strategies\n- Load balancing\n- Auto-scaling\n- Latency optimization\n- Cost optimization\n\n## Best Practices\n\n### Development\n\n- Test-driven development\n- Code reviews and pair programming\n- Documentation as code\n- Version control everything\n- Continuous integration\n\n### Production\n\n- Monitor everything critical\n- Automate deployments\n- Feature flags for releases\n- Canary deployments\n- Comprehensive logging\n\n### Team Leadership\n\n- Mentor junior engineers\n- Drive technical decisions\n- Establish coding standards\n- Foster learning culture\n- Cross-functional collaboration\n\n## Performance Targets\n\n**Latency:**\n- P50: < 50ms\n- P95: < 100ms\n- P99: < 200ms\n\n**Throughput:**\n- Requests/second: > 1000\n- Concurrent users: > 10,000\n\n**Availability:**\n- Uptime: 99.9%\n- Error rate: < 0.1%\n\n## Security & Compliance\n\n- Authentication & authorization\n- Data encryption (at rest & in transit)\n- PII handling and anonymization\n- GDPR/CCPA compliance\n- Regular security audits\n- Vulnerability management\n\n## Common Commands\n\n```bash\n# Development\npython -m pytest tests/ -v --cov\npython -m black src/\npython -m pylint src/\n\n# Training\npython scripts/train.py --config prod.yaml\npython scripts/evaluate.py --model best.pth\n\n# Deployment\ndocker build -t service:v1 .\nkubectl apply -f k8s/\nhelm upgrade service ./charts/\n\n# Monitoring\nkubectl logs -f deployment/service\npython scripts/health_check.py\n```\n\n## Resources\n\n- Advanced Patterns: `references/statistical_methods_advanced.md`\n- Implementation Guide: `references/experiment_design_frameworks.md`\n- Technical Reference: `references/feature_engineering_patterns.md`\n- Automation Scripts: `scripts/` directory\n\n## Senior-Level Responsibilities\n\nAs a world-class senior professional:\n\n1. **Technical Leadership**\n   - Drive architectural decisions\n   - Mentor team members\n   - Establish best practices\n   - Ensure code quality\n\n2. **Strategic Thinking**\n   - Align with business goals\n   - Evaluate trade-offs\n   - Plan for scale\n   - Manage technical debt\n\n3. **Collaboration**\n   - Work across teams\n   - Communicate effectively\n   - Build consensus\n   - Share knowledge\n\n4. **Innovation**\n   - Stay current with research\n   - Experiment with new approaches\n   - Contribute to community\n   - Drive continuous improvement\n\n5. **Production Excellence**\n   - Ensure high availability\n   - Monitor proactively\n   - Optimize performance\n   - Respond to incidents\n",
      "frontmatter": {
        "name": "senior-data-scientist",
        "description": "World-class data science skill for statistical modeling, experimentation, causal inference, and advanced analytics. Expertise in Python (NumPy, Pandas, Scikit-learn), R, SQL, statistical methods, A/B testing, time series, and business intelligence. Includes experiment design, feature engineering, model evaluation, and stakeholder communication. Use when designing experiments, building predictive models, performing causal analysis, or driving data-driven decisions."
      },
      "content": "\n# Senior Data Scientist\n\nWorld-class senior data scientist skill for production-grade AI/ML/Data systems.\n\n## Quick Start\n\n### Main Capabilities\n\n```bash\n# Core Tool 1\npython scripts/experiment_designer.py --input data/ --output results/\n\n# Core Tool 2  \npython scripts/feature_engineering_pipeline.py --target project/ --analyze\n\n# Core Tool 3\npython scripts/model_evaluation_suite.py --config config.yaml --deploy\n```\n\n## Core Expertise\n\nThis skill covers world-class capabilities in:\n\n- Advanced production patterns and architectures\n- Scalable system design and implementation\n- Performance optimization at scale\n- MLOps and DataOps best practices\n- Real-time processing and inference\n- Distributed computing frameworks\n- Model deployment and monitoring\n- Security and compliance\n- Cost optimization\n- Team leadership and mentoring\n\n## Tech Stack\n\n**Languages:** Python, SQL, R, Scala, Go\n**ML Frameworks:** PyTorch, TensorFlow, Scikit-learn, XGBoost\n**Data Tools:** Spark, Airflow, dbt, Kafka, Databricks\n**LLM Frameworks:** LangChain, LlamaIndex, DSPy\n**Deployment:** Docker, Kubernetes, AWS/GCP/Azure\n**Monitoring:** MLflow, Weights & Biases, Prometheus\n**Databases:** PostgreSQL, BigQuery, Snowflake, Pinecone\n\n## Reference Documentation\n\n### 1. Statistical Methods Advanced\n\nComprehensive guide available in `references/statistical_methods_advanced.md` covering:\n\n- Advanced patterns and best practices\n- Production implementation strategies\n- Performance optimization techniques\n- Scalability considerations\n- Security and compliance\n- Real-world case studies\n\n### 2. Experiment Design Frameworks\n\nComplete workflow documentation in `references/experiment_design_frameworks.md` including:\n\n- Step-by-step processes\n- Architecture design patterns\n- Tool integration guides\n- Performance tuning strategies\n- Troubleshooting procedures\n\n### 3. Feature Engineering Patterns\n\nTechnical reference guide in `references/feature_engineering_patterns.md` with:\n\n- System design principles\n- Implementation examples\n- Configuration best practices\n- Deployment strategies\n- Monitoring and observability\n\n## Production Patterns\n\n### Pattern 1: Scalable Data Processing\n\nEnterprise-scale data processing with distributed computing:\n\n- Horizontal scaling architecture\n- Fault-tolerant design\n- Real-time and batch processing\n- Data quality validation\n- Performance monitoring\n\n### Pattern 2: ML Model Deployment\n\nProduction ML system with high availability:\n\n- Model serving with low latency\n- A/B testing infrastructure\n- Feature store integration\n- Model monitoring and drift detection\n- Automated retraining pipelines\n\n### Pattern 3: Real-Time Inference\n\nHigh-throughput inference system:\n\n- Batching and caching strategies\n- Load balancing\n- Auto-scaling\n- Latency optimization\n- Cost optimization\n\n## Best Practices\n\n### Development\n\n- Test-driven development\n- Code reviews and pair programming\n- Documentation as code\n- Version control everything\n- Continuous integration\n\n### Production\n\n- Monitor everything critical\n- Automate deployments\n- Feature flags for releases\n- Canary deployments\n- Comprehensive logging\n\n### Team Leadership\n\n- Mentor junior engineers\n- Drive technical decisions\n- Establish coding standards\n- Foster learning culture\n- Cross-functional collaboration\n\n## Performance Targets\n\n**Latency:**\n- P50: < 50ms\n- P95: < 100ms\n- P99: < 200ms\n\n**Throughput:**\n- Requests/second: > 1000\n- Concurrent users: > 10,000\n\n**Availability:**\n- Uptime: 99.9%\n- Error rate: < 0.1%\n\n## Security & Compliance\n\n- Authentication & authorization\n- Data encryption (at rest & in transit)\n- PII handling and anonymization\n- GDPR/CCPA compliance\n- Regular security audits\n- Vulnerability management\n\n## Common Commands\n\n```bash\n# Development\npython -m pytest tests/ -v --cov\npython -m black src/\npython -m pylint src/\n\n# Training\npython scripts/train.py --config prod.yaml\npython scripts/evaluate.py --model best.pth\n\n# Deployment\ndocker build -t service:v1 .\nkubectl apply -f k8s/\nhelm upgrade service ./charts/\n\n# Monitoring\nkubectl logs -f deployment/service\npython scripts/health_check.py\n```\n\n## Resources\n\n- Advanced Patterns: `references/statistical_methods_advanced.md`\n- Implementation Guide: `references/experiment_design_frameworks.md`\n- Technical Reference: `references/feature_engineering_patterns.md`\n- Automation Scripts: `scripts/` directory\n\n## Senior-Level Responsibilities\n\nAs a world-class senior professional:\n\n1. **Technical Leadership**\n   - Drive architectural decisions\n   - Mentor team members\n   - Establish best practices\n   - Ensure code quality\n\n2. **Strategic Thinking**\n   - Align with business goals\n   - Evaluate trade-offs\n   - Plan for scale\n   - Manage technical debt\n\n3. **Collaboration**\n   - Work across teams\n   - Communicate effectively\n   - Build consensus\n   - Share knowledge\n\n4. **Innovation**\n   - Stay current with research\n   - Experiment with new approaches\n   - Contribute to community\n   - Drive continuous improvement\n\n5. **Production Excellence**\n   - Ensure high availability\n   - Monitor proactively\n   - Optimize performance\n   - Respond to incidents\n"
    }
  },
  "alirezarezvani-claude-skills-senior-data-engineer": {
    "id": "alirezarezvani-claude-skills-senior-data-engineer",
    "name": "senior-data-engineer",
    "description": "World-class data engineering skill for building scalable data pipelines, ETL/ELT systems, and data infrastructure. Expertise in Python, SQL, Spark, Airflow, dbt, Kafka, and modern data stack. Includes data modeling, pipeline orchestration, data quality, and DataOps. Use when designing data architectures, building data pipelines, optimizing data workflows, or implementing data governance.",
    "repo": {
      "owner": "alirezarezvani",
      "name": "claude-skills",
      "fullName": "alirezarezvani/claude-skills",
      "url": "https://github.com/alirezarezvani/claude-skills/tree/main/engineering-team/senior-data-engineer",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 521,
      "forks": 103,
      "language": "Python",
      "topics": [
        "agentic-ai",
        "agentic-coding",
        "anthropic-claude",
        "claude-ai",
        "claude-ai-skills",
        "claude-code",
        "claude-code-skills",
        "claude-skills",
        "claudecode-subagents"
      ],
      "updatedAt": "2026-01-08T12:36:14Z",
      "pushedAt": "2026-01-08T08:39:21Z",
      "createdAt": "2025-10-19T04:04:05Z",
      "license": "MIT License"
    },
    "category": "development",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: senior-data-engineer\ndescription: World-class data engineering skill for building scalable data pipelines, ETL/ELT systems, and data infrastructure. Expertise in Python, SQL, Spark, Airflow, dbt, Kafka, and modern data stack. Includes data modeling, pipeline orchestration, data quality, and DataOps. Use when designing data architectures, building data pipelines, optimizing data workflows, or implementing data governance.\n---\n\n# Senior Data Engineer\n\nWorld-class senior data engineer skill for production-grade AI/ML/Data systems.\n\n## Quick Start\n\n### Main Capabilities\n\n```bash\n# Core Tool 1\npython scripts/pipeline_orchestrator.py --input data/ --output results/\n\n# Core Tool 2  \npython scripts/data_quality_validator.py --target project/ --analyze\n\n# Core Tool 3\npython scripts/etl_performance_optimizer.py --config config.yaml --deploy\n```\n\n## Core Expertise\n\nThis skill covers world-class capabilities in:\n\n- Advanced production patterns and architectures\n- Scalable system design and implementation\n- Performance optimization at scale\n- MLOps and DataOps best practices\n- Real-time processing and inference\n- Distributed computing frameworks\n- Model deployment and monitoring\n- Security and compliance\n- Cost optimization\n- Team leadership and mentoring\n\n## Tech Stack\n\n**Languages:** Python, SQL, R, Scala, Go\n**ML Frameworks:** PyTorch, TensorFlow, Scikit-learn, XGBoost\n**Data Tools:** Spark, Airflow, dbt, Kafka, Databricks\n**LLM Frameworks:** LangChain, LlamaIndex, DSPy\n**Deployment:** Docker, Kubernetes, AWS/GCP/Azure\n**Monitoring:** MLflow, Weights & Biases, Prometheus\n**Databases:** PostgreSQL, BigQuery, Snowflake, Pinecone\n\n## Reference Documentation\n\n### 1. Data Pipeline Architecture\n\nComprehensive guide available in `references/data_pipeline_architecture.md` covering:\n\n- Advanced patterns and best practices\n- Production implementation strategies\n- Performance optimization techniques\n- Scalability considerations\n- Security and compliance\n- Real-world case studies\n\n### 2. Data Modeling Patterns\n\nComplete workflow documentation in `references/data_modeling_patterns.md` including:\n\n- Step-by-step processes\n- Architecture design patterns\n- Tool integration guides\n- Performance tuning strategies\n- Troubleshooting procedures\n\n### 3. Dataops Best Practices\n\nTechnical reference guide in `references/dataops_best_practices.md` with:\n\n- System design principles\n- Implementation examples\n- Configuration best practices\n- Deployment strategies\n- Monitoring and observability\n\n## Production Patterns\n\n### Pattern 1: Scalable Data Processing\n\nEnterprise-scale data processing with distributed computing:\n\n- Horizontal scaling architecture\n- Fault-tolerant design\n- Real-time and batch processing\n- Data quality validation\n- Performance monitoring\n\n### Pattern 2: ML Model Deployment\n\nProduction ML system with high availability:\n\n- Model serving with low latency\n- A/B testing infrastructure\n- Feature store integration\n- Model monitoring and drift detection\n- Automated retraining pipelines\n\n### Pattern 3: Real-Time Inference\n\nHigh-throughput inference system:\n\n- Batching and caching strategies\n- Load balancing\n- Auto-scaling\n- Latency optimization\n- Cost optimization\n\n## Best Practices\n\n### Development\n\n- Test-driven development\n- Code reviews and pair programming\n- Documentation as code\n- Version control everything\n- Continuous integration\n\n### Production\n\n- Monitor everything critical\n- Automate deployments\n- Feature flags for releases\n- Canary deployments\n- Comprehensive logging\n\n### Team Leadership\n\n- Mentor junior engineers\n- Drive technical decisions\n- Establish coding standards\n- Foster learning culture\n- Cross-functional collaboration\n\n## Performance Targets\n\n**Latency:**\n- P50: < 50ms\n- P95: < 100ms\n- P99: < 200ms\n\n**Throughput:**\n- Requests/second: > 1000\n- Concurrent users: > 10,000\n\n**Availability:**\n- Uptime: 99.9%\n- Error rate: < 0.1%\n\n## Security & Compliance\n\n- Authentication & authorization\n- Data encryption (at rest & in transit)\n- PII handling and anonymization\n- GDPR/CCPA compliance\n- Regular security audits\n- Vulnerability management\n\n## Common Commands\n\n```bash\n# Development\npython -m pytest tests/ -v --cov\npython -m black src/\npython -m pylint src/\n\n# Training\npython scripts/train.py --config prod.yaml\npython scripts/evaluate.py --model best.pth\n\n# Deployment\ndocker build -t service:v1 .\nkubectl apply -f k8s/\nhelm upgrade service ./charts/\n\n# Monitoring\nkubectl logs -f deployment/service\npython scripts/health_check.py\n```\n\n## Resources\n\n- Advanced Patterns: `references/data_pipeline_architecture.md`\n- Implementation Guide: `references/data_modeling_patterns.md`\n- Technical Reference: `references/dataops_best_practices.md`\n- Automation Scripts: `scripts/` directory\n\n## Senior-Level Responsibilities\n\nAs a world-class senior professional:\n\n1. **Technical Leadership**\n   - Drive architectural decisions\n   - Mentor team members\n   - Establish best practices\n   - Ensure code quality\n\n2. **Strategic Thinking**\n   - Align with business goals\n   - Evaluate trade-offs\n   - Plan for scale\n   - Manage technical debt\n\n3. **Collaboration**\n   - Work across teams\n   - Communicate effectively\n   - Build consensus\n   - Share knowledge\n\n4. **Innovation**\n   - Stay current with research\n   - Experiment with new approaches\n   - Contribute to community\n   - Drive continuous improvement\n\n5. **Production Excellence**\n   - Ensure high availability\n   - Monitor proactively\n   - Optimize performance\n   - Respond to incidents\n",
      "frontmatter": {
        "name": "senior-data-engineer",
        "description": "World-class data engineering skill for building scalable data pipelines, ETL/ELT systems, and data infrastructure. Expertise in Python, SQL, Spark, Airflow, dbt, Kafka, and modern data stack. Includes data modeling, pipeline orchestration, data quality, and DataOps. Use when designing data architectures, building data pipelines, optimizing data workflows, or implementing data governance."
      },
      "content": "\n# Senior Data Engineer\n\nWorld-class senior data engineer skill for production-grade AI/ML/Data systems.\n\n## Quick Start\n\n### Main Capabilities\n\n```bash\n# Core Tool 1\npython scripts/pipeline_orchestrator.py --input data/ --output results/\n\n# Core Tool 2  \npython scripts/data_quality_validator.py --target project/ --analyze\n\n# Core Tool 3\npython scripts/etl_performance_optimizer.py --config config.yaml --deploy\n```\n\n## Core Expertise\n\nThis skill covers world-class capabilities in:\n\n- Advanced production patterns and architectures\n- Scalable system design and implementation\n- Performance optimization at scale\n- MLOps and DataOps best practices\n- Real-time processing and inference\n- Distributed computing frameworks\n- Model deployment and monitoring\n- Security and compliance\n- Cost optimization\n- Team leadership and mentoring\n\n## Tech Stack\n\n**Languages:** Python, SQL, R, Scala, Go\n**ML Frameworks:** PyTorch, TensorFlow, Scikit-learn, XGBoost\n**Data Tools:** Spark, Airflow, dbt, Kafka, Databricks\n**LLM Frameworks:** LangChain, LlamaIndex, DSPy\n**Deployment:** Docker, Kubernetes, AWS/GCP/Azure\n**Monitoring:** MLflow, Weights & Biases, Prometheus\n**Databases:** PostgreSQL, BigQuery, Snowflake, Pinecone\n\n## Reference Documentation\n\n### 1. Data Pipeline Architecture\n\nComprehensive guide available in `references/data_pipeline_architecture.md` covering:\n\n- Advanced patterns and best practices\n- Production implementation strategies\n- Performance optimization techniques\n- Scalability considerations\n- Security and compliance\n- Real-world case studies\n\n### 2. Data Modeling Patterns\n\nComplete workflow documentation in `references/data_modeling_patterns.md` including:\n\n- Step-by-step processes\n- Architecture design patterns\n- Tool integration guides\n- Performance tuning strategies\n- Troubleshooting procedures\n\n### 3. Dataops Best Practices\n\nTechnical reference guide in `references/dataops_best_practices.md` with:\n\n- System design principles\n- Implementation examples\n- Configuration best practices\n- Deployment strategies\n- Monitoring and observability\n\n## Production Patterns\n\n### Pattern 1: Scalable Data Processing\n\nEnterprise-scale data processing with distributed computing:\n\n- Horizontal scaling architecture\n- Fault-tolerant design\n- Real-time and batch processing\n- Data quality validation\n- Performance monitoring\n\n### Pattern 2: ML Model Deployment\n\nProduction ML system with high availability:\n\n- Model serving with low latency\n- A/B testing infrastructure\n- Feature store integration\n- Model monitoring and drift detection\n- Automated retraining pipelines\n\n### Pattern 3: Real-Time Inference\n\nHigh-throughput inference system:\n\n- Batching and caching strategies\n- Load balancing\n- Auto-scaling\n- Latency optimization\n- Cost optimization\n\n## Best Practices\n\n### Development\n\n- Test-driven development\n- Code reviews and pair programming\n- Documentation as code\n- Version control everything\n- Continuous integration\n\n### Production\n\n- Monitor everything critical\n- Automate deployments\n- Feature flags for releases\n- Canary deployments\n- Comprehensive logging\n\n### Team Leadership\n\n- Mentor junior engineers\n- Drive technical decisions\n- Establish coding standards\n- Foster learning culture\n- Cross-functional collaboration\n\n## Performance Targets\n\n**Latency:**\n- P50: < 50ms\n- P95: < 100ms\n- P99: < 200ms\n\n**Throughput:**\n- Requests/second: > 1000\n- Concurrent users: > 10,000\n\n**Availability:**\n- Uptime: 99.9%\n- Error rate: < 0.1%\n\n## Security & Compliance\n\n- Authentication & authorization\n- Data encryption (at rest & in transit)\n- PII handling and anonymization\n- GDPR/CCPA compliance\n- Regular security audits\n- Vulnerability management\n\n## Common Commands\n\n```bash\n# Development\npython -m pytest tests/ -v --cov\npython -m black src/\npython -m pylint src/\n\n# Training\npython scripts/train.py --config prod.yaml\npython scripts/evaluate.py --model best.pth\n\n# Deployment\ndocker build -t service:v1 .\nkubectl apply -f k8s/\nhelm upgrade service ./charts/\n\n# Monitoring\nkubectl logs -f deployment/service\npython scripts/health_check.py\n```\n\n## Resources\n\n- Advanced Patterns: `references/data_pipeline_architecture.md`\n- Implementation Guide: `references/data_modeling_patterns.md`\n- Technical Reference: `references/dataops_best_practices.md`\n- Automation Scripts: `scripts/` directory\n\n## Senior-Level Responsibilities\n\nAs a world-class senior professional:\n\n1. **Technical Leadership**\n   - Drive architectural decisions\n   - Mentor team members\n   - Establish best practices\n   - Ensure code quality\n\n2. **Strategic Thinking**\n   - Align with business goals\n   - Evaluate trade-offs\n   - Plan for scale\n   - Manage technical debt\n\n3. **Collaboration**\n   - Work across teams\n   - Communicate effectively\n   - Build consensus\n   - Share knowledge\n\n4. **Innovation**\n   - Stay current with research\n   - Experiment with new approaches\n   - Contribute to community\n   - Drive continuous improvement\n\n5. **Production Excellence**\n   - Ensure high availability\n   - Monitor proactively\n   - Optimize performance\n   - Respond to incidents\n"
    }
  },
  "alirezarezvani-claude-skills-senior-prompt-engineer": {
    "id": "alirezarezvani-claude-skills-senior-prompt-engineer",
    "name": "senior-prompt-engineer",
    "description": "World-class prompt engineering skill for LLM optimization, prompt patterns, structured outputs, and AI product development. Expertise in Claude, GPT-4, prompt design patterns, few-shot learning, chain-of-thought, and AI evaluation. Includes RAG optimization, agent design, and LLM system architecture. Use when building AI products, optimizing LLM performance, designing agentic systems, or implementing advanced prompting techniques.",
    "repo": {
      "owner": "alirezarezvani",
      "name": "claude-skills",
      "fullName": "alirezarezvani/claude-skills",
      "url": "https://github.com/alirezarezvani/claude-skills/tree/main/engineering-team/senior-prompt-engineer",
      "defaultBranch": "main"
    },
    "metadata": {
      "stars": 521,
      "forks": 103,
      "language": "Python",
      "topics": [
        "agentic-ai",
        "agentic-coding",
        "anthropic-claude",
        "claude-ai",
        "claude-ai-skills",
        "claude-code",
        "claude-code-skills",
        "claude-skills",
        "claudecode-subagents"
      ],
      "updatedAt": "2026-01-08T12:36:14Z",
      "pushedAt": "2026-01-08T08:39:21Z",
      "createdAt": "2025-10-19T04:04:05Z",
      "license": "MIT License"
    },
    "category": "development",
    "tags": [],
    "skillMd": {
      "raw": "---\nname: senior-prompt-engineer\ndescription: World-class prompt engineering skill for LLM optimization, prompt patterns, structured outputs, and AI product development. Expertise in Claude, GPT-4, prompt design patterns, few-shot learning, chain-of-thought, and AI evaluation. Includes RAG optimization, agent design, and LLM system architecture. Use when building AI products, optimizing LLM performance, designing agentic systems, or implementing advanced prompting techniques.\n---\n\n# Senior Prompt Engineer\n\nWorld-class senior prompt engineer skill for production-grade AI/ML/Data systems.\n\n## Quick Start\n\n### Main Capabilities\n\n```bash\n# Core Tool 1\npython scripts/prompt_optimizer.py --input data/ --output results/\n\n# Core Tool 2  \npython scripts/rag_evaluator.py --target project/ --analyze\n\n# Core Tool 3\npython scripts/agent_orchestrator.py --config config.yaml --deploy\n```\n\n## Core Expertise\n\nThis skill covers world-class capabilities in:\n\n- Advanced production patterns and architectures\n- Scalable system design and implementation\n- Performance optimization at scale\n- MLOps and DataOps best practices\n- Real-time processing and inference\n- Distributed computing frameworks\n- Model deployment and monitoring\n- Security and compliance\n- Cost optimization\n- Team leadership and mentoring\n\n## Tech Stack\n\n**Languages:** Python, SQL, R, Scala, Go\n**ML Frameworks:** PyTorch, TensorFlow, Scikit-learn, XGBoost\n**Data Tools:** Spark, Airflow, dbt, Kafka, Databricks\n**LLM Frameworks:** LangChain, LlamaIndex, DSPy\n**Deployment:** Docker, Kubernetes, AWS/GCP/Azure\n**Monitoring:** MLflow, Weights & Biases, Prometheus\n**Databases:** PostgreSQL, BigQuery, Snowflake, Pinecone\n\n## Reference Documentation\n\n### 1. Prompt Engineering Patterns\n\nComprehensive guide available in `references/prompt_engineering_patterns.md` covering:\n\n- Advanced patterns and best practices\n- Production implementation strategies\n- Performance optimization techniques\n- Scalability considerations\n- Security and compliance\n- Real-world case studies\n\n### 2. Llm Evaluation Frameworks\n\nComplete workflow documentation in `references/llm_evaluation_frameworks.md` including:\n\n- Step-by-step processes\n- Architecture design patterns\n- Tool integration guides\n- Performance tuning strategies\n- Troubleshooting procedures\n\n### 3. Agentic System Design\n\nTechnical reference guide in `references/agentic_system_design.md` with:\n\n- System design principles\n- Implementation examples\n- Configuration best practices\n- Deployment strategies\n- Monitoring and observability\n\n## Production Patterns\n\n### Pattern 1: Scalable Data Processing\n\nEnterprise-scale data processing with distributed computing:\n\n- Horizontal scaling architecture\n- Fault-tolerant design\n- Real-time and batch processing\n- Data quality validation\n- Performance monitoring\n\n### Pattern 2: ML Model Deployment\n\nProduction ML system with high availability:\n\n- Model serving with low latency\n- A/B testing infrastructure\n- Feature store integration\n- Model monitoring and drift detection\n- Automated retraining pipelines\n\n### Pattern 3: Real-Time Inference\n\nHigh-throughput inference system:\n\n- Batching and caching strategies\n- Load balancing\n- Auto-scaling\n- Latency optimization\n- Cost optimization\n\n## Best Practices\n\n### Development\n\n- Test-driven development\n- Code reviews and pair programming\n- Documentation as code\n- Version control everything\n- Continuous integration\n\n### Production\n\n- Monitor everything critical\n- Automate deployments\n- Feature flags for releases\n- Canary deployments\n- Comprehensive logging\n\n### Team Leadership\n\n- Mentor junior engineers\n- Drive technical decisions\n- Establish coding standards\n- Foster learning culture\n- Cross-functional collaboration\n\n## Performance Targets\n\n**Latency:**\n- P50: < 50ms\n- P95: < 100ms\n- P99: < 200ms\n\n**Throughput:**\n- Requests/second: > 1000\n- Concurrent users: > 10,000\n\n**Availability:**\n- Uptime: 99.9%\n- Error rate: < 0.1%\n\n## Security & Compliance\n\n- Authentication & authorization\n- Data encryption (at rest & in transit)\n- PII handling and anonymization\n- GDPR/CCPA compliance\n- Regular security audits\n- Vulnerability management\n\n## Common Commands\n\n```bash\n# Development\npython -m pytest tests/ -v --cov\npython -m black src/\npython -m pylint src/\n\n# Training\npython scripts/train.py --config prod.yaml\npython scripts/evaluate.py --model best.pth\n\n# Deployment\ndocker build -t service:v1 .\nkubectl apply -f k8s/\nhelm upgrade service ./charts/\n\n# Monitoring\nkubectl logs -f deployment/service\npython scripts/health_check.py\n```\n\n## Resources\n\n- Advanced Patterns: `references/prompt_engineering_patterns.md`\n- Implementation Guide: `references/llm_evaluation_frameworks.md`\n- Technical Reference: `references/agentic_system_design.md`\n- Automation Scripts: `scripts/` directory\n\n## Senior-Level Responsibilities\n\nAs a world-class senior professional:\n\n1. **Technical Leadership**\n   - Drive architectural decisions\n   - Mentor team members\n   - Establish best practices\n   - Ensure code quality\n\n2. **Strategic Thinking**\n   - Align with business goals\n   - Evaluate trade-offs\n   - Plan for scale\n   - Manage technical debt\n\n3. **Collaboration**\n   - Work across teams\n   - Communicate effectively\n   - Build consensus\n   - Share knowledge\n\n4. **Innovation**\n   - Stay current with research\n   - Experiment with new approaches\n   - Contribute to community\n   - Drive continuous improvement\n\n5. **Production Excellence**\n   - Ensure high availability\n   - Monitor proactively\n   - Optimize performance\n   - Respond to incidents\n",
      "frontmatter": {
        "name": "senior-prompt-engineer",
        "description": "World-class prompt engineering skill for LLM optimization, prompt patterns, structured outputs, and AI product development. Expertise in Claude, GPT-4, prompt design patterns, few-shot learning, chain-of-thought, and AI evaluation. Includes RAG optimization, agent design, and LLM system architecture. Use when building AI products, optimizing LLM performance, designing agentic systems, or implementing advanced prompting techniques."
      },
      "content": "\n# Senior Prompt Engineer\n\nWorld-class senior prompt engineer skill for production-grade AI/ML/Data systems.\n\n## Quick Start\n\n### Main Capabilities\n\n```bash\n# Core Tool 1\npython scripts/prompt_optimizer.py --input data/ --output results/\n\n# Core Tool 2  \npython scripts/rag_evaluator.py --target project/ --analyze\n\n# Core Tool 3\npython scripts/agent_orchestrator.py --config config.yaml --deploy\n```\n\n## Core Expertise\n\nThis skill covers world-class capabilities in:\n\n- Advanced production patterns and architectures\n- Scalable system design and implementation\n- Performance optimization at scale\n- MLOps and DataOps best practices\n- Real-time processing and inference\n- Distributed computing frameworks\n- Model deployment and monitoring\n- Security and compliance\n- Cost optimization\n- Team leadership and mentoring\n\n## Tech Stack\n\n**Languages:** Python, SQL, R, Scala, Go\n**ML Frameworks:** PyTorch, TensorFlow, Scikit-learn, XGBoost\n**Data Tools:** Spark, Airflow, dbt, Kafka, Databricks\n**LLM Frameworks:** LangChain, LlamaIndex, DSPy\n**Deployment:** Docker, Kubernetes, AWS/GCP/Azure\n**Monitoring:** MLflow, Weights & Biases, Prometheus\n**Databases:** PostgreSQL, BigQuery, Snowflake, Pinecone\n\n## Reference Documentation\n\n### 1. Prompt Engineering Patterns\n\nComprehensive guide available in `references/prompt_engineering_patterns.md` covering:\n\n- Advanced patterns and best practices\n- Production implementation strategies\n- Performance optimization techniques\n- Scalability considerations\n- Security and compliance\n- Real-world case studies\n\n### 2. Llm Evaluation Frameworks\n\nComplete workflow documentation in `references/llm_evaluation_frameworks.md` including:\n\n- Step-by-step processes\n- Architecture design patterns\n- Tool integration guides\n- Performance tuning strategies\n- Troubleshooting procedures\n\n### 3. Agentic System Design\n\nTechnical reference guide in `references/agentic_system_design.md` with:\n\n- System design principles\n- Implementation examples\n- Configuration best practices\n- Deployment strategies\n- Monitoring and observability\n\n## Production Patterns\n\n### Pattern 1: Scalable Data Processing\n\nEnterprise-scale data processing with distributed computing:\n\n- Horizontal scaling architecture\n- Fault-tolerant design\n- Real-time and batch processing\n- Data quality validation\n- Performance monitoring\n\n### Pattern 2: ML Model Deployment\n\nProduction ML system with high availability:\n\n- Model serving with low latency\n- A/B testing infrastructure\n- Feature store integration\n- Model monitoring and drift detection\n- Automated retraining pipelines\n\n### Pattern 3: Real-Time Inference\n\nHigh-throughput inference system:\n\n- Batching and caching strategies\n- Load balancing\n- Auto-scaling\n- Latency optimization\n- Cost optimization\n\n## Best Practices\n\n### Development\n\n- Test-driven development\n- Code reviews and pair programming\n- Documentation as code\n- Version control everything\n- Continuous integration\n\n### Production\n\n- Monitor everything critical\n- Automate deployments\n- Feature flags for releases\n- Canary deployments\n- Comprehensive logging\n\n### Team Leadership\n\n- Mentor junior engineers\n- Drive technical decisions\n- Establish coding standards\n- Foster learning culture\n- Cross-functional collaboration\n\n## Performance Targets\n\n**Latency:**\n- P50: < 50ms\n- P95: < 100ms\n- P99: < 200ms\n\n**Throughput:**\n- Requests/second: > 1000\n- Concurrent users: > 10,000\n\n**Availability:**\n- Uptime: 99.9%\n- Error rate: < 0.1%\n\n## Security & Compliance\n\n- Authentication & authorization\n- Data encryption (at rest & in transit)\n- PII handling and anonymization\n- GDPR/CCPA compliance\n- Regular security audits\n- Vulnerability management\n\n## Common Commands\n\n```bash\n# Development\npython -m pytest tests/ -v --cov\npython -m black src/\npython -m pylint src/\n\n# Training\npython scripts/train.py --config prod.yaml\npython scripts/evaluate.py --model best.pth\n\n# Deployment\ndocker build -t service:v1 .\nkubectl apply -f k8s/\nhelm upgrade service ./charts/\n\n# Monitoring\nkubectl logs -f deployment/service\npython scripts/health_check.py\n```\n\n## Resources\n\n- Advanced Patterns: `references/prompt_engineering_patterns.md`\n- Implementation Guide: `references/llm_evaluation_frameworks.md`\n- Technical Reference: `references/agentic_system_design.md`\n- Automation Scripts: `scripts/` directory\n\n## Senior-Level Responsibilities\n\nAs a world-class senior professional:\n\n1. **Technical Leadership**\n   - Drive architectural decisions\n   - Mentor team members\n   - Establish best practices\n   - Ensure code quality\n\n2. **Strategic Thinking**\n   - Align with business goals\n   - Evaluate trade-offs\n   - Plan for scale\n   - Manage technical debt\n\n3. **Collaboration**\n   - Work across teams\n   - Communicate effectively\n   - Build consensus\n   - Share knowledge\n\n4. **Innovation**\n   - Stay current with research\n   - Experiment with new approaches\n   - Contribute to community\n   - Drive continuous improvement\n\n5. **Production Excellence**\n   - Ensure high availability\n   - Monitor proactively\n   - Optimize performance\n   - Respond to incidents\n"
    }
  }
}